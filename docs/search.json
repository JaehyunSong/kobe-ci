[
  {
    "objectID": "slide/intro_rct.html#講師紹介",
    "href": "slide/intro_rct.html#講師紹介",
    "title": "方法論特殊講義III",
    "section": "講師紹介",
    "text": "講師紹介\n\n\n\n\n\nLINEスタンプ絶賛販売中!\n\n\n\n\n宋(そん)  財泫(じぇひょん) (SONG JAEHYUN)\n\n関西大学総合情報学部\n博士（政治学）\n政治行動論、政治学方法論\n\nsong@kansai-u.ac.jp\nhttps://www.jaysong.net"
  },
  {
    "objectID": "slide/intro_rct.html#内容",
    "href": "slide/intro_rct.html#内容",
    "title": "方法論特殊講義III",
    "section": "内容",
    "text": "内容\n各講義は以下の内容に関する理論と実習を5:5で行う予定。また、履修者の理解・進捗状況に応じて変更の可能性がある。\n\n1日目:8月17日(水)\n\n因果推論の考え方\nランダム化比較試験\n\n2日目:8月18日(木)\n\nLab Session: R の使い方\n2日目にLab Sessionを行わない場合は、以下の内容を繰り上げ、5日目は操作変数を解説\n\n3日目:8月19日(金)\n\n回帰分析とマッチング、その応用\n\n4日目:8月29日(月)\n\n差分の差分法とその応用\n\n5日目:8月30日(火)\n\n回帰不連続デザイン"
  },
  {
    "objectID": "slide/intro_rct.html#実習",
    "href": "slide/intro_rct.html#実習",
    "title": "方法論特殊講義III",
    "section": "実習",
    "text": "実習\n実習はRで行う。1・2日目はRの導入および使い方についても解説（復習レベル）する。\n\n本講義の分析はExcel, SPSS, Stata, Julia, Pythonなどでも可能\nJared P. Lander. 2017. R for Everyone: Advanced Analytics and Graphics (2nd Edition), Addison-Wesley Professional.（和訳有り）\n宋財泫・矢内勇生.『私たちのR: ベストプラクティスの探究』Web-book\n\n無料のR入門書: Rを広く、深く勉強したい人におすすめ\n\n\n\n\n宋のR環境\n\nmacOS 12.4 “Monterey”\nR version 4.2.1 (2022-06-23)\n\nR > 4.1ならOK\n\nRStudio 2022.02.3+492 “Prairie Trillium”\nスライド、サポートページ、実習用資料の執筆環境\n\nQuarto 1.0.36\nR package {quarto} 1.2"
  },
  {
    "objectID": "slide/intro_rct.html#rの学習資料",
    "href": "slide/intro_rct.html#rの学習資料",
    "title": "方法論特殊講義III",
    "section": "Rの学習資料",
    "text": "Rの学習資料\n計量政治学とR\n\n浅野正彦・矢内勇生. 2019『Rによる計量政治学』オーム社.\n飯田健. 2013.『計量政治分析』共立出版.\nKosuke Imai. 2017. Quantitative Social Science: An Introduction, Princeton University Press. (邦訳あり[上/下])\n\nR全般\n\nWickham, Hadley and Grolemund, Garrett. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, O’Reilly. (邦訳あり/原著はインターネットから無料で閲覧可)\n松村優哉 他. 2021. 『改訂2版 Rユーザのための RStudio[実践] 入門—tidyverseによるモダンな分析フローの世界—』技術評論社.\nWickham, Hadley. 2019. Advanced R (Second Edition), O’Reilly. (邦訳あり/原著はインターネットから無料で閲覧可)"
  },
  {
    "objectID": "slide/intro_rct.html#slackについて",
    "href": "slide/intro_rct.html#slackについて",
    "title": "方法論特殊講義III",
    "section": "Slackについて",
    "text": "Slackについて\n登録方法\n\nTAの高根さんにE-mail\n\n201j024j@stu.kobe-u.ac.jp\nメールのタイトルは「計量政治学方法論I」、または「ソンさん、かっこいい！」\n空メールでOK\n\n\nSlackの使い方\n\nGoogleで「Slack 使い方」で検索\nSlack上の表示名は実名で"
  },
  {
    "objectID": "slide/intro_rct.html#評価",
    "href": "slide/intro_rct.html#評価",
    "title": "方法論特殊講義III",
    "section": "評価",
    "text": "評価\n平常点と期末レポート\n\n平常点: 30%\n\n授業への参加度 (質問/発言)\nSlackでの参加度も含む\n\n期末レポート: 70%\n\n提出方法、期限は講義最終日に告知"
  },
  {
    "objectID": "slide/intro_rct.html#レポート",
    "href": "slide/intro_rct.html#レポート",
    "title": "方法論特殊講義III",
    "section": "レポート",
    "text": "レポート\n以下の3つのオプションから選択\n\n研究のプロポーザル\n\n本講義で紹介した手法を用いた分析のプロポーザルを作成\n実現可能性があること\n\n既存研究の再現 (replication)\n\n分析をそのまま再現\n本講義で紹介した手法を用いて再現\n\n本講義で紹介した手法を用いた2次分析"
  },
  {
    "objectID": "slide/intro_rct.html#前提知識",
    "href": "slide/intro_rct.html#前提知識",
    "title": "方法論特殊講義III",
    "section": "前提知識",
    "text": "前提知識\n本講義の前提知識\n\n仮説検定\n統計的有意性検定\n回帰分析\n電卓としてのRの使い方\n\nデータクリーニング、回帰分析、可視化などができるならベスト\n2日目にRの解説は行うが、深入りはしない(できない)\n\nhttps://bit.ly/2VPXL9x \\(\\leftarrow\\) これくらいは知っておけば2日目の内容を省略し、5日目に操作変数法について解説\n\n最低限の使い方（データの読み込みなど）は知っている前提で解説\n『私たちのR』を読もう！"
  },
  {
    "objectID": "slide/intro_rct.html#社会科学における因果推論の意味",
    "href": "slide/intro_rct.html#社会科学における因果推論の意味",
    "title": "方法論特殊講義III",
    "section": "社会科学における因果推論の意味",
    "text": "社会科学における因果推論の意味\nMorgan and Winship (2014) Counterfactuals and Causal Inference: Methods And Principles For Social Research. Cambridge.\n\nMore has been learned about causal inference in the last few decades than sum total of everything that had been learned about it in all prior recorded history. (Gary King)"
  },
  {
    "objectID": "slide/intro_rct.html#相関から因果へ",
    "href": "slide/intro_rct.html#相関から因果へ",
    "title": "方法論特殊講義III",
    "section": "相関から因果へ",
    "text": "相関から因果へ\n原因 (X) と結果 (Y) の関係\n\n\n\n\n\n\n\n\n\n\n\n\n年齢(世代)と投票率の関係（架空の例）\n\n年齢が上がると高い投票率\n\n相関関係\n統計分析から得られる結果は相関関係のみ\n\n理論/デザインを用いて相関関係が因果関係であることを説得\n相関関係→因果関係における障害物\n\nSelection Bias\nSimultaneity\nSpurious Correlation\nReverse Causality\nOmitted Variable Biasなど"
  },
  {
    "objectID": "slide/intro_rct.html#同時性",
    "href": "slide/intro_rct.html#同時性",
    "title": "方法論特殊講義III",
    "section": "同時性",
    "text": "同時性\nSimultaneity\n\n\n\n\n\n\n\n\n原因と結果の間に双方向の因果関係が存在\n\n例) お酒 (原因; X) とストレス (結果; Y) の関係\n\n酒を飲むとストレスが貯まる\nストレス解消のために酒を飲む\n酒を飲むとストレスが貯まる\nストレス解消のために酒を飲む\n酒を飲むとストレスが貯まる\n…\n\n\\(\\rightarrow\\) 地獄のような無限ループ\n\n\\(\\Rightarrow\\) 酒がストレスに与える影響は?"
  },
  {
    "objectID": "slide/intro_rct.html#見かけ上の相関",
    "href": "slide/intro_rct.html#見かけ上の相関",
    "title": "方法論特殊講義III",
    "section": "見かけ上の相関",
    "text": "見かけ上の相関\nSpurious Correlation、擬似相関\n\nたまたま相関関係がある場合\n\n例) メイン州の離婚率一人当たりマーガリンの消費量"
  },
  {
    "objectID": "slide/intro_rct.html#見かけ上の相関-1",
    "href": "slide/intro_rct.html#見かけ上の相関-1",
    "title": "方法論特殊講義III",
    "section": "見かけ上の相関",
    "text": "見かけ上の相関\nSpurious Correlation、擬似相関\n\n共通の要因からの影響\n\n例) ビール消費量とアイスクリーム消費量"
  },
  {
    "objectID": "slide/intro_rct.html#見かけ上の相関-2",
    "href": "slide/intro_rct.html#見かけ上の相関-2",
    "title": "方法論特殊講義III",
    "section": "見かけ上の相関",
    "text": "見かけ上の相関\nSpurious Correlation、擬似相関\n\n共通の要因からの影響\n\n例) ゲームをやると身長が伸びる説"
  },
  {
    "objectID": "slide/intro_rct.html#逆の因果",
    "href": "slide/intro_rct.html#逆の因果",
    "title": "方法論特殊講義III",
    "section": "逆の因果",
    "text": "逆の因果\nReverse Causality\n\n例) 心臓移植と生存率の例\n\n\n\n\n\n\n5年後に死亡\n5年後に生存\n\n\n\n\n心臓移植を\n受けた\n10名\n5名\n\n\n\n受けなかった\n5名\n10名\n\n\n\n\n心臓移植を受けたら死亡確率が上がる?\n死亡確率が高い人が心臓移植を受ける?"
  },
  {
    "objectID": "slide/intro_rct.html#逆の因果-1",
    "href": "slide/intro_rct.html#逆の因果-1",
    "title": "方法論特殊講義III",
    "section": "逆の因果",
    "text": "逆の因果\nReverse Causality\n\n「人気だから4文字に略されるのか、4文字に略せるからヒットす るのか、どっちなんでしょうね」"
  },
  {
    "objectID": "slide/intro_rct.html#欠落変数バイアス",
    "href": "slide/intro_rct.html#欠落変数バイアス",
    "title": "方法論特殊講義III",
    "section": "欠落変数バイアス",
    "text": "欠落変数バイアス\nOmitted Variable Bias\n例) 真のモデルが\\(Y = \\beta_0 + \\beta_1 \\cdot X + \\beta_2 \\cdot Z + e\\)の場合\n\n\n\n\n\n\n\n\n\n\n\n\n\nモデルに\\(Z\\)が含まれていなくても\\(\\beta_1\\)の推定値は変化\\(\\times\\)\n\n\\(X\\)と\\(Z\\)は独立（\\(X \\perp Z\\)）\n=\\(X\\)と\\(Z\\)の共分散が0（\\(\\sigma_{X, Z} = 0\\)）"
  },
  {
    "objectID": "slide/intro_rct.html#欠落変数バイアス-1",
    "href": "slide/intro_rct.html#欠落変数バイアス-1",
    "title": "方法論特殊講義III",
    "section": "欠落変数バイアス",
    "text": "欠落変数バイアス\nOmitted Variable Bias\n例) 真のモデルが\\(Y = \\beta_0 + \\beta_1 \\cdot X + \\beta_2 \\cdot Z + e\\)の場合\n\n\n\n\n\n\n\n\n\n\n\n\n\nモデルに \\(X2\\) が含まれていない場合、 \\(\\beta_1\\) の推定値にバイアス\n\n\\(X_1\\) と \\(X_2\\) が相関\n\\(\\sigma_{X_1, X_2} \\neq 0\\)\n\n\\(\\beta_1\\)の真の値（=不偏推定量）を推定するためには\\(X\\)と\\(Y\\)両方と相関する変数すべてが必要\n\nそもそも、「真の値」とは？\n\\(X\\)と\\(Y\\)両方と相関するすべての変数は特定可能? 測定可能?\n\n\\(\\rightarrow\\) データ分析から得られた結果はあくまでも「分析モデルが想定している世界」のものに過ぎない\n\n定量的手法は反証可能性を高めやすい手法（=科学的な手法になりやすい）であって、科学そのものを保障するものでもなく、得られた結果が真理であることを保障するものでもない。"
  },
  {
    "objectID": "slide/intro_rct.html#セレクションバイアス",
    "href": "slide/intro_rct.html#セレクションバイアス",
    "title": "方法論特殊講義III",
    "section": "セレクション・バイアス",
    "text": "セレクション・バイアス\nSelection Bias\n\n例) 職業訓練と期待収入（参考) 心臓移植）\n\n\n\n\n\n\n3年後の収入\n\n\n\n\n職業訓練を\n受けた\n6349ドル\n\n\n\n受けなかった\n6984ドル\n\n\n\n\n職業訓練を受けたら収入が上がる?\nもともと低収入の人が職業訓練を受けようとする?\n参考) 交絡因子の可能性も（就労意欲など）"
  },
  {
    "objectID": "slide/intro_rct.html#内生性",
    "href": "slide/intro_rct.html#内生性",
    "title": "方法論特殊講義III",
    "section": "内生性",
    "text": "内生性\nこれまでの多くの例は内生性（endogeneity）の問題\n\n内生性: 説明変数と誤差項間に相関が存在\n\n誤差項と相関のある説明変数: 内生変数（endogenous variable）\n\n内生性がある場合、推定値は一致推定量でも、不偏推定量でもはない\n\nサンプルサイズ（\\(N\\)）をいくら増やしても無駄\n\n内生性の原因\n\n同時性\n欠落変数バイアス\n測定誤差\nセレクション・バイアスなど\n\n最近の教科書はこれはすべてをセレクション・バイアスや欠落変数バイアスでまとめる傾向"
  },
  {
    "objectID": "slide/intro_rct.html#相関から因果へ-1",
    "href": "slide/intro_rct.html#相関から因果へ-1",
    "title": "方法論特殊講義III",
    "section": "相関から因果へ",
    "text": "相関から因果へ\n内生性の除外 \\(\\rightarrow\\) 因果効果の推定"
  },
  {
    "objectID": "slide/intro_rct.html#因果関係の例",
    "href": "slide/intro_rct.html#因果関係の例",
    "title": "方法論特殊講義III",
    "section": "因果関係の例",
    "text": "因果関係の例\nソンさんの講義を履修することで期待年収が上がるか\n\n藤村君の場合: ソンさんの講義を履修し、年収が5000万円に\n\nソンさんの授業のおかげで富裕層になった.font50[(次は社交界進出)]\n友達に教えてあげよう\n\n\n講義履修の効果\n\n処置: ソンさんの講義を履修するか否か\n効果: 履修した場合の年収 − 履修しなかった場合の年収"
  },
  {
    "objectID": "slide/intro_rct.html#因果関係の例-1",
    "href": "slide/intro_rct.html#因果関係の例-1",
    "title": "方法論特殊講義III",
    "section": "因果関係の例",
    "text": "因果関係の例\nソンさんの講義を履修することで期待年収が上がるか\n\n藤村君の場合: ソンさんの講義を履修し、年収が5000万円に\n\nソンさんの授業のおかげで富裕層になった（次は社交界進出）\n友達に教えてあげよう\n\n\n講義履修の効果（ケース1）\n\n藤村君がソンさんの授業を履修しなくても年収5000万円なら\n\nソンさんの講義の因果効果は0\n\n\n\n\n\n\n履修しなかった場合の年収(A)\n履修した場合の年収(B)\n効果(B-A)\n\n\n\n\nケース1\n5000万\n5000万\n0万"
  },
  {
    "objectID": "slide/intro_rct.html#因果関係の例-2",
    "href": "slide/intro_rct.html#因果関係の例-2",
    "title": "方法論特殊講義III",
    "section": "因果関係の例",
    "text": "因果関係の例\nソンさんの講義を履修することで期待年収が上がるか\n\n藤村君の場合: ソンさんの講義を履修し、年収が5000万円に\n\nソンさんの授業のおかげで富裕層になった（次は社交界進出）\n友達に教えてあげよう\n\n\n講義履修の効果（ケース2）\n\n藤村君がソンさんの授業を履修しなかった場合、年収1000万円なら\n\nソンさんの講義の因果効果は4000万円\n一生ソンさんには頭が上がらない\n\n\n\n\n\n\n履修しなかった場合の年収(A)\n履修した場合の年収(B)\n効果(B-A)\n\n\n\n\nケース2\n1000万\n5000万\n4000万"
  },
  {
    "objectID": "slide/intro_rct.html#因果関係の例-3",
    "href": "slide/intro_rct.html#因果関係の例-3",
    "title": "方法論特殊講義III",
    "section": "因果関係の例",
    "text": "因果関係の例\nソンさんの講義を履修することで期待年収が上がるか\n\n藤村君の場合: ソンさんの講義を履修し、年収が5000万円に\n\nソンさんの授業のおかげで富裕層になった（次は社交界進出）\n友達に教えてあげよう\n\n\n講義履修の効果（ケース3）\n\n藤村君がソンさんの授業を履修しなかった場合、年収8000万円なら\n\nソンさんの講義の因果効果は-3000万\nソンさんは悪くない\n\n\n\n\n\n\n履修しなかった場合の年収(A)\n履修した場合の年収(B)\n効果(B-A)\n\n\n\n\nケース3\n8000万\n5000万\n-3000万"
  },
  {
    "objectID": "slide/intro_rct.html#因果関係の例-4",
    "href": "slide/intro_rct.html#因果関係の例-4",
    "title": "方法論特殊講義III",
    "section": "因果関係の例",
    "text": "因果関係の例\nソンさんの講義を履修することで期待年収が上がるか\n\n藤村君の場合: ソンさんの講義を履修し、年収が5000万円に\n\nソンさんの授業のおかげで富裕層になった（次は社交界進出）\n友達に教えてあげよう\n\n\n講義履修の効果\n\nソンさんの講義を履修しなかった場合の藤村君の年収は…?\n\n個人（藤村君）における処置効果を推定する際にはこれが不可欠\n\n\n\n\n\n\n履修しなかった場合の年収(A)\n履修した場合の年収(B)\n効果(B-A)\n\n\n\n\nケース1\n5000万\n5000万\n0万\n\n\nケース2\n1000万\n5000万\n4000万\n\n\nケース3\n8000万\n5000万\n-3000万"
  },
  {
    "objectID": "slide/intro_rct.html#潜在的結果枠組み",
    "href": "slide/intro_rct.html#潜在的結果枠組み",
    "title": "方法論特殊講義III",
    "section": "潜在的結果枠組み",
    "text": "潜在的結果枠組み\nNeyman-Rubin-HollandのPotential Outcome Framework\n\n\\(i\\) : 学生ID ( \\(i = 1,2,3,...,N\\) )\n\\(T\\) : 処置\n\n学生 \\(i\\) が謎の薬を飲んだ ( \\(T_i = 1\\) )\n学生 \\(i\\) が謎の薬を飲まなかった ( \\(T_i = 0\\) )\n\n\\(Y_i(T_i = 1)\\) : 学生 \\(i\\) が謎の薬を飲んだ場合の数学成績\n\\(Y_i(T_i = 0)\\) : 学生 \\(i\\) が謎の薬を飲まなかった場合の数学成績\n\\(ITE_i = Y_i(T_i = 1) − Y_i(T_i = 0)\\) : 学生iにおける薬の処置効果\n\nITE: Individual Treatment Effect (個人における処置効果)\n\n= UTE: Unit Treatment Effect\n\n全く同じ個人において薬を飲んだ場合と飲まなかった場合の数学成績の差 = 謎の薬の因果効果"
  },
  {
    "objectID": "slide/intro_rct.html#薬の効果は",
    "href": "slide/intro_rct.html#薬の効果は",
    "title": "方法論特殊講義III",
    "section": "薬の効果は?",
    "text": "薬の効果は?\nITEの平均値は−4であり、個人差はあるものの、全体的に薬は成績に負の影響\n\n\n\n\\(i\\)\n\\(T_i\\)\n\\(Y_i(T_i = 0)\\)\n\\(Y_i(T_i = 1)\\)\n\\(ITE_i\\)\n\n\n\n\n1\n1\n77\n85\n8\n\n\n2\n1\n49\n59\n10\n\n\n3\n1\n60\n66\n6\n\n\n4\n0\n61\n44\n-17\n\n\n5\n0\n50\n39\n-11\n\n\n6\n0\n75\n55\n-20\n\n\n平均\n\n62\n58\n-4"
  },
  {
    "objectID": "slide/intro_rct.html#因果推論の根本問題-1",
    "href": "slide/intro_rct.html#因果推論の根本問題-1",
    "title": "方法論特殊講義III",
    "section": "因果推論の根本問題",
    "text": "因果推論の根本問題\nしかし、観察できるのは\\(Y_i(T_i = 1)\\)か\\(Y_i(T_i = 0)\\)、片方のみ\n\n\\(Y_i(T_i = 0)\\)は反実仮想（counterfactual）であり、観察不可 (\\(i \\in \\{1,2,3\\}\\))\n\\(Y_i(T_i = 1)\\)も反実仮想(\\(i \\in \\{4,5,6\\}\\))\n\n\n\n\n\\(i\\)\n\\(T_i\\)\n\\(Y_i(T_i = 0)\\)\n\\(Y_i(T_i = 1)\\)\n\\(ITE_i\\)\n\n\n\n\n1\n1\n?\n85\n?\n\n\n2\n1\n?\n59\n?\n\n\n3\n1\n?\n66\n?\n\n\n4\n0\n61\n?\n?\n\n\n5\n0\n50\n?\n?\n\n\n6\n0\n75\n?\n?\n\n\n平均\n\n62\n70\n8\n\n\n\n\n\n「みんなで薬やろうぜ」って言っていいのか"
  },
  {
    "objectID": "slide/intro_rct.html#世界一受けたいソンさんの授業",
    "href": "slide/intro_rct.html#世界一受けたいソンさんの授業",
    "title": "方法論特殊講義III",
    "section": "世界一受けたいソンさんの授業",
    "text": "世界一受けたいソンさんの授業\n履修者5名と非履修者5名の年収の比較\n\nITEは分からないが、平均値の差分を見ると、+100万円の効果\n\n\n\n\n\\(i\\)\n\\(T_i\\)\n\\(Y_i(T_i = 0)\\)\n\\(Y_i(T_i = 1)\\)\n\\(ITE_i\\)\n\n\n\n\n1\n1\n?\n700\n?\n\n\n2\n1\n?\n1000\n?\n\n\n3\n1\n?\n550\n?\n\n\n4\n1\n?\n350\n?\n\n\n5\n1\n?\n400\n?\n\n\n6\n0\n400\n?\n?\n\n\n7\n0\n500\n?\n?\n\n\n8\n0\n350\n?\n?\n\n\n9\n0\n750\n?\n?\n\n\n10\n0\n500\n?\n?\n\n\n平均\n\n500\n600\n100"
  },
  {
    "objectID": "slide/intro_rct.html#世界一受けたいソンさんの授業-1",
    "href": "slide/intro_rct.html#世界一受けたいソンさんの授業-1",
    "title": "方法論特殊講義III",
    "section": "世界一受けたいソンさんの授業",
    "text": "世界一受けたいソンさんの授業\n履修者5名と非履修者5名の年収の比較（ケース1）\n\nITEは分からないが、平均値の差分を見ると、+100万円の効果\n80万円の価値があるソンさんの講義、みんなで履修しよう!\n\n\n\n\n\\(i\\)\n\\(T_i\\)\n\\(Y_i(T_i = 0)\\)\n\\(Y_i(T_i = 1)\\)\n\\(ITE_i\\)\n\n\n\n\n1\n1\n550\n700\n150\n\n\n2\n1\n650\n1000\n350\n\n\n3\n1\n600\n550\n-50\n\n\n4\n1\n300\n350\n50\n\n\n5\n1\n300\n400\n100\n\n\n6\n0\n400\n300\n-100\n\n\n7\n0\n500\n700\n200\n\n\n8\n0\n350\n600\n250\n\n\n9\n0\n750\n700\n-50\n\n\n10\n0\n500\n400\n-100\n\n\n平均\n\n490\n570\n80"
  },
  {
    "objectID": "slide/intro_rct.html#世界一受けたいソンさんの授業-2",
    "href": "slide/intro_rct.html#世界一受けたいソンさんの授業-2",
    "title": "方法論特殊講義III",
    "section": "世界一受けたいソンさんの授業",
    "text": "世界一受けたいソンさんの授業\n履修者5名と非履修者5名の年収の比較（ケース2）\n\nITEは分からないが、平均値の差分を見ると、+100万円の効果\nソンさんは悪くない\n\n\n\n\n\\(i\\)\n\\(T_i\\)\n\\(Y_i(T_i = 0)\\)\n\\(Y_i(T_i = 1)\\)\n\\(ITE_i\\)\n\n\n\n\n1\n1\n800\n700\n-100\n\n\n2\n1\n650\n1000\n350\n\n\n3\n1\n600\n550\n-50\n\n\n4\n1\n400\n350\n-50\n\n\n5\n1\n350\n400\n50\n\n\n6\n0\n400\n300\n-100\n\n\n7\n0\n500\n500\n0\n\n\n8\n0\n350\n400\n50\n\n\n9\n0\n750\n500\n-250\n\n\n10\n0\n500\n400\n-100\n\n\n平均\n\n530\n510\n-20"
  },
  {
    "objectID": "slide/intro_rct.html#因果推論の根本問題-2",
    "href": "slide/intro_rct.html#因果推論の根本問題-2",
    "title": "方法論特殊講義III",
    "section": "因果推論の根本問題",
    "text": "因果推論の根本問題\n\n\\(Y_i(T_i = 1)\\)か\\(Y_i(T_i = 0)\\)、片方のみしか観察できない状態においてITEから因果効果を推定することは不可能\n\n因果推論の根本問題 (The Fundamental Problem of Causal Inference)\n\n\n\n\n解決方法\n\nもう一回、過去に戻って異なる処置を行う"
  },
  {
    "objectID": "slide/intro_rct.html#因果推論の根本問題-3",
    "href": "slide/intro_rct.html#因果推論の根本問題-3",
    "title": "方法論特殊講義III",
    "section": "因果推論の根本問題",
    "text": "因果推論の根本問題\n\n\\(Y_i(T_i = 1)\\)か\\(Y_i(T_i = 0)\\)、片方のみしか観察できない状態において、ITEから因果効果を推定することは不可能\n\nただし、のび太くん、ドラえもん、ジャイアンくんを除く\n因果推論の根本問題 (The Fundamental Problem of Causal Inference)\n\n潜在的結果を直接観察する方法\n\nただし、個々人の潜在的結果ではなく、集団における潜在的結果\n平均処置効果 (ATE; Average Treatment Effect)\n\n平均値の差分から平均的な因果効果を推定\nしかし、…\n\n無作為割当の重要性"
  },
  {
    "objectID": "slide/intro_rct.html#平均取るだけでok",
    "href": "slide/intro_rct.html#平均取るだけでok",
    "title": "方法論特殊講義III",
    "section": "平均取るだけでOK?",
    "text": "平均取るだけでOK?\n観察されたデータから差分を計算するだけではATEは推定不可能\n\n\n\n\\(i\\)\n\\(T_i\\)\n\\(Y_i(T_i = 0)\\)\n\\(Y_i(T_i = 1)\\)\n\\(ITE_i\\)\n\n\n\n\n1\n1\n?\n700\n?\n\n\n2\n1\n?\n1000\n?\n\n\n3\n1\n?\n550\n?\n\n\n4\n1\n?\n350\n?\n\n\n5\n1\n?\n400\n?\n\n\n6\n0\n400\n?\n?\n\n\n7\n0\n500\n?\n?\n\n\n8\n0\n350\n?\n?\n\n\n9\n0\n750\n?\n?\n\n\n10\n0\n500\n?\n?\n\n\n平均\n\n500\n600\n100"
  },
  {
    "objectID": "slide/intro_rct.html#信頼できるateの条件",
    "href": "slide/intro_rct.html#信頼できるateの条件",
    "title": "方法論特殊講義III",
    "section": "信頼できるATEの条件",
    "text": "信頼できるATEの条件\nATE 推定値の信頼性を損なう敵: 内生性 (しかも、常に存在する)\n例) やる気のある学生だけがソンさんの講義を履修した場合\n\nセレクション・バイアス\n\nソンさんの講義は鬼畜すぎるため、やる気満々の学生には役に立つものの、やる気のない学生にとってはむしろ学習意欲が低下\n\n疑似相関\n\nやる気のある学生はいろんな方面で頑張るから、将来年収が高くなる。\n\n測定誤差\n\n履修者の年収はジンバブエ・ドルで測定されている可能性も（これはないか）\n\n\n\n\n内生性は因果推論の敵! どうすれば…?\n\\(\\downarrow\\)\n無作為割当 (Random Assignment)"
  },
  {
    "objectID": "slide/intro_rct.html#無作為割当とは",
    "href": "slide/intro_rct.html#無作為割当とは",
    "title": "方法論特殊講義III",
    "section": "無作為割当とは",
    "text": "無作為割当とは\n無作為割当 (Random Assignment)\n\n処置を受けるかどうかを無作為に割り当てる方法\n\n完全無作為割当: 全ての被験者において、どのグループに属するかの確率が等しい\n\\(Pr(T_i = 1) = Pr(T_j = 1) \\text{ where } i \\neq j\\)\n\\(Pr(T_i = 0) = Pr(T_j = 0) \\text{ where } i \\neq j\\)\n無作為割当の方法は色々\n\n無作為に割り当てると、処置を受けないグループと処置を受けるグループは「集団」として同質なグループになる。\n\n受けないグループ: 統制群 (Control Group)\n受けるグループ: 処置群 (Treatment Group)\n\n一つの集団を一人の個人として扱い、ITEを測定 ⇒ ATE"
  },
  {
    "objectID": "slide/intro_rct.html#無作為割当の力",
    "href": "slide/intro_rct.html#無作為割当の力",
    "title": "方法論特殊講義III",
    "section": "無作為割当の力",
    "text": "無作為割当の力\nコインを投げ、表( \\(H\\) )なら統制群、裏( \\(T\\) )なら処置群に割当\n\n女性比率が55%、平均年齢が38歳の集団の例\n\n\n\n\nset.seed(19861008)\nData <- tibble(\n   ID = 1:20,\n   Female = sample(0:1, 20, replace = TRUE, \n                   prob = c(0.4, 0.6)),\n   Age    = round(rnorm(20, 38, 10), 0))\n\nData %>%\n   summarise(Female = mean(Female),\n             Age    = mean(Age))\n\n# A tibble: 1 × 2\n  Female   Age\n   <dbl> <dbl>\n1   0.55    38\n\n\n\n\n\n\n\n \n  \n    ID \n    Female \n    Age \n    　 \n    ID \n    Female \n    Age \n  \n \n\n  \n    1 \n    1 \n    31 \n     \n    11 \n    0 \n    38 \n  \n  \n    2 \n    1 \n    41 \n     \n    12 \n    1 \n    29 \n  \n  \n    3 \n    0 \n    31 \n     \n    13 \n    0 \n    21 \n  \n  \n    4 \n    1 \n    46 \n     \n    14 \n    0 \n    26 \n  \n  \n    5 \n    1 \n    37 \n     \n    15 \n    1 \n    36 \n  \n  \n    6 \n    1 \n    37 \n     \n    16 \n    1 \n    40 \n  \n  \n    7 \n    0 \n    30 \n     \n    17 \n    0 \n    50 \n  \n  \n    8 \n    1 \n    46 \n     \n    18 \n    0 \n    42 \n  \n  \n    9 \n    1 \n    56 \n     \n    19 \n    0 \n    29 \n  \n  \n    10 \n    0 \n    47 \n     \n    20 \n    1 \n    47"
  },
  {
    "objectID": "slide/intro_rct.html#無作為割当の力-1",
    "href": "slide/intro_rct.html#無作為割当の力-1",
    "title": "方法論特殊講義III",
    "section": "無作為割当の力",
    "text": "無作為割当の力\nコイン投げの結果\n\nset.seed(19861008)\nCoin <- sample(c(\"H\", \"T\"), 20, replace = TRUE)\nData$Coin <- Coin\n\n\n\n\n\n \n  \n    ID \n    Female \n    Age \n    Coin \n    　 \n    ID \n    Female \n    Age \n    Coin \n  \n \n\n  \n    1 \n    1 \n    31 \n    H \n     \n    11 \n    0 \n    38 \n    H \n  \n  \n    2 \n    1 \n    41 \n    T \n     \n    12 \n    1 \n    29 \n    T \n  \n  \n    3 \n    0 \n    31 \n    T \n     \n    13 \n    0 \n    21 \n    H \n  \n  \n    4 \n    1 \n    46 \n    T \n     \n    14 \n    0 \n    26 \n    T \n  \n  \n    5 \n    1 \n    37 \n    H \n     \n    15 \n    1 \n    36 \n    H \n  \n  \n    6 \n    1 \n    37 \n    H \n     \n    16 \n    1 \n    40 \n    T \n  \n  \n    7 \n    0 \n    30 \n    H \n     \n    17 \n    0 \n    50 \n    T \n  \n  \n    8 \n    1 \n    46 \n    T \n     \n    18 \n    0 \n    42 \n    T \n  \n  \n    9 \n    1 \n    56 \n    H \n     \n    19 \n    0 \n    29 \n    H \n  \n  \n    10 \n    0 \n    47 \n    H \n     \n    20 \n    1 \n    47 \n    H"
  },
  {
    "objectID": "slide/intro_rct.html#無作為割当の力-2",
    "href": "slide/intro_rct.html#無作為割当の力-2",
    "title": "方法論特殊講義III",
    "section": "無作為割当の力",
    "text": "無作為割当の力\n統制群と処置群が比較的同質的なグループに\n\n統制群（11名）: 女性比率が54.5%、平均年齢が37.2歳\n処置群 (9名): 女性比率が55.6%、平均年齢が39歳\n\n\nData %>%\n  group_by(Coin) %>%\n  summarise(Female = mean(Female),\n            Age    = mean(Age),\n            N      = n())\n\n# A tibble: 2 × 4\n  Coin  Female   Age     N\n  <chr>  <dbl> <dbl> <int>\n1 H      0.545  37.2    11\n2 T      0.556  39       9"
  },
  {
    "objectID": "slide/intro_rct.html#無作為割当の力-3",
    "href": "slide/intro_rct.html#無作為割当の力-3",
    "title": "方法論特殊講義III",
    "section": "無作為割当の力",
    "text": "無作為割当の力\n集団として処置群と統制群は、母集団とほぼ同質\n\n母集団:女性率が55%、平均年齢が38歳\n統制群:女性率が54.5%、平均年齢が37.2歳\n処置群:女性率が55.6%、平均年齢が39歳\n\\(n \\rightarrow \\infty\\) なら2つのグループはより同質的に（大数の弱法則）\n統制群と処置群、母集団はそれぞれ交換可能 (exchangeable)\n\n処置群に処置を与えること = 母集団全体に処置を与えること\n統制群に処置を与えないこと = 母集団全体に処置を与えないこと\n\n統制群と処置群の比較で集団を一つの単位としたITE (= ATE)が推定可能\n\n処置を与えた母集団 vs. 処置を与えなかった母集団"
  },
  {
    "objectID": "slide/intro_rct.html#無作為割当の力-4",
    "href": "slide/intro_rct.html#無作為割当の力-4",
    "title": "方法論特殊講義III",
    "section": "無作為割当の力",
    "text": "無作為割当の力\n無作為割当は均質な複数のグループを作る手法\n\n講義履修と年収の例だと、無作為割当をすることによって …\n\n各グループにやる気のある学生とない学生が均等に\n\nセレクション・バイアス、擬似相関の除去\n\nジンバブエ・ドルで測定される学生も均等に（これはないか）\n\n測定誤差の除去\n\n\n内生性:処置変数（講義の履修）と誤差項（やる気など）間の相関\n\nコイン投げの結果は被験者（学生）の性質と無関係に行われるため、誤差項と相関がない。\n外生変数 (Exogenous variable)\n学生の性質 (X) と処置有無 (T) は独立している ⇒ \\(X \\perp T\\)\n\n\n\n\n無作為割当は内生性を除去する最良の手法"
  },
  {
    "objectID": "slide/intro_rct.html#無作為抽出と無作為割当",
    "href": "slide/intro_rct.html#無作為抽出と無作為割当",
    "title": "方法論特殊講義III",
    "section": "無作為抽出と無作為割当",
    "text": "無作為抽出と無作為割当\n\n無作為抽出によってサンプル（標本）と母集団が交換可能（実はここが難しい）\n無作為割当によって各グループとサンプルに交換可能（=各グループ間で交換可能）\n無作為抽出&無作為割当によって各グループと母集団が交換可能（グループへの刺激=母集団への刺激）"
  },
  {
    "objectID": "slide/intro_rct.html#ランダム化比較試験とは",
    "href": "slide/intro_rct.html#ランダム化比較試験とは",
    "title": "方法論特殊講義III",
    "section": "ランダム化比較試験とは",
    "text": "ランダム化比較試験とは\nRandomized Controlled Trial (RCT)\n\n無作為割当で複数のグループを作り上げた上で、異なる刺激・処置を与え、結果を観察する手法\n\n社会科学でいう「実験」の多くはこれを指す\n因果推論の王道\n\n因果効果をもたらす(と想定される)処置変数が外生的\n\nグループ間における結果変数の差 = 因果効果\n\nデータ生成過程(Data Generating Process; DGP)への直接介入\n\n「真のモデル」が分かる\n\n\n実験の方法\n\nフィールド実験\n実験室実験\nサーベイ実験\n\nSONG Jaehyun・秦正樹. 2020. 「オンライン・サーベイ実験の方法: 理論編」『理論と方法』35 (1): 92-108.\n秦正樹・SONG Jaehyun. 2020. 「オンライン・サーベイ実験の方法: 実践編」『理論と方法』35 (1): 109-127."
  },
  {
    "objectID": "slide/intro_rct.html#データ生成過程への介入",
    "href": "slide/intro_rct.html#データ生成過程への介入",
    "title": "方法論特殊講義III",
    "section": "データ生成過程への介入",
    "text": "データ生成過程への介入\n以下のデータ生成過程を仮定\n\\[\n\\text{Income} = \\beta_0 + \\beta_1 \\cdot \\text{Quant} + \\varepsilon\n\\]\n\nIncome: 10年後の年収 (\\(\\in [0, \\infty)\\))\nQuant:ソンさんの講義を履修したか否か (\\(\\in \\{0, 1\\})\\) )\n誤差項( \\(\\varepsilon\\) )には「やる気」や「真面目さ」が含まれるため、Quantと相関がある (\\(\\rightarrow\\) 内生性)\n無作為割当で受講有無を決めると、「やる気」や「真面目さ」はQunatと無関係 (独立) になる\n\n例) 受講有無をコイン投げで決める場合、コインの結果は誤差項（やる気や真面目さ）と独立（ただし、全員がコイン投げの結果に従うと仮定）\n\\(\\Rightarrow\\) 内生性がなくなる!"
  },
  {
    "objectID": "slide/intro_rct.html#rctの例",
    "href": "slide/intro_rct.html#rctの例",
    "title": "方法論特殊講義III",
    "section": "RCTの例",
    "text": "RCTの例\nBertand and Mullainathan (2004)\n\n労働市場における人種差別\n約5000人分の架空の履歴書を求人中の会社へ送る\n\n履歴書の内容 (性別、人種、能力など) は完全無作為\n履歴書に人種は記入できないため、白人っぽい名前 (Emily など)、黒人っぽい名前 (Jamal など) を記入\n\n後は、返事を待つだけ\n\n処置変数: 人種 ( \\(\\in \\{\\text{black}, \\text{white}\\}\\) )\n結果変数: 連絡の有無 ( \\(\\in \\{0, 1\\}\\) )"
  },
  {
    "objectID": "slide/intro_rct.html#内生性の可能性",
    "href": "slide/intro_rct.html#内生性の可能性",
    "title": "方法論特殊講義III",
    "section": "内生性の可能性",
    "text": "内生性の可能性\n\n\n\n\n\n\n\n\n\n\n\n\n誤差項 (\\(\\varepsilon\\)) には教育水準、親の所得、居住地などが含まれる可能性\n\n実際に人種と上記の要因には相関あり\n人種 (処置) と誤差項 (\\(\\varepsilon\\)) 間の相関関係 \\(\\rightarrow\\) 内生性\n\n黒人が採用されなかった場合 …\n\n黒人だから? \\(\\leftarrow\\) 人種差別\\(\\bigcirc\\)\n教育水準が低いから \\(\\leftarrow\\) 人種差別\\(\\times\\)\n\n\n\n \\(\\Rightarrow\\) 内生性がある限り、因果効果の識別は困難  \\(\\Rightarrow\\) ケースによって政策的含意が変わる。"
  },
  {
    "objectID": "slide/intro_rct.html#rctの力",
    "href": "slide/intro_rct.html#rctの力",
    "title": "方法論特殊講義III",
    "section": "RCTの力",
    "text": "RCTの力\n\n\n\n\n白人の名前\n黒人の名前\n\n\n\n\nFemale\n76.42%\n77.45%\n\n\nHighQuality\n50.23%\n50.23%\n\n\nCall Rate\n9.65%\n6.45%\n\n\n計 (人)\n2435\n2435\n\n\n\n\n無作為割当の結果、人種と性別・能力の相関がほぼ0に\n\n内生性のない状態\nこの場合、労働市場における人種の因果効果は\n\nATE = 黒人の平均連絡率 − 白人の平均連絡率\n黒人という理由だけで会社から連絡が来る確率が 3.2%p\\(\\downarrow\\)\n-3.2%p: 人種の因果効果 or 処置効果 (treatment effect)"
  },
  {
    "objectID": "slide/intro_rct.html#バランスチェック",
    "href": "slide/intro_rct.html#バランスチェック",
    "title": "方法論特殊講義III",
    "section": "バランスチェック",
    "text": "バランスチェック\n無作為割当が行われているか否かを確認\n\n\n\n\n\n\n\n\n\n\n\n\n標準化差分を使用\n\nStandardized Bias (or Difference)\n\nサンプルサイズの影響\\(\\times\\)\n統計的検定ではない\n\n\\(t\\) 検定、ANOVA、 \\(\\chi^2\\) 検定は\\(\\times\\)\n\nバランスチェックに統計的有意性検定は使わない\n\n{cobalt}、{BalanceR}など"
  },
  {
    "objectID": "slide/intro_rct.html#標準化差分について",
    "href": "slide/intro_rct.html#標準化差分について",
    "title": "方法論特殊講義III",
    "section": "標準化差分について",
    "text": "標準化差分について\n連続変数\n\\[\n\\text{SB}_{T-C} = 100 \\cdot \\frac{\\bar{X}_T - \\bar{X}_C}{\\sqrt{0.5 \\cdot (s_T^2 + s_C^2)}}\n\\]\n二値変数\n\\[\n\\text{SB}_{T-C} = 100 \\cdot \\frac{\\bar{X}_T - \\bar{X}_C}{\\sqrt{0.5 \\cdot (\\bar{X}_T(1-\\bar{X}_T) + \\bar{X}_C(1-\\bar{X}_C))}}\n\\]\n\n\\(\\bar{X}_T\\) : 処置群におけるXの平均値\n\\(s_T^2\\) : 処置群におけるXの分散\n|SB|が小さいほどバランス\n\n明確な基準はないが、3、5、10、25などを使用\n\nグループが3つ以上の場合、それぞれのペアで実行"
  },
  {
    "objectID": "slide/intro_rct.html#因果効果の推定",
    "href": "slide/intro_rct.html#因果効果の推定",
    "title": "方法論特殊講義III",
    "section": "因果効果の推定",
    "text": "因果効果の推定\n\n方法1: グループ間の結果変数の差分の検定 (\\(t\\)検定)\n\n因果効果 (ATE): \\(\\mathbb{E}[\\mbox{Call}|\\mbox{Race = Black}] - \\mathbb{E}[\\mbox{Call}|\\mbox{Race = White}] = -0.032\\)\nATE = 0の帰無仮説の検定\n\n\\(t_{\\text{df} = 4711.7} = −4.117\\); \\(p\\) < 0.001; 95% CI = [−0.047, −0.017]\n\n応答変数の尺度に応じてノンパラメトリック分析\n\n方法2: 単回帰分析 (線形 or ロジスティックス/プロビット)\n\n\n\n\n\n\n線形回帰分析（LPM）\n\n\nCovriates\nEst.\nS.E.\n\n\n\n\nIntercept\n0.064\n0.006\n\n\nRace: White\n0.032\n0.008\n\n\n\n\n\n\nロジスティック回帰分析\n\n\nCovriates\nEst.\nS.E.\n\n\n\n\nIntercept\n-2.675\n0.083\n\n\nRace: White\n0.438\n0.107\n\n\n\n\n\n\nプロビット回帰分析\n\n\nCovriates\nEst.\nS.E.\n\n\n\n\nIntercept\n-1.518\n0.039\n\n\nRace: White\n0.21654\n0.053\n\n\n\n\n\n\n\n\nFreedman, David A. 2008. “Randomization Does Not Justify Logistic Regression,” Statistical Science Statistical Science, 23(2): 237-249.\n\nLogit: 一致推定量\\(\\times\\) & 不偏推定量\\(\\times\\)\nLinear: 一致推定量\\(\\bigcirc\\) & 不偏推定量\\(\\times\\)\n一致性と不偏性の違いについて"
  },
  {
    "objectID": "slide/intro_rct.html#因果効果の推定-重回帰分析は",
    "href": "slide/intro_rct.html#因果効果の推定-重回帰分析は",
    "title": "方法論特殊講義III",
    "section": "因果効果の推定: 重回帰分析は?",
    "text": "因果効果の推定: 重回帰分析は?\n無作為割当のおかげですべての変数が互いに独立\n\n重回帰分析をしても人種のATEは変化しない (OVB がない)\n\n無作為割当の場合、回帰はしてもしなくても良い\n\n現実的に完全にバランスが取れていないため、若干の変化はある\n\n\n\n\nCovriates\nEst.\nS.E.\n\n\n\n\nIntercept\n0.057\n0.007\n\n\nRace: White\n0.032\n0.08\n\n\nFemale\n0.007\n0.009\n\n\nMilitary\n-0.027\n0.014\n\n\nEducation\n-0.002\n0.005\n\n\nHigh Quality\n0.019\n0.008"
  },
  {
    "objectID": "slide/intro_rct.html#因果効果の不均一性",
    "href": "slide/intro_rct.html#因果効果の不均一性",
    "title": "方法論特殊講義III",
    "section": "因果効果の不均一性",
    "text": "因果効果の不均一性\n因果効果が下位グループによって異なる場合\n\n因果効果の不均一性 (heterogeneous treatment effects)\n\n例) 性別によって薬の効果が異なる場合\n薬の効果が男性なら 1、女性なら 2 の場合\n\n男女比が1:1なら、ATEは1.5に\n\n薬の効果が男性なら 4、女性なら-1 の場合\n\n男女比が1:1なら、ATEは1.5だが…\n\n\n方法1: 男女に分けてATEを推定\n方法2: 性別と処置有無の交差項を投入した重回帰分析\n参考) Bryan, Christopher J., Elizabeth Tipton and David S. Yeager. 2021. “Behavioural science is unlikely to change the world without a heterogeneity revolution,” Nature Human Behaviour. 5: 980–989."
  },
  {
    "objectID": "slide/intro_rct.html#因果効果の不均一性-1",
    "href": "slide/intro_rct.html#因果効果の不均一性-1",
    "title": "方法論特殊講義III",
    "section": "因果効果の不均一性",
    "text": "因果効果の不均一性\n\nintro_data2.csvの例\n\n方法1: 男女に分けてATEを推定\n\n\n\n\n統制群\n処置群\nATE\n\\(t\\)\n\\(p\\)\n\n\n\n\n男性のみ\n0.611\n1.561\n0.951\n-7.521\n< 0.001\n\n\n女性のみ\n0.493\n2.480\n1.987\n-15.573\n< 0.001\n\n\n全体\n0.551\n2.057\n1.506\n-15.945\n< 0.001"
  },
  {
    "objectID": "slide/intro_rct.html#因果効果の不均一性-2",
    "href": "slide/intro_rct.html#因果効果の不均一性-2",
    "title": "方法論特殊講義III",
    "section": "因果効果の不均一性",
    "text": "因果効果の不均一性\n\nintro_data2.csvの例\n\n方法2: 性別と処置有無の交差項を投入した重回帰分析\n\n\n\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    0.611 (0.091) \n  \n  \n    Treatment \n    0.951 (0.131) \n  \n  \n    Female \n    −0.117 (0.127) \n  \n  \n    Treatment × Female \n    1.036 (0.180) \n  \n  \n    Num.Obs. \n    500 \n  \n  \n    R2 \n    0.401 \n  \n  \n    R2 Adj. \n    0.398 \n  \n  \n    AIC \n    1431.3 \n  \n  \n    BIC \n    1452.3 \n  \n  \n    Log.Lik. \n    −710.629 \n  \n  \n    F \n    110.905 \n  \n  \n    RMSE \n    1.00 \n  \n\n\n\n\n\n\n\\[\n\\begin{align}\n\\hat{y} & = \\beta_0 + \\beta_1 \\mbox{Treatment} + \\beta_2 \\mbox{Female} + \\beta_3 \\mbox{Treatment} \\cdot \\mbox{Female} \\\\\n& = \\beta_0 + (\\beta_1 + \\beta_3 \\mbox{Female}) \\mbox{Treatment} + \\beta_2 \\mbox{Female}.\n\\end{align}\n\\]\n\n男性のATE: \\(\\beta_1 + \\beta_3 \\cdot 0 = \\beta_1\\) = 0.951\n女性のATE: \\(\\beta_1 + \\beta_3 \\cdot 1 = \\beta_1 + \\beta_3\\) = 1.987"
  },
  {
    "objectID": "slide/intro_rct.html#因果推論の前提sutva",
    "href": "slide/intro_rct.html#因果推論の前提sutva",
    "title": "方法論特殊講義III",
    "section": "因果推論の前提:SUTVA",
    "text": "因果推論の前提:SUTVA\nStable Unit Treatment Value Assumption\n\n非干渉性処置の無分散性\n\n\n非干渉性: 他人の処置・統制有無が処置効果に影響を与えないこと\n\n例) AさんITEは\n\n例1) Bさんが統制群の場合は10、処置群の場合は5 \\(\\leftarrow\\)  \n例2) Bさんが統制群の場合も、処置群の場合も、5 \\(\\leftarrow\\)  \n\n\n\n\n\n\n\n例1\n\n\n\nAさんが統制群\nAさんが処置群\n\n\n\n\nBさんが統制群\n0\n10\n\n\nBさんが処置群\n15\n20\n\n\n\n\n\n\n例2\n\n\n\nAさんが統制群\nAさんが処置群\n\n\n\n\nBさんが統制群\n5\n10\n\n\nBさんが処置群\n15\n20\n\n\n\n\n\n\n\n\n\n処置の無分散性: 同じグループに属する対象は同じ処置を受けること\n\n手術の場合: 医者、設備、手順、環境など\n投票参加: 当日、期日前など\n\n\n\n\n\nサーベイ実験では問題にならない場合が多い\n\n実験室実験、フィールド実験の場合、「非干渉性」には気をつける\n例) 隣の人が見てるのとと私が見てるのが違いますが…?"
  },
  {
    "objectID": "slide/intro_rct.html#二重盲検法",
    "href": "slide/intro_rct.html#二重盲検法",
    "title": "方法論特殊講義III",
    "section": "二重盲検法",
    "text": "二重盲検法\n二重盲検法 (Double Blind Test):ある被験者がどのような処置を受けているかについて研究者と被験者両方において不明な状態で実験を行う\n\n二重盲検法を使えば以下の問題点に対処することが可能\n\nプラセボ効果 (placebo effect):偽薬が与えられても、薬だと信じ込む 事によって何らかの効果が生じる\nホーソン効果 (Hawthorne effect):自分が観察されていることを認知さ れることによって何らかの効果が生じる\n観察者効果 (observer/experimenter effect):研究者の期待により被験者へ の対応が異なったり、被験者がその期待に添えるように行動すること"
  },
  {
    "objectID": "slide/intro_rct.html#データ",
    "href": "slide/intro_rct.html#データ",
    "title": "方法論特殊講義III",
    "section": "データ",
    "text": "データ\nこれまで紹介した例題\n\n労働市場と人種差別: intro_data1.dta\n\nMarianne Bertrand and Sendhil Mullainathan. 2004. “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” American Economic Review. 94(4) pp. 991–1013\n\n処置効果の不均一性: intro_data2.csv\n\n架空データ\n\n\nLab Session用のデータ (第2講)\n\n社会的圧力と投票参加: intro_data3.csv\n\nAlan S. Gerber, Donald P. Green, and Christopher W. Larimer. 2008. “Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment.” American Political Science Review. 102(1) pp. 33–48"
  },
  {
    "objectID": "slide/intro_rct.html#バランスチェック-1",
    "href": "slide/intro_rct.html#バランスチェック-1",
    "title": "方法論特殊講義III",
    "section": "バランスチェック",
    "text": "バランスチェック\n処置が複数の場合、組み合わせごとに標準化差分を計算"
  },
  {
    "objectID": "slide/intro_rct.html#処置効果の可視化",
    "href": "slide/intro_rct.html#処置効果の可視化",
    "title": "方法論特殊講義III",
    "section": "処置効果の可視化",
    "text": "処置効果の可視化"
  },
  {
    "objectID": "slide/intro_rct.html#実習内容",
    "href": "slide/intro_rct.html#実習内容",
    "title": "方法論特殊講義III",
    "section": "実習内容",
    "text": "実習内容\n\nRの導入\n\nR + RStudioがインストールされていない場合、NIIオンライン分析システムを利用する。\n\nRの基礎（プロジェクト管理、データの読み込みなど）\n記述統計量の計算\nバランスチェック\n処置効果の推定\n処置効果の報告\n\n\n\n\nhttps://www.jaysong.net/kobe-ci"
  },
  {
    "objectID": "intro/filesystem.html",
    "href": "intro/filesystem.html",
    "title": "本講義のためのファイル管理術",
    "section": "",
    "text": "PC内に存在するほとんどのファイルは「名前.拡張子」と名付けられている1。名前の拡張子は.で区切られており、名前は英数字と_のみで構成することを推奨する（ファイル名に.が推奨されない理由の一つが名前と拡張子を区分する文字として使われるからだ）。ここで注目したいのはファイルの名前でなく、拡張子のことだ。拡張子とはファイルの特徴を示すものである。たとえば、拡張子が.htmlであれば、ウェブページ形式を意味し、.pngなら図、.pdfなら図・文書、.exeなら実行ファイル、.dmgならディスクイメージを意味する。ファイル名がFigure01.pngならFigure01という名の画像ファイルであることを意味する2。この拡張子によって、パソコンは当該ファイルをどのアプリケーションで開くかを判定する。.exeファイルをダブルクリックするとアプリケーションが立ち上がるし、.pdfファイルをダブルクリックするとPDFビュアーソフトが起動され、中身が表示される。\n　これは拡張子を変えると問題が生じ得ることを意味する。画像ファイルであるFigure01.pngのファイル名を動画ファイル拡張子であるMovie01.mp4に修正しても、そのファイルは動画ファイルにはならない。また、拡張子が.mp4になると、そのファイルを開く際、動画プレイヤーが起動されるが、ファイルの中身は画像ファイルのままなのでエラーが出る。したがって、拡張子は勝手に変えてはならない。たまに課題の結果物としてファイルを提出する際、自分の名前を入れたくてファイル名をXXXX.htmlからXXXX.html_Songへ変更して提出する場合がある。しかし、これは大きな間違いだ。もやはこのファイルはHTMLファイル（.html）でなく、未知のファイル形式（.html_Song）として認識され、ダブルクリックしてもPCはどのアプリケーションで開けば良いかが分からなくなる。ファイル名を修正するならXXXX_Song.htmlのように修正しよう3。\n　Rを用いたデータ分析の場面において頻繁に登場する拡張子は以下の通りである。ファイルの名前は大文字と小文字を区別するが、拡張子の場合、区別されないケースが多い。\n\n\n\n\n\n\n\n\n拡張子\n説明\n備考\n\n\n\n\n.R\nRスクリプトファイル\n\n\n\n.Rproj\nRプロジェクトファイル\n\n\n\n.Rmd\nR Markdownファイル\n\n\n\n.qmd\nQuartoファイル\nRMarkdownに似たようなもの\n\n\n.csv\n表形式ファイル\n業界標準のフォーマット\n\n\n.xlsx or .xls\n表形式ファイル\nExcelで使うフォーマット\n\n\n.dta\n表形式ファイル\nStataで使うフォーマット\n\n\n.sav\n表形式ファイル\nSPSSで使うフォーマット\n\n\n.html\nウェブページファイル\nR Markdown/QuartoをKnitした場合に得られる\n\n\n.png\n画像ファイル\n\n\n\n.pdf\n画像/文書ファイル\n画像にも文書にもなるファイル"
  },
  {
    "objectID": "intro/filesystem.html#ファイルシステム",
    "href": "intro/filesystem.html#ファイルシステム",
    "title": "本講義のためのファイル管理術",
    "section": "2 ファイルシステム",
    "text": "2 ファイルシステム\n　R上でファイルを入出力を行うためにはファイルシステム（file system）を理解する必要がある。\n\n2.1 ファイルの入出力\n　そもそも、ファイルの「入出力」とは何だろうか。これはコンピューターの構造に関わる話なので極めて難しい内容であるが、我々のような消費者（end user）側から見れば、ファイルの入力（input）とは、いわゆるファイルの読み込みを意味し、多くの場合、表形式のデータ（.csv、.xlsxなど）をR上に読み込む作業を意味する。また、ファイルの出力（output）とは、いわゆるファイルの保存。たとえば、作成したスクリプトを.R形式で保存したり、クリーニングしたデータを.csv形式で保存したり、作成した図を.png、.pdf形式で保存したらい、作成した文書を.pdf、.html形式で保存したらいすることがファイルの出力だ。\n\n\n2.2 パスとは\n　ファイルを読み込む場合はファイル名を指定する必要がある。また、ファイルを書き出す場合もファイルに名付ける必要がある。そしてファイル名は名前.拡張子である。ただし、これらのファイルは全て一箇所に集まっているわけではない。もし、全てのファイルが一箇所に集まっていると、必要なファイルを探すのは非常に難しい。通常、PC内には数万のファイルがある。これらのファイルから必要なファイルを探すのは至難の業だろう。したがって、これらのファイルをいくつかの部屋に分けて保管し、この部屋のことをフォルダー（folder）、またはディレクトリ（directory）と呼ぶ（ここでは「フォルダー」と呼ぶとする）。\n　これは我々が住んでいる居住地の「住所」と同じ概念だ。もし、日本に「都道府県」も「市区町村」も「〜丁目、〜番、〜号」という概念がないとしよう。ここでAmazonで魚を購入し、受取先を指定する場合はどうすれば良いだろうか。日本に人が数十人しか住んでいないのであれば、「XXXさんの家」と書くだけで十分かも知れない。しかし、日本には1億人以上の人がある。「ソンさんの家」と書いても届かない。届いたとしても数年、あるいは数十年後に魚の化石の状態で届くかも知れない。そもそも日本に「ソンさん」はこの授業の担当教員以外にもいくらでもいる（ちなみに송（Song; 宋・松）さんも、손（Sohn; 孫）さんも、성（Seong/Sung; 成・星）さんも、선（Sun/Seon; 宣）さんも韓国語では発音が全く別だが、日本ではソンさんになってしまう。）。それぞれの家を何かの区域内に位置づけないとモノが届くまで数年かかってしまう。そこで必要なのが住所だ。「東京都千代田区永田町1丁目7番1号の田中さん」は「東京都」、「千代田区」、「永田町」、「1丁目」、「7番」、「1号」、「田中さん」で構成される。これをファイルシステムに例えると、東京都というフォルダーの中に千代田区というフォルダーがあり、その中には永田町というフォルダー、その中に1丁目といるファルダー、…が存在する。むろん、一つのフォルダーには複数のフォルダーが存在する可能性もある。東京都のフォルダーには千代田区以外にも大田区、中野区、文京区、葛飾区といった複数のフォルダーがあり、千代田区の中にも複数のフォルダーがある。最後の「田中さん」は受け取る人、コンピューターでいるファイル名である。\n　この住所のことをコンピューターではパス（path）と呼ぶ。それぞれのフォルダーは/で区切られる（macOS/Linuxでは/、Windowsでは\\または￥; NIIオンライン分析システムはLinuxベースである）。先ほどの住所の例だと、東京都/千代田区/永田町/1丁目/7番/1号/田中さんとなる（macOS/Linuxの場合）。左に行くほど上位のフォルダーとなり、最後のものはファイル名である。ただし、コンピュターではパスの最初に/を付ける。Windowsなら主にC:\\でスタートし、C:\\東京都\\千代田区\\永田町\\1丁目\\7番\\1号\\田中さんとなる。\n　たとえば、以下のような構造でファイルが保存されているとしよう。拡張子が付いているものはファイル、それ以外はフォルダー、1行目の.は最上位フォルダーである。\n\n\n                      levelName\n1  .                           \n2   ¦--Day01                   \n3   ¦   ¦--Day01.Rproj         \n4   ¦   ¦--Script01.R          \n5   ¦   ¦--Script02.R          \n6   ¦   ¦--Data                \n7   ¦   ¦   ¦--raw_data.csv    \n8   ¦   ¦   °--cleaned_data.csv\n9   ¦   °--Figs                \n10  ¦       ¦--Figure01.png    \n11  ¦       °--Figure02.png    \n12  °--Day02                   \n13      ¦--Day02.Rproj         \n14      ¦--Script01.R          \n15      ¦--Document01.Rmd      \n16      ¦--Document01.html     \n17      ¦--Data                \n18      ¦   °--my_data.csv     \n19      °--Figs                \n20          ¦--Old             \n21          ¦   ¦--Figure01.pdf\n22          ¦   ¦--Figure02.pdf\n23          ¦   °--Figure03.pdf\n24          °--New             \n25              °--Figure01.png\n\n\n　ここでread_csv()関数を使ってDay01フォルダー内のDataフォルダー内のraw_data.csvを読み込む場合はread_csv(\"/Day01/Data/raw_data.csv\")となる。また、ggsave()を用いて、Day02内のFigs内のNew内にFigure02.pngという名で図を保存する場合は、ggsave(filename = \"/Day02/Figs/New/Figure02.png\", ...)と入力する必要がある。しかし、通常、パスを指定する際に、/（WindownならC:\\）から始めることは滅多にない。それは「作業フォルダーはパスで省略可能」だからだ。"
  },
  {
    "objectID": "intro/filesystem.html#rstudioのプロジェクト機能",
    "href": "intro/filesystem.html#rstudioのプロジェクト機能",
    "title": "本講義のためのファイル管理術",
    "section": "3 RStudioのプロジェクト機能",
    "text": "3 RStudioのプロジェクト機能\n　Rでファイルを入出力する時に頭に入れておくべき概念として「作業フォルダー（working folder/working directory）」がある。通常、Rの作業フォルダーはmacOSだと/Users/ユーザー名/、NIIオンライン分析システムだと/home/joyvan/が作業フォルダーだ。そして、パスを指定する場合、作業フォルダーは省略することができる。つまり、現在の作業フォルダーが/home/joyvan/なら\"/home/joyvan/Day01/Data/raw_data.csv\"は\"Day01/Data/raw_data.csv\"に、\"/home/joyvan/Day02/Figs/New/Figure02.png\"は\"Day02/Figs/New/Figure02.png\"に省略される。我々が郵便局で手紙を送る際、住所にわざわざ「日本国」と書かないものと同じである。作業フォルダーはRコンソール上でgetwd()と入力すれば出力される。macOSなら/Users/ユーザー名、NIIオンライン分析システムなら/home/joyvanと出力される。\n\n\n\n\n\n\n最上位フォルダーの話\n\n\n\n　macOSとLinuxに限定した話であるが、最上位フォルダーは/であり、これはシステム上の最上位フォルダーである。個人レベルの最上位フォルダーはmacOSだと/Users/ユーザー名、NIIオンライン分析システムだと/home/joyvanである。そして、この個人レベルの最上位フォルダーは~/と表記することができる。Rコンソールでgetwd()を入力し、以上のように出力されれば個人レベルの最上位フォルダー（~/）が作業フォルダーになっている理解しても良い。ちなみに、WindowsはC:\\がシステム上の最上位フォルダーである。\n\n\n　もし、自分がこれから全ての作業をDay03という名のフォルダー内で完結するとする。つまり、保存するスクリプト（たとえば、Script.R）もDay03に保存し、図（たとえば、FigureA.png）はDay03のFigsフォルダーに、読み込む表形式データ（たとえば、Day03_Data.csv）もDay03の中のDataフォルダーに入れるとする。この場合、それぞれのファイルのパスはDay03/Script.R、Day03/Figs/FigureA.png、Day03/Data/Day03_Data.csvとなる（作業フォルダーが~/であるため、~/は省略可能）。全ての作業が同じフォルダー（とその下位フォルダー）内で行うとしたら、パス名にDay03も不要な気がする。そこで必要なのがRStudioのプロジェクト機能である。\n　RStudioでDay03というプロジェクトを作成すると、Day03フォルダーが自動的に生成され、Day03.Rprojというファイルも生成される。プロジェクトを最上位フォルダーに作成したのであれば、~/Day03/Day03.Rprojファイルが生成されるのである。ここでRStudioのFile > Open ProjectでこのDay03.Rprojファイルを開くとRStudio画面の右上にプロジェクト名が表示され、作業フォルダーがDay03.Rprojが保存されているフォルダー、つまり~/Day03へ変更される（プロジェクトが開かれていな場合は「Project: (None)」と表示される）。実際、NIIオンライン分析システムでXXXという名前のプロジェクトを生成し、そのプロジェクトを開けば作業フォルダーは/home/joyvan/XXX/（=~/XXX/）になる（getwd()で確認可能）。これは大変便利な機能である。なぜなら作業フォルダーまでのパスは全て省略可能だからだ。これまでDay03/Script.R、Day03/Figs/FigureA.png、Day03/Data/Day03_Data.csvだったパスが、それぞれScript.R、Figs/FigureA.png、Data/Day03_Data.csvになる。\n　また、何らかの理由でDay03フォルダーの名前をDay05に変更したとしよう。もし、プロジェクト機能を使っていないのであれば、パスのDay03を全てDay05に変更する必要がある。しかし、プロジェクト機能を使っているのであれば、.Rprojファイルが存在するフォルダーが作業フォルダーになるため、そもそもパスにDay03は存在しない。つまり、修正不要ということだ。ちなみに、プロジェクトを一旦作成したら、そのプロジェクトのフォルダー名や.Rprojファイルの名前は自由に修正しても良いし、フォルダー名と.Rprojファイルの名前が一致しなくても良い。\n　以上の内容をまた住所と郵便の話で例えるとしよう。社内でも郵便物の行き来は頻繁に行われる。とりわけ面積が広く、キャンパスも複数ある大学なら尚更だ。たとえば、「兵庫県神戸市灘区六甲台町2-1 神戸大学 ラーメン研究科」の宋が「兵庫県神戸市灘区六甲台町2-1 神戸大学 さつまいも研究科」の重村に郵便を送る場合、同じ大学であるにも関わらず、住所を全て書くのは面倒なことであろう。ここで神戸大学専用の郵便局を作れば問題は解決される（「学内便」と呼ばれる）。そうすれば差出人は「ラーメン研究科　宋」、受取人は「さつまいも研究科　重村」と書くだけで郵便物は届く。つまり、「兵庫県神戸市灘区六甲台町2-1 神戸大学」は省略できる（同じ市区町村内の引っ越した際、転入・転出届けの住所欄に市区町村名までは省略できるのと同じ）。また、神戸大学がなぜかキャンパスを北海道に移転した場合を考えてみよう。学内便がなければ、郵便物の住所を全て「北海道〜」に変えなければならない。しかし、学内便が存在すればこれまで使ってきた「ラーメン研究科　宋」という表記は有効であろう。\n　このようにRStudioのプロジェクト機能は必須といっても過言ではない。簡単な計算目的として使う場合は問題ないが、何かの分析をする時、授業の実習時、課題時には必ずRStudioの右上が「Project: (None)」になっていないことを確認しよう。\n\n\n\n\n\n\n絶対パスと相対パス\n\n\n\n　これまで紹介したパスの書き方で/（WindowsならC:\\）から始まるパスは、絶対パス（absolute path）またはフルパス（full path）と呼ばれる。これはファイル名を最上位フォルダーを起点に書く方法である。一方、/（WindowsならC:\\）で始まらないパスは相対パス（relative path）呼ばれ、ファイル名を作業フォルダーを起点に書く方法である。"
  },
  {
    "objectID": "intro/filesystem.html#本講義でおすすめするフォルダー構造",
    "href": "intro/filesystem.html#本講義でおすすめするフォルダー構造",
    "title": "本講義のためのファイル管理術",
    "section": "4 本講義でおすすめするフォルダー構造",
    "text": "4 本講義でおすすめするフォルダー構造\n　プロジェクトを作成すれば、プロジェクトフォルダー内に以下のようなフォルダーを作成しよう。\n\n表形式のデータを読み込んだり、保存したりするのであればDataフォルダーをプロジェクトフォルダー内に作成する。\n\n.csv、.xlsx、.sav、.dtaのような表形式ファイルはDataフォルダーに入れる。\nデータを加工し、保存する場合はData/ファイル名.csvなどと指定する。\n\n図を作成し、保存する予定があれば、Figsフォルダーをプロジェクトフォルダー内に作成する。\n\n図を保存する場合、ファイルのパスをFigs/ファイル名.pngやFigs/ファイル名.pdfとする。\n\n\n　よく分からない場合はとりあえずプロジェクトフォルダー内にDataとFigsというフォルダーを作っておこう。ただし、.R、.Rmdなどコードファイルはプロジェクトフォルダーの下位フォルダーに入れず、プロジェクトフォルダーの直に入れよう4。この場合、R Markdown / Quartoで作成された文書（.html、.pdfなど）は.Rmdや.qmdファイルと同じフォルダーに保存される（別途の設定をすれば別フォルダーに保存することも可能だが、そこまではしなくても良い）。"
  },
  {
    "objectID": "intro/filesystem.html#参考",
    "href": "intro/filesystem.html#参考",
    "title": "本講義のためのファイル管理術",
    "section": "5 参考",
    "text": "5 参考\n　以下の内容も合わせて読むことを強く推奨する。\n\n宋財泫・矢内勇生.『私たちのR』の第6.1章: コンピュータの基礎知識"
  },
  {
    "objectID": "intro/file.html",
    "href": "intro/file.html",
    "title": "ファイル管理",
    "section": "",
    "text": "フォルダー/ファイルの管理はJupyterHub内でも、RStudio内でもできるが、ここではRStudio側で管理する方法を紹介する。RStudioを起動し、作業するプロジェクトを開き、Filesペインを確認しよう。RStudioを経由したフォルダー/ファイルの管理は全てFilesペイン上で行われる。"
  },
  {
    "objectID": "intro/file.html#フォルダーの管理",
    "href": "intro/file.html#フォルダーの管理",
    "title": "ファイル管理",
    "section": "1 フォルダーの管理",
    "text": "1 フォルダーの管理\n　講義、または課題ごとのプロジェクトを作ったら、JupyterHubにプロジェクトのフォルダーが生成される。各プロジェクトごとにRスクリプト、Markdownファイル、出力物（図、文書など）が管理できるが、プロジェクト内のファイルが多くなる可能性もある。この場合、プロジェクト・フォルダー内に更に下位フォルダーを作成し、ファイルを管理した方が望ましい。\n\n1.1 フォルダーの作成\n手順1: 現在、Filesペインで表示されているフォルダーがプロジェクトの最上位フォルダーであることを確認する。「Home > プロジェクト名」と表示されていれば問題ない。\n\n\n\n\n\n\n\n\n\n手順2: New Folderをクリックする。\n\n\n\n\n\n\n\n\n\n手順3: 作成するフォルダーの名前を入力する。ここではデータなどを集めておくDataという名のフォルダーを作成する。\n\n\n\n\n\n\n\n\n\n手順4: 正しくフォルダーが作成されているかを確認する。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nフォルダー名の付け方\n\n\n\nフォルダー名にはローマ字、数字のみを使おう。スペースもなるべく使わず、空白を入れたい場合はスペースの代わりにアンダースコア（_）を使おう。\n\n\n\n\n\n\n\n\nフォルダー in フォルダー\n\n\n\nフォルダー内に更にフォルダーを作成することもできる。一つのフォルダー内にファイルが多すぎる場合、更にフォルダー分けして管理した方が効率的だろう。\n\n\n\n\n1.2 フォルダーの削除\n\n\n\n\n\n\nフォルダーの削除は慎重に!\n\n\n\nフォルダーを削除するとフォルダー内のファイルも全て削除される。削除する前には慎重にフォルダー内のファイルを確認しておくこと。\n\n\n手順1: 削除するフォルダーの左にチェックを付け、Deleteをクリックする。\n\n\n\n\n\n\n\n\n\n手順2: Yesをクリックする。"
  },
  {
    "objectID": "intro/file.html#ファイルの管理",
    "href": "intro/file.html#ファイルの管理",
    "title": "ファイル管理",
    "section": "2 ファイルの管理",
    "text": "2 ファイルの管理\n　分析に使用するデータセットを自分のPCにダウンロードしてもそのままNIIオンライン分析システムで使うことはできない。NIIオンライン分析システムで使用するためには、ファイルをアップロードする必要がある。これはデータだけでなく、本講義の課題用ファイルについても同様である。\n\n2.1 ファイルのアップロード\n手順1: ファイルをアップロードしたいフォルダーへ移動する。\n\n下位フォルダーへの移動: フォルダー名をクリックする。\n上位フォルダーへの移動: 「..」をクリックするか、パスが表示されているバーで移動先をクリックする。\n\n手順2: ファイルのアップロード先が正しいかを確認し、Uploadをクリックする。\n\n以下の例はHomework_01プロジェクト・フォルダー内のDataフォルダーがアップロード先である。\n\n\n\n\n\n\n\n\n\n\n手順3: File to upload:でアップロードしたいファイルを選択する。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n複数のファイルをアップロードしたい場合\n\n\n\nRStudio上でファイルは一度の一つしかアップロードできない。複数のファイルを同時にアップロードしたい場合は、この作業を繰り返すか、JupyterHubのホーム画面でアップロードする必要がある。\n\n\n手順4: アップロードするファイルをダブルクリックする。\n\n以下ではPrev_Vote.csvというファイルをアップロードする例である。\n\n\n\n\n\n\n\n\n\n\n手順5: OKをクリックする。\n\n\n\n\n\n\n\n\n\n手順6: 正しくファイルがアップロードされているかを確認する。\n\n\n\n\n\n\n\n\n\n\n\n2.2 ファイルのダウンロード\n　作成した図表をLaTex/Microsoft Word/Powerpoint/Pages/Keynoteなどで使うためには、その図表を自分のPCにダウンロードする必要がある。同様に、課題の出力物をLMSに提出するためにも、出力物を一旦自分のPCにダウンロードしてから提出する必要がある。\n\n\n\n\n\n\n複数ファイルのダウンロード\n\n\n\nアップロードは一度ごとに一つのファイルしかアップロードできないが、ダウンロードは複数のファイルを同時にダウンロードできる。ただし、個別のファイルがダウンロードされるのではなく、一つのファイルととして圧縮（zip形式）されてからダウンロードされる。ダウンロード後はファイルを解凍すること。\n\n\n手順1: ダウンロードするファイル名の左にチェックを付ける。\n\n以下ではMicro_HW01.htmlというファイルをダウンロードする例である。\n\n\n\n\n\n\n\n\n\n\n手順2: More > Export…をクリックする。\n\n\n\n\n\n\n\n\n\n手順3: Downloadをクリックする。"
  },
  {
    "objectID": "intro/install.html",
    "href": "intro/install.html",
    "title": "Rの導入",
    "section": "",
    "text": "本講義の実習はNIIオンライン分析システムの使用を推奨する。自分のPCにインストールしたR + RStudio、RStuio.cloud、大学PCにインストールされているR + RStudioなどの使用を妨げないが、本ページの資料と同じ結果が得られることは保証しない。また、実習・分析中に起きた不具合についても授業中には対応しない。\n　以下ではNIIオンライン分析システムを用いたRおよびRStudioの導入方法について解説する。\n\n\n\n\n\n\n注意!!!\n\n\n\n初期設定は国立情報学研究所（以下、NII）のサーバーに自分の作業用スペースを借りる作業である。つまり、初期設定を繰り返すことはNIIのサーバー（のスペース）をいくつも借りることとなり、サーバーを圧迫してしまう可能性がある。したがって、初期設定は授業全体を通じて1回のみ実行すること。\n\n\n手順1: OpenIdPアカウントを登録する。\n\n以下のページの手順1〜9を参照すること。\n\nhttps://meatwiki.nii.ac.jp/confluence/pages/viewpage.action?pageId=67620540\n\n登録するメールアドレスは必ずac.jp、go.jpで終わる大学アドレスを使うこと。\n\n手順2: 以下のアドレスにアクセスする。\n\nhttps://binder.cs.rcos.nii.ac.jp/v2/gh/JaehyunSong/Binder_R/HEAD\n\n手順2: 所属機関に「OpenIdP」を入力・選択し、「選択」をクリックする。\n\n\n\n\n\n\n\n\n\n手順3: OpenIdPのアカウント名とパスワードを入力する。\n\nなにか同意を求める画面が表示される場合、そのまま「同意します」をクリックする。\n\n\n\n\n\n\n\n\n\n\n手順4: 以下のような画面が表示されたらしばらく待つ。\n\n\n\n\n\n\n\n\n\n手順5: 以下のような画面が表示されたら初期設定は完了\n\n\n\n\n\n\n\n\n\n手順6: 初期設定が終わったら、すぐRおよびRStudioが利用可能だが、ここでは一旦右上の「Logout」をクリックし、タブ (or ウィンドウ) を閉じる。"
  },
  {
    "objectID": "intro/install.html#niiオンライン分析システムの起動",
    "href": "intro/install.html#niiオンライン分析システムの起動",
    "title": "Rの導入",
    "section": "2 NIIオンライン分析システムの起動",
    "text": "2 NIIオンライン分析システムの起動\n初期設定が終わったら、今後、以下の手順でNIIオンライン分析システムを起動する。\n手順1: 以下のアドレスにアクセスするか、本ページの右上にある右上の  ボタンをクリックする（右クリックし、新しいタブ or ウィンドウで開くことを推奨する）。\n\nhttps://jupyter.cs.rcos.nii.ac.jp/\n\n手順2: 必要に応じて認証を行う（初期設定の手順2, 3, 4と同じ）。\n手順3: サーバーリストが表示される。URL列のアドレスをクリックする。\n\n参考) 初期設定を1回のみ行ったら1行のみ表示されるため混同することはないが、個人利用などを目的に初期設定を複数回行った場合は2行以上が表示されるだろう。本講義に使うサーバーのURLをクリックすること。\n\n\n\n\n\n\n\n\n\n\n手順4: 以下のような画面が表示されたらNIIオンライン分析システムの起動完了である。この画面を今後、「JupyterHub（ジュピターハブ）のホーム画面」と呼ぶ。"
  },
  {
    "objectID": "intro/install.html#rstudioの起動",
    "href": "intro/install.html#rstudioの起動",
    "title": "Rの導入",
    "section": "3 RStudioの起動",
    "text": "3 RStudioの起動\n手順1: JupyterHubのホーム画面の右上の「New」をクリックし、「RStudio」をクリックする。\n\n\n\n\n\n\n\n\n\n手順2: 以下の画面が表示されたら、RStudioの起動完了である（RStudioの見栄は初期状態の場合、白ベースである）。"
  },
  {
    "objectID": "intro/install.html#rstudioの終了",
    "href": "intro/install.html#rstudioの終了",
    "title": "Rの導入",
    "section": "4 RStudioの終了",
    "text": "4 RStudioの終了\n手順1: RStudio画面右上のオレンジ色のボタンをクリックする。\n\n\n\n\n\n\n\n\n\n手順2: 以下のダイアログが表示されたらタブ、またはウィンドウを閉じる。"
  },
  {
    "objectID": "intro/packages.html",
    "href": "intro/packages.html",
    "title": "パッケージ",
    "section": "",
    "text": "Rには数万以上のパッケージが存在し、Rをインストールするだけでも数十のパッケージが自動的にインストールされる。しかし、データ分析/ハンドリング/可視化の手法は日々発展しており、R内蔵パッケージだけでは対応が難しい (できないわけではない)。したがって、必要に応じて新しいパッケージを導入する必要があるが、パッケージのインストールするにはConsoleペインに以下のように入力する。\ninstall.packages(\"インストールするパッケージ名\")\n　前期の「ミクロ政治データ分析実習」では{tidyverse}パッケージのみ使用する予定である。ただし、本講義ようにセッティングされた環境を導入する場合、{tidyverse}は既に導入済みであるため、以下のコードは実行しなくても良い。"
  },
  {
    "objectID": "intro/packages.html#アップデート",
    "href": "intro/packages.html#アップデート",
    "title": "パッケージ",
    "section": "2 アップデート",
    "text": "2 アップデート\n　特定のパッケージをアップデートする方法はインストールと同じだが、一つ一つのパッケージが全て最新バージョンかどうかを確認するのは大変である。また、久々のアップデートで数十個のパッケージをアップデートする必要があるケースもあろう。この場合、RStudioの内蔵機能を使えば一瞬で更新可能なパッケージのリスト化、インストールができる。\n手順1: PackagesペインのUpdateをクリックする。\n\n\n\n\n\n\n\n\n\n手順2: アップデートしたいパッケージの左にチェックを付けるか、左下のSelect Allをクリックし、右下のInstall Updatesをクリックする。\n\n\n\n\n\n\n\n\n\n　インストール、またはアップデートの際、以下のようなメッセージが出力される場合がある。\n  There are binary versions available but the source versions\n  are later:\n      binary source needs_compilation\nterra 1.5-17 1.5-21              TRUE\nyaml   2.2.2  2.3.4              TRUE\n\nDo you want to install from sources the packages which need compilation? (Yes/no/cancel)\n　この場合、Consoleペイン上でYes、no、cancelのいずれかを入力してReturnキー (Enterキー)を押す必要がある。大文字と小文字は区別すること。どうしても最新のパッケージが欲しい場合はYesを入力すれば良いが、インストールに時間がかかる場合がある。一方、noを入力した場合は、若干古いバージョンがインストールされるが、インストールに必要な時間が短いため、基本的にはnoでも問題ないだろう。cancelを入力した場合はアップデートが全てキャンセルされる。"
  },
  {
    "objectID": "intro/packages.html#教科書",
    "href": "intro/packages.html#教科書",
    "title": "パッケージ",
    "section": "3 教科書",
    "text": "3 教科書\n『私たちのR』の第5章「Rパッケージ」: https://www.jaysong.net/RBook/packages.html"
  },
  {
    "objectID": "intro/project.html",
    "href": "intro/project.html",
    "title": "プロジェクト管理",
    "section": "",
    "text": "手順1: File > New Project…をクリックする。\n\n\n\n\n\n\n\n\n\n手順2: New Directoryをクリックする。\n\n\n\n\n\n\n\n\n\n手順3: New Projectをクリックする。\n\n\n\n\n\n\n\n\n\n手順4: Directory name:にプロジェクト名を入力し、Create Projectをクリックする。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n注意: プロジェクト名の付け方\n\n\n\n　プロジェクト名にはローマ字、数字のみを使おう。つまり、日本語、中国語、韓国語、全角文字、スペースはなるべく使わないこと。空白を入れたい場合はスペースの代わりにアンダースコア（_）を使おう。"
  },
  {
    "objectID": "intro/project.html#プロジェクトの開き方",
    "href": "intro/project.html#プロジェクトの開き方",
    "title": "プロジェクト管理",
    "section": "2 プロジェクトの開き方",
    "text": "2 プロジェクトの開き方\n　プロジェクトを作成すれば、自動的に出来たてのプロジェクトが開かれる。しかし、NIIオンライン分析システムから一旦ログアウトし、改めてRStudioを起動する場合、プロジェクトをロードする必要がある。\n手順1: File > Open Project…をクリックする。\n\n\n\n\n\n\n\n\n\n手順2: プロジェクト・フォルダー名をダブルクリックする。\n\n\n\n\n\n\n\n\n\n手順3: .Rprojで終わるファイルをダブルクリックする。\n\n\n\n\n\n\n\n\n\nプロジェクトが正しくロードされている場合、RStudioの右上にプロジェクト名が表示される。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n常にプロジェクト機能を使おう!\n\n\n\n　RStudionの右上のプロジェクト名表示が「Project: (None)」になっていることは、現在プロジェクトを開いていないことを意味する。簡単な計算機として使う目的以外（つまり、授業中の実習や課題）は必ずプロジェクト機能を使おう。"
  },
  {
    "objectID": "intro/rstudio.html",
    "href": "intro/rstudio.html",
    "title": "RStudioの設定",
    "section": "",
    "text": "RStudioはそのままでも使えるが、少しカスタマイズするとより使い勝手が良くなる。RStudioのカスタマイズ画面はTools > Global Optionsをクリックすることで表示される。\n以下の設定はNIIオンライン分析システムで使用可能なRStudio最新版 (RStudio Server 2021.09.1+372)の設定であり、宋の設定と同じである。"
  },
  {
    "objectID": "intro/rstudio.html#general",
    "href": "intro/rstudio.html#general",
    "title": "RStudioの設定",
    "section": "1 General",
    "text": "1 General\n\n\n\n\n\n\nRestore .RData into workspace at startupのチェックを消す。\nSave workspace to .RData on exit:をNeverに変更する。\nAlways save history (even when not saving .RData)のチェックを消す。"
  },
  {
    "objectID": "intro/rstudio.html#code",
    "href": "intro/rstudio.html#code",
    "title": "RStudioの設定",
    "section": "2 Code",
    "text": "2 Code\n\n2.1 Editingタブ\n\n\n\n\n\n\nInsert spaces for tabのチェックを付ける。\nTab widthは2、または4を指定する。\nAuto-detect code indentationのチェックを付ける。\nInsert matching parens/quotesのチェックを付ける。\nAuto-indent code after pasteのチェックを付ける。\nVertically align arguments in auto-indentのチェックを付ける。\nAlways save R scripts before sourcingのチェックを付ける。\nCtrl + Return executes:をMulti-line R statementに変更する。\n\n\n\n2.2 Displayタブ\n\n\n\n\n\n\nHighlight selected wordのチェックを付ける。\nHighlight selected lineのチェックを付ける。\nShow line numbersのチェックを付ける。\nShow syntax highlighting in console inputのチェックを付ける。\nHighlight R function callsのチェックを付ける。\nRainbow parenthesesのチェックを付ける。\n\n\n\n2.3 Savingタブ\n\n\n\n\n\n\nDefault text encoding:のChangeをクリックし、UTF-8を選択する。\n\n\n\n2.4 Completionタブ\n\n\n\n\n\n\nShow code completion:をAutomaticallyに変更する。\nAllow automatic completions in consoleのチェックを付ける。\nInsert parentheses after function completionsのチェックを付ける。\nShow help tooltip after function completionsのチェックを付ける。\nInsert spaces around equals for argument completionsのチェックを付ける。\nUse tab for autocompletionのチェックを付ける。"
  },
  {
    "objectID": "intro/rstudio.html#console",
    "href": "intro/rstudio.html#console",
    "title": "RStudioの設定",
    "section": "3 Console",
    "text": "3 Console\n\n\n\n\n\n\nShow syntax highlighting in console inputのチェックを付ける。"
  },
  {
    "objectID": "intro/rstudio.html#appearance",
    "href": "intro/rstudio.html#appearance",
    "title": "RStudioの設定",
    "section": "4 Appearance",
    "text": "4 Appearance\n\n\n\n\n\n\n自分の好みのものを選択する。ただし、小さすぎる文字サイズ (font size) は推奨しない。目に優しくないだけでなく、誤字脱字が見つけにくくなる。"
  },
  {
    "objectID": "intro/rstudio.html#pane-layout",
    "href": "intro/rstudio.html#pane-layout",
    "title": "RStudioの設定",
    "section": "5 Pane Layout",
    "text": "5 Pane Layout\n\n\n\n\n\n\n左上: Source\n右上: Console\n左下: 全てチェックを消す。\n左下: 全てチェックを付ける。"
  },
  {
    "objectID": "intro/rstudio.html#r-markdown",
    "href": "intro/rstudio.html#r-markdown",
    "title": "RStudioの設定",
    "section": "6 R Markdown",
    "text": "6 R Markdown\n\n\n\n\n\n\nShow output preview in:をViewer Paneに変更する。\nShow output inline for all R Markdown documentsのチェックを消す。\n\n設定が終わったら右下のOK、またはApplyをクリックする。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Causal Inference@Kobe",
    "section": "",
    "text": "アイコン説明\n\n\n\n\n：NIIオンライン分析システムの起動\n\n右クリックし、新しいタブ（or ウィンドウ）で開いてください。\n初期設定が必要です。初期設定の方法はRの使い方 > Rの導入を参照してください。\n\n：Rの教科書（『私たちのR』）\n：本ウェブサイト内の検索\n\n\n\n\n\n\n\n\n\nページ情報\n\n\n\n\n最終更新日: 2022年07月28日\n開発環境\n\nmacOS 12.4 “Monterey”\nFirefox 102.0.0\nR version 4.2.1 (2022-06-23)\nRStudio 2022.02.3+492 “Prairie Trillium”\nQuarto 1.0.36\nR package {quarto} 1.2\n\n本サポートページのレポジトリ"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "本講義について",
    "section": "",
    "text": "科目名: 計量政治学方法論 I (実証分析と方法)\n講師: 宋財泫 (ソン ジェヒョン)\n所属: 関西大学総合情報学部\n\nE-mail: song [at] kansai-u.ac.jp\nHomepage: https://www.jaysong.net\n\n時間: 2021年9月1,2,3,13,14日2∼4限目\n教室: フロンティア館 304 教室"
  },
  {
    "objectID": "syllabus.html#授業の内容",
    "href": "syllabus.html#授業の内容",
    "title": "本講義について",
    "section": "授業の内容",
    "text": "授業の内容\n　本講義は、近年政治学において関心が高まっている「因果推論」を行うための諸手段を 理解・習得することを目的とする。最初に、最良の因果推論とも称される RCT(ランダム 化比較試験)を説明し、RCT が不可能な際の手法としてマッチング、回帰不連続デザイ ン、差分の差などを紹介する。"
  },
  {
    "objectID": "syllabus.html#評価",
    "href": "syllabus.html#評価",
    "title": "本講義について",
    "section": "評価",
    "text": "評価\n\n授業貢献度 30%\n\n授業への参加度、質問など\n\n期末レポート 70%\n\n期末レポートの内容は初回の授業で紹介する"
  },
  {
    "objectID": "syllabus.html#履修上の注意",
    "href": "syllabus.html#履修上の注意",
    "title": "本講義について",
    "section": "履修上の注意",
    "text": "履修上の注意\n　統計学に関する基礎知識が必要である。目安は母平均の差の検定、および線形回帰分析が理解でき、統計ソフトウェアで実行・解釈が可能なレベルである。\n　本講義における共通言語はRである。Rの使い方に関しては既にインターネット上に膨大な情報がある。宋と矢内勇生(高知工科大学)が執筆中の以下の資料(無料で閲覧可能)を参照することも1つの選択肢である。\n\n宋財泫・矢内勇生. 『私たちの R: ベストプラクティスの探究』(web-book)\n\nRの導入方法は講義中、宋が解説する。\n\n\n　統計学および定量的分析、Rの使い方については以下の書籍を講義開始日までに読んで おくことを強く推奨する。\n\n浅野正彦・矢内勇生. 2019『Rによる計量政治学』オーム社.\n\n　R スクリプト作成の際、{tidyverse} というパッケージ群を積極的に活用する。この パッケージには {dplyr}、{ggplot2} などのパッケージが含まれている。各パッケージの 使い方を習得するには以下の教材を推奨する。\n\nWickham, Hadley and Grolemund, Garrett. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, O’Reilly. (邦訳あり/原著はインターネットから無料で閲覧可)\n松村優哉・湯谷啓明・紀ノ定保礼・前田和寛 . 2021. 『改訂2版 Rユーザのための RStudio[実践] 入門—tidyverseによるモダンな分析フローの世界—』技術評論社."
  },
  {
    "objectID": "syllabus.html#教科書参考書",
    "href": "syllabus.html#教科書参考書",
    "title": "本講義について",
    "section": "教科書・参考書",
    "text": "教科書・参考書\n以下は本書の内容を(一部)カバーする書籍の目録である。必ずしも購入する必要はないが、予習・復習において適宜参照することを推奨する。\n\n因果推論の理論と実例\n\n【AP 2008】 Angrist, Joahua D., and Jorn-steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.\n\n『「ほとんど無害」な計量経済学―応用経済学のための実証分析ガイド』 (翻訳はかなり有害)\n\n【AP 2014】 Angrist, Joahua D., and Jorn-steffen Pischke. 2014. Mastering ’Metrics: The Path from Cause to Effect. Princeton University Press.\n【森田 2014】 森田果. 2014.『実証分析入門—データから「因果関係」を読み解く作法』日本評論社.\n【中室・津川 2017】 中室牧子・津川友介. 2017.『「原因と結果」の経済学—データから真実を見抜く思考法』ダイヤモンド社.\n【伊藤 2017】 伊藤公一郎. 2017.『データ分析の力—因果関係に迫る思考法』光文社新書.\n【松林 2021】 松林哲也. 2021.『政治学と因果推論』岩波書店.\n\n理論+R\n\n【星野・田中 2016】 星野匡郎・田中久稔. 2016.『Rによる実証分析—回帰分析から因果分析へ—』オーム社.\n【安井 2020】 安井翔太. 2020. 『効果検証入門—正しい比較のための因果推論/計量経済学の基礎』技術評論社.\n【Cunningham 2021】 Cunningham, Scott. 2021. Causal Inference: The Mixtape. Yale University Press.\n【高橋 2022】 高橋将宜. 2022. 『統計的因果推論の理論と実装』共立出版.\n\n\n\n本講義との関係\n\n\n\n\nIntro/RCT\nMatching\nDiff-in-Diff\nRDD\nIV\n\n\n\n\nAP 2008\nCh.2\nCh.3\nCh.5\nCh.6\nCh.4\n\n\nAP 2014\nCh.1-2\n\nCh.5\nCh.4\nCh.3\n\n\n森田 2014\n第16章\n\n第18章\n第22章\n第20章\n\n\n星野・田中 2016\n第1-8章\n第9章\n\n第10章\n第11章\n\n\n中室・津川 2017\n第1-3章\n第7-8章\n第4章\n第6章\n第5章\n\n\n伊藤 2017\n第1-2章\n\n第5章\n第3章\n\n\n\n安井 2020\n第1章\n第2-3章\n第4章\n第5章\n\n\n\nCunningham 2021\nCh.2-4\nCh.5\nCh.8-10\nCh.6\nCh.7\n\n\n松林 2021\n第1-5章\n\n第8章\n第6章\n第7章\n\n\n高橋 2022\n第1-3章\n第4-12章\n\n第15-18章\n第13-14章"
  },
  {
    "objectID": "syllabus.html#講義内容参考文献",
    "href": "syllabus.html#講義内容参考文献",
    "title": "本講義について",
    "section": "講義内容・参考文献",
    "text": "講義内容・参考文献\n\n1日目：因果推論の考え方\n\nTextbook\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press. (Ch. 1 and 2)\nHernan, Miguel A., James M. Robins. 2020. Causal Inference. Chapman & Hall/CRC. (Ch. 1)\n\nArticle\n\nHolland, Paul W. 1986. “Statistics and Causal Inference.” Journal of the American Statistical Association, 81: 945–960.\nMarini, Margaret Mooney, and Burton Singer. 1988. “Causality in the Social Sciences.” Sociological Methodology, 18: 347–409.\n\nRubin, Donald B. 2005. “Causal Inference Using Potential Outcomes: Design, Modeling, Decisions.” Journal of the American Statistical Association, 100: 322–331.\n\nBrady, Henry E. 2008. “Causation and Explanation in Social Science.” In Janet M. BoxSteffensmeier, Henry E. Brady, and David Collier, eds. The Oxford Handbook of Political Methodology. Oxford University Press. (Ch. 10)\nKeele, Luke. 2015. “The Statistics of Causal Inference: A View from Political Methodology.” Political Analysis, 23: 313–335.\n\nMonograph\n\n岩波データサイエンス刊行委員会. 2016.『岩波データサイエンス Vol.3』岩波書店.\n\n\n\n\nランダム化比較試験とLab Session\n履修者全員がRの操作に慣れていると判断した場合、Lab Sessionは省略し、最終日に操作変数法の講義を行う。\n\nTextbook (RCT)\n\nImbens, Guido W., and Donald B. Rubin. 2015. Causal Inference for Statistics, Social, and Biomedical Sciences. Cambridge University Press. (Part II)}\nHernan, Miguel A., James M. Robins. 2020. Causal Inference. Chapman & Hall/CRC. (Ch. 2)\n\nTextbook (R Language)\n\n飯田健. 2013.『計量政治学』共立出版\nLander, Jared P.. 2017. R for Everyone: Advanced Analytics and Graphics (2nd Edition). Addison-Wesley Professional.\n\n『みんなのR 第2版』\n\nImai, Kosuke. 2017. Quantitative Social Science: An Introduction. Princeton University Press.\n\n『社会科学のためのデータ分析入門 (上) / (下)』\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly Media.\n\n『Rではじめるデータサイエンス』\n\n高橋康介. 2018.『再現可能性のすゝめ―RStudioによるデータ解析とレポート作成』共立出版\n松村優哉・湯谷啓明・紀ノ定保礼・前田和寛. 2018.『RユーザのためのRStudio[実践]入門―tidyverseによるモダンな分析フローの世界―』技術評論社.\n\nArticle\n\nBertrand, Marianne, and Sendhil Mullainathan. 2004. “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.” American Economic Review, 94 (4): 991-1013.\nDruckman, James N., Donald P. Green, James H. Kuklinski, and Arthur Lupia. 2006. “The Growth and Development of Experimental Research in Political Science.” American Political Science Review, 100(4): 627-635.\nTomz, Michael. 2007. “Domestic Audience Costs in International Relations: An Experimental Approach.” International Organization, 61 (4): 821-840.\nGerber, Alan S., Donald P. Green, and Christopher W. Larimer. 2008. “Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment.” American Political Science Review, 102 (1): 33-48.\nImai, Kosuke, Gary King, and Elizabeth A. Stuart. 2008. “Misunderstandings between Experimentalists and Observationalists about Causal Inference.” Journal of the Royal Statistical Society. Series A, 171(2): 481-502.\nde Rooji, Eline A., Donald P. Green, and Alan S. Gerber. 2009. “Field Experiments on Political Behavior and Collective Action.” Annual Review of Political Science. 12: 389-395.\nPalfrey, Thomas R. 2009. “Laboratory Experiments in Political Economy.” Annual Review of Political Science, 12: 379-388.}\n\n谷口尚子. 2014. 「政治学における実験研究」『選挙研究』30 (1): 5-15.\n\n\nMonograph\n\n河野勝. 2007. 『社会科学の実験アプローチ』勁草書房.\n肥前洋一. 2016. 『実験政治学』勁草書房.\nBlais, Andre, Jean-Francois Laslier, and Karine Van der Straeten Ed. 2016. Voting Experiments. Springer.\n\nR Packages for Data Analysis\n\n{tidyverse}: Easily Install and Load the ‘Tidyverse’.\n\n{tidyverse}パッケージは{dplyr}、{ggplot2}、{haven}、{magrittr}、{purrr}、{readr}、{stringr}、{tibble}、{tidyr}などを含むパッケージ群である。\n\n\n\n\n\nマッチングとその応用\n\nTextbook\n\nRosenbaum. Paul R. 2002. Observational Studies, 2nd Ed. Springer.\n星野崇宏. 2009.『調査観察データの統計科学―因果推論・選択バイアス・データ融合―』岩波書店（第2・3・4章）\n\nArticle\n\nRosenbaum, Paul R., and Donald B. Rubin. 1983. “The Central Role of the Propensity Score in Observational Studies for Causal Effects.” Biometrika, 70 (1): 41-55.\nAbadie, Alberto and Javier Gardeazabal. 2003. “The Economic Costs of Conflict: A Case Study of the Basque Country.” American Economic Review. 93(1): 113-132.\nMorgan, Stephen L., and David J. Harding. 2006. “Matching Estimators of Causal Effects: Prospects and Pitfalls in Theory and Practice.” Sociological Methods & Research, 35(1): 3-60.\nHo, Daniel E., Kosuke Imai, Gary King, and Elizabeth A. Stuart. 2007. “Matching as Nonparametric Preprocessing for Reducing Model Dependence in Parametric Causal Inference.” Political Analysis, 15: 199-236.\nSekhon, Jasjeet S. 2008. “The Neyman-Rubin Model of Causal Inference and Estimation via Matching Methods.” In Janet M. Box-Steffensmeier, Henry E. Brady, and David Collier, eds. The Oxford Handbook of Political Methodology, New York: Oxford University Press, Ch.11.\nStuart, Elizabeth A., and Donald B. Rubin. 2008. “Best Practice in Quasi-Experimental Designs: Matching Methods for Causal Inference.” In Jason W. Osborne, ed. Best Practices in Quantitative Methods, Thousand Oaks: Sage, Ch.11.\nSekhon, Jasjeet S. 2009. “Opiates for the Matches: Matching Methods for Causal Inference.” Annual Review of Political Science, 12: 487-508.\nAbadie, Alberto,Alexis Diamond, and Jens Hainmueller. 2010. “Synthetic Control Methods for Comparative Case Studies: Estimating the Effect of California’s Tobacco Control Program.” Journal of the American Statistical Association. 105 (490): 493-505.\nStuart, Elizabeth A. 2010. “Matching Methods for Causal Inference: A Review and a Look Forward.” Statistical Science, 25(1): 1-21.\nIacus, Stefano M., Gary King, and Giuseppe Porro. 2012. “Causal Inference without Balance Checking: Coarsened Exact Matching.” Political Analysis, 20: 1-24.\nAbadie, Alberto, Alexis Diamond, and Jens Hainmueller. 2014. “Comparative Politics and the Synthetic Control Method.” American Journal of Political Science. 59 (2): 495-510.\nBrodersen, Kay H., Fabian Gallusser, Jim Koehler, Nicolas Remy, and Steven L. Scott. 2015. “Inferring causal impact using Bayesian structural time-series models.” Annals of Applied Statistics. 9(1): 247-274.\n登藤直弥・小林哲郎・稲増一憲. 2016.「ソフトニュースへの接触は政治的関心を高めるか―一般化傾向スコアを用いた因果推論―」『行動計量学』43 (2): 129-141.\nSamii, Cyrus, Laura Paler, and Sarah Zukerman Daly. 2017. “Retrospective Causal Inference with Machine Learning Ensembles: An Application to Anti-recidivism Policies in Colombia.” Political Analysis, 24 (4): 434-456.\n\nR Package\n\n{Matching}: Multivariate and Propensity Score Matching with Balance Optimization\n{MatchIt}: Nonparametric Preprocessing for Parametric Causal Inference\n{WeightIt}: Weighting for Covariate Balance in Observational Studies\n{SuperLearner}: Super Learner Prediction\n\n\n\n\n差分の差分法\n\nArticle\n\nCard, David, and Alan B. Krueger. 1994. “Minimum Wages and Employment: A Case Study of the Fast Food Industry in New Jersey and Pennsylvania.” American Economic Review, 90 (5): 1397-1420.\nBertrand, Marianne, Esther Duflo, and Sendhil Mullainathan. 2004. “How Much Should We Trust Differences-In-Differences Estimates?” Quarterly Journal of Economics, 119(1): 249-275.\nDi Tella, Rafael, and Ernesto Schargrodsky. 2004. “Do Police Reduce Crime? Estimates Using the Allocation of Police Forces After a Terrorist Attack.” American Economic Review, 94 (1): 115-133.\nAbadie, Alberto. 2005. “Semiparametric Difference-in-Differences Estimators.” Review of Economic Studies, 72(1): 1-19.\nLyall, Jason. 2009. “Does Indiscriminate Violence Incite Insurgent Attacks? Evidence from Chechnya.” Journal of Conflict Resolution, 53(3): 331-362.\nLechner, Michael. 2010. “The Estimation of Causal Effects by Difference-in-Difference Methods.” Working paper.\nAsai, Yukiko, Ryo Kamibayashi, and Shintaro Yamaguchi. 2015. “Childcare availability, household structure, and maternal employment.” Journal of the Japanese and International Economies, 38: 172-192.\nFouirnaies, Alexander, and Hande Mutlu-Eren. 2015. “English Bacon: Copartisan Bias in Intergovernmental Grant Allocation in England.” Journal of Politics, 77(3): 805–817.\nXu, Yiqing. 2017. “Generalized Synthetic Control Method: Causal Inference with Interactive Fixed Effects Models,” Political Analysis, 25(1): 56-76.\n\nR Packages\n\n{estimatr}: Fast Estimators for Design-Based Inference\n{CausalImpact}: Inferring Causal Effects using Bayesian Structural Time-Series Models\n{Synth}: Synthetic Control Group Method for Comparative Case Studies\n{gsynth}: Generalized Synthetic Control Method\n\n\n\n\n回帰不連続デザイン\n\nTextbook\n\nCattaneo, Matias D. 2020. A Practical Introduction to Regression Discontinuity Designs, Cambridge University Press.\n\nArticle\n\nThistlethwaite, Donald L., and Donald T. Campbell. 1960. “Regression-discontinuity analysis: An alternative to the ex post facto experiment.” Journal of Educational Psychology, 51(6): 309-317.\nHahn, Jinyong, Petra Todd, and Wilbert Van der Klaauw. 2001. “Identification and Estimation of Treatment Effects with a Regression-Discontinuity Design.” Econometrica, 69 (1): 201-209.\nPoter, Jack. 2003. “Estimation in the Regression Discontinuity Model.” Working Paper.\nImbens, Guido W., and Thomas Lemieux. 2008. “Regression Discontinuity Designs: A Guide to Practice.” Journal of Econometrics, 142 (2): 615-635.\nLee, David S. 2008. “Randomized experiments from non-random selection in U.S. House elections.” Journal of Econometrics, 142 (2): 675-697.\nJustin McCrary. 2008. “Manipulation of the running variable in the regression discontinuity design: A density test.” Journal of Econometrics, 142 (2): 698-714.\nLee, David S., and Thomas Lemieux. 2010. “Regression Discontinuity Designs in Economics. Journal of Economic Literature, 48 (2): 281-355.\nLee, David S., and Thomas Lemieux. 2010. “Regression Discontinuity Designs in Economics.” Journal of Economic Literature, 48 (2): 281-355.\nImbens, Guido, and Karthik Kalyanaraman. 2011. “Optimal Bandwidth Choice for the Regression Discontinuity Estimator.” Review of Economic Studies, 79 (3): 933-959.\nAriga, Kenichi, Yusaku Horiuchi, Roland Mansilla, and Michio Umeda. 2016. “No sorting, no advantage: Regression discontinuity estimates of incumbency advantage in Japan.” Electoral Studies, 43: 21-31.\nAndrew Gelman and Guido Imbens. 2019. “Why High-Order Polynomials Should Not Be Used in Regression Discontinuity Designs,” Journal of Business & Economic Statistics, 37(3): 447-456.\n\nMonograph\n\nCattaneo, Matias D., Nicolas Idrobo and Rocio Titiunik. 2018. A Practical Introduction to Regression Discontinuity Designs: Volume I. Cambridge University Press.\nCattaneo, Matias D., Nicol'as Idrobo and Roc'io Titiunik. 2018. A Practical Introduction to Regression Discontinuity Designs: Volume II. Cambridge University Press.\n\nR package\n\n{rdd}: Regression Discontinuity Estimation\n{rddtools}: Toolbox for Regression Discontinuity Design (‘RDD’)\n{rddapp}: Regression Discontinuity Design Application\n{rdrobust}: Robust Data-Driven Statistical Inference in Regression-Discontinuity Designs\n{rdmulti}: Analysis of RD Designs with Multiple Cutoffs or Scores\n{rdpower}: Power Calculations for RD Designs\n\n\n\n\n操作変数法\n　以下の内容はLab Sessionを行わない場合のみ、解説する。\n\nArticle\n\nAngrist, Joshua D. and Alan B. Krueger. 2001. “Instrumental Variables and the Search for Identification: From Supply and Demand to Natural Experiments,” Journal of Economic Perspectives, 15 (4): 69-85\nDunning, Thad. 2008. “Model Specification in Instrumental-Variables Regression.” Political Analysis, 16 (3): 290-302.\nKern, Holger Lutz and Jens Hainmueller. 2009. “Opium for the Masses: How Foreign Media Can Stabilize Authoritarian Regimes.” Political Analysis, 17 (4): 377-399.\nAllison J. Sovey and Donald P. Green. 2010. “Instrumental Variables Estimation in Political Science: A Readers’ Guide,” American Journal of Political Science, 55(1): 188-200.\nBollen, Kenneth A. 2012. “Instrumental Variables in Sociology and the Social Sciences,” Annual Review of Sociology, 38:37-72.\nAronow, Peter M. and Allison Carnegie. 2013. “Beyond LATE: Estimation of the Average Treatment Effect with an Instrumental Variable.” Political Analysis, 21 (4): 492-506.\n\nR packages\n\n{AER}: Applied Econometrics with R}\n{ivreg}: Instrumental-Variables Regression by ‘2SLS’, ‘2SM’, or ‘2SMM’, with Diagnostics}"
  },
  {
    "objectID": "material/did.html",
    "href": "material/did.html",
    "title": "差分の差分法",
    "section": "",
    "text": "新しいタブで開く"
  },
  {
    "objectID": "material/rdd.html",
    "href": "material/rdd.html",
    "title": "回帰不連続デザイン",
    "section": "",
    "text": "新しいタブで開く"
  },
  {
    "objectID": "material/rdd.html#スライド-1",
    "href": "material/rdd.html#スライド-1",
    "title": "回帰不連続デザイン",
    "section": "スライド",
    "text": "スライド"
  },
  {
    "objectID": "material/intro_rct.html",
    "href": "material/intro_rct.html",
    "title": "因果推論の考え方とランダム化比較試験",
    "section": "",
    "text": "新しいタブで開く"
  },
  {
    "objectID": "material/intro_rct.html#パッケージ",
    "href": "material/intro_rct.html#パッケージ",
    "title": "因果推論の考え方とランダム化比較試験",
    "section": "パッケージ",
    "text": "パッケージ\n　通常、Rでのパッケージのインストールとアップーでとにはinstall.packages()関数、読み込みにはlibrary()、またはrequire()関数を使う。また、R公式レポジトリにないパッケージは{devtools}か{remote}パッケージを使う。これらの関数を使い分けることは面倒なので、本講義ではこれらの処理を統合した{pacman}パッケージを使用する。まずは、{pacman}パッケージをインストールする。\n\n# NIIオンライン分析システムを利用する場合、導入済み\ninstall.packages(\"pacman\")\n\n　パッケージを読み込む際、pacman::p_load(読み込むパッケージ名)を入力する。インストールされていない場合は、自動的にCRANからダウンロード&インストールした上で読み込んでくれるので便利だ1。以下では本講義で使用するパッケージとして{tidyverse}、{summarytools}、{fastDummies}、{modelsummary}を読み込む。\n\npacman::p_load(tidyverse, summarytools, fastDummies,\n               modelsummary, broom)\n\n　CRANでなく、GitHub上で公開されているパッケージを使う場合はpacman::p_load_gh()を使用する。()の中には\"ユーザー名/リポジトリ名\"を入力。たとえば、{BalanceR}の作成者のGitHubアカウント名はJaehyunSongであり、{BalanceR}のリポジトリ名はBalanceRだから、以下のように入力する。\n\npacman::p_load_gh(\"JaehyunSong/BalanceR\")"
  },
  {
    "objectID": "material/intro_rct.html#データの読み込みと確認",
    "href": "material/intro_rct.html#データの読み込みと確認",
    "title": "因果推論の考え方とランダム化比較試験",
    "section": "データの読み込みと確認",
    "text": "データの読み込みと確認\n　.csv形式のデータを読み込むにはread_csv()関数を使用する。()内には読み込むファイルのパスを\"で囲んで記入する。read_csv()関数はファイルの読み込みのみの機能しか持たない。現在の作業環境内に読み込んだデータを格納するためには代入演算子<-を使う。ここではdataフォルダー内のDay1_Data3.csvを読み込み、raw_dfという名のオブジェクトとしてく格納する。作業環境内のオブジェクトはRを再起動すると削除されるため、改めてパッケージ・データの読み込みが必要だ。\n\nraw_df <- read_csv(\"data/Day1_Data3.csv\")\n\n　オブジェクトの中身を出力するためにはオブジェクト名を入力する。\n\nraw_df\n\n# A tibble: 344,084 × 8\n   treatment  gender   yob hh_size voted2000 voted2002 voted2004 voted2006\n   <chr>      <chr>  <dbl>   <dbl> <chr>     <chr>     <chr>     <chr>    \n 1 Civic Duty male    1941       2 no        yes       no        no       \n 2 Civic Duty female  1947       2 no        yes       no        no       \n 3 Hawthorne  male    1951       3 no        yes       no        yes      \n 4 Hawthorne  female  1950       3 no        yes       no        yes      \n 5 Hawthorne  female  1982       3 no        yes       no        yes      \n 6 Control    male    1981       3 no        no        no        no       \n 7 Control    female  1959       3 no        yes       no        yes      \n 8 Control    male    1956       3 no        yes       no        yes      \n 9 Control    female  1968       2 no        yes       no        no       \n10 Control    male    1967       2 no        yes       no        no       \n# … with 344,074 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　表形式データの大きさ（行の列の数）の確認にはdim()関数を使う。\n\ndim(raw_df)\n\n[1] 344084      8\n\n\n　表形式データの場合、各列には名前が付いており、それぞれが一つの変数に該当する。これら変数名のみの出力にはnames()関数を使う。今回のデータだと、列の数が少ないこともあり、一画面に全列が表示されるが、数百列のデータとなると画面に収まらないので、変数名を確認しておくことを推奨する。\n\nnames(raw_df)\n\n[1] \"treatment\" \"gender\"    \"yob\"       \"hh_size\"   \"voted2000\" \"voted2002\"\n[7] \"voted2004\" \"voted2006\""
  },
  {
    "objectID": "material/intro_rct.html#データハンドリングとパイプ演算子",
    "href": "material/intro_rct.html#データハンドリングとパイプ演算子",
    "title": "因果推論の考え方とランダム化比較試験",
    "section": "データハンドリングとパイプ演算子",
    "text": "データハンドリングとパイプ演算子\n　パイプ演算子には{magrittr}パッケージが提供する%>%とR 4.1から提供されるネイティブパイプ演算子の|>がある。現在の主流は古くから使われた%>%であるが、今後、|>が主流になると考えられるため、以下では|>を使用する。しかし、多くの場合、|>の代わりに%>%を使っても同じ結果が得られる。\n　パイプ演算子はパイプ前のオブジェクトを、パイプ後の関数の第一引数として渡す単純な演算子だ。たとえば、列名を変更する関数はrename()であるが、使い方はrenames(データ名, 新しい列名 = 既存の列名, ...)である。raw_dfのgender列の名前をfemaleに変更する場合は以下のように書く。\n\nrename(raw_df, female = gender)\n\n# A tibble: 344,084 × 8\n   treatment  female   yob hh_size voted2000 voted2002 voted2004 voted2006\n   <chr>      <chr>  <dbl>   <dbl> <chr>     <chr>     <chr>     <chr>    \n 1 Civic Duty male    1941       2 no        yes       no        no       \n 2 Civic Duty female  1947       2 no        yes       no        no       \n 3 Hawthorne  male    1951       3 no        yes       no        yes      \n 4 Hawthorne  female  1950       3 no        yes       no        yes      \n 5 Hawthorne  female  1982       3 no        yes       no        yes      \n 6 Control    male    1981       3 no        no        no        no       \n 7 Control    female  1959       3 no        yes       no        yes      \n 8 Control    male    1956       3 no        yes       no        yes      \n 9 Control    female  1968       2 no        yes       no        no       \n10 Control    male    1967       2 no        yes       no        no       \n# … with 344,074 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　ここで第1引数がraw_dfだが、パイプ演算子を使うと以下のようになり、人間にとって読みやすいコードになる。\n\nraw_df |>\n  rename(female = gender)\n\n# A tibble: 344,084 × 8\n   treatment  female   yob hh_size voted2000 voted2002 voted2004 voted2006\n   <chr>      <chr>  <dbl>   <dbl> <chr>     <chr>     <chr>     <chr>    \n 1 Civic Duty male    1941       2 no        yes       no        no       \n 2 Civic Duty female  1947       2 no        yes       no        no       \n 3 Hawthorne  male    1951       3 no        yes       no        yes      \n 4 Hawthorne  female  1950       3 no        yes       no        yes      \n 5 Hawthorne  female  1982       3 no        yes       no        yes      \n 6 Control    male    1981       3 no        no        no        no       \n 7 Control    female  1959       3 no        yes       no        yes      \n 8 Control    male    1956       3 no        yes       no        yes      \n 9 Control    female  1968       2 no        yes       no        no       \n10 Control    male    1967       2 no        yes       no        no       \n# … with 344,074 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　要するに、X |> Yは「X（の結果）を使ってYを行う」ことを意味する。\n　続いて、変数のリコーディングをしてみよう。xの値が\"A\"なら1、それ以外は0のように、戻り値が2種類の場合のリコーディングにはif_else()を使用する。書き方は以下の通りだ。\n\nif_else(条件式, 条件が満たされる場合の戻り値, 条件が満たされない場合の戻り値)\n\n　たとえば、raw_dfのgender列の値が\"female\"なら1、それ以外なら0とし、その結果をfemale列として追加するコードは以下の通り。同値を意味する演算子が=でなく、==であることに注意すること（=は<-と同じ代入演算子であるが、Rでは代入演算子として=より<-の使用を推奨している）。\n\nmutate(raw_df, \n       female = if_else(gender == \"female\", 1, 0))\n\n# A tibble: 344,084 × 9\n   treatment  gender   yob hh_size voted2000 voted2002 voted2004 voted2…¹ female\n   <chr>      <chr>  <dbl>   <dbl> <chr>     <chr>     <chr>     <chr>     <dbl>\n 1 Civic Duty male    1941       2 no        yes       no        no            0\n 2 Civic Duty female  1947       2 no        yes       no        no            1\n 3 Hawthorne  male    1951       3 no        yes       no        yes           0\n 4 Hawthorne  female  1950       3 no        yes       no        yes           1\n 5 Hawthorne  female  1982       3 no        yes       no        yes           1\n 6 Control    male    1981       3 no        no        no        no            0\n 7 Control    female  1959       3 no        yes       no        yes           1\n 8 Control    male    1956       3 no        yes       no        yes           0\n 9 Control    female  1968       2 no        yes       no        no            1\n10 Control    male    1967       2 no        yes       no        no            0\n# … with 344,074 more rows, and abbreviated variable name ¹​voted2006\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　mutate()は指定された列に対して何らかの処理を行い、その結果を新しい列として追加するか、上書きする関数である。このmutate()関数の第1引数もデータであるため、以下のようにパイプ演算子を使うこともできる。\n\nraw_df |>\n  mutate(female = if_else(gender == \"female\", 1, 0))\n\n# A tibble: 344,084 × 9\n   treatment  gender   yob hh_size voted2000 voted2002 voted2004 voted2…¹ female\n   <chr>      <chr>  <dbl>   <dbl> <chr>     <chr>     <chr>     <chr>     <dbl>\n 1 Civic Duty male    1941       2 no        yes       no        no            0\n 2 Civic Duty female  1947       2 no        yes       no        no            1\n 3 Hawthorne  male    1951       3 no        yes       no        yes           0\n 4 Hawthorne  female  1950       3 no        yes       no        yes           1\n 5 Hawthorne  female  1982       3 no        yes       no        yes           1\n 6 Control    male    1981       3 no        no        no        no            0\n 7 Control    female  1959       3 no        yes       no        yes           1\n 8 Control    male    1956       3 no        yes       no        yes           0\n 9 Control    female  1968       2 no        yes       no        no            1\n10 Control    male    1967       2 no        yes       no        no            0\n# … with 344,074 more rows, and abbreviated variable name ¹​voted2006\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　また、mutate()内には複数のコードを書くのもできる。voted2000列からvoted2006列までそれぞれの値が\"yes\"であれば、1を、それ以外の場合は0にリコーディングしてみよう。\n\nraw_df |>\n  mutate(female    = if_else(gender    == \"female\", 1, 0),\n         voted2000 = if_else(voted2000 == \"yes\", 1, 0),\n         voted2002 = if_else(voted2002 == \"yes\", 1, 0),\n         voted2004 = if_else(voted2004 == \"yes\", 1, 0),\n         voted2006 = if_else(voted2006 == \"yes\", 1, 0))\n\n# A tibble: 344,084 × 9\n   treatment  gender   yob hh_size voted2000 voted2002 voted2004 voted2…¹ female\n   <chr>      <chr>  <dbl>   <dbl>     <dbl>     <dbl>     <dbl>    <dbl>  <dbl>\n 1 Civic Duty male    1941       2         0         1         0        0      0\n 2 Civic Duty female  1947       2         0         1         0        0      1\n 3 Hawthorne  male    1951       3         0         1         0        1      0\n 4 Hawthorne  female  1950       3         0         1         0        1      1\n 5 Hawthorne  female  1982       3         0         1         0        1      1\n 6 Control    male    1981       3         0         0         0        0      0\n 7 Control    female  1959       3         0         1         0        1      1\n 8 Control    male    1956       3         0         1         0        1      0\n 9 Control    female  1968       2         0         1         0        0      1\n10 Control    male    1967       2         0         1         0        0      0\n# … with 344,074 more rows, and abbreviated variable name ¹​voted2006\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　また、パイプ演算子は2つ以上使うこともできる。たとえば、rename()を使ってgender列をfemaleに変更し、mutate()でリコーディングを行う場合、以下のように書く。これはraw_dfを使ってrename()の処理を行い、その結果をmutate()関数のデータとして渡すことを意味する。\n\nraw_df |>\n  rename(female = gender) |>\n  mutate(female    = if_else(female    == \"female\", 1, 0),\n         voted2000 = if_else(voted2000 == \"yes\", 1, 0),\n         voted2002 = if_else(voted2002 == \"yes\", 1, 0),\n         voted2004 = if_else(voted2004 == \"yes\", 1, 0),\n         voted2006 = if_else(voted2006 == \"yes\", 1, 0))\n\n# A tibble: 344,084 × 8\n   treatment  female   yob hh_size voted2000 voted2002 voted2004 voted2006\n   <chr>       <dbl> <dbl>   <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1 Civic Duty      0  1941       2         0         1         0         0\n 2 Civic Duty      1  1947       2         0         1         0         0\n 3 Hawthorne       0  1951       3         0         1         0         1\n 4 Hawthorne       1  1950       3         0         1         0         1\n 5 Hawthorne       1  1982       3         0         1         0         1\n 6 Control         0  1981       3         0         0         0         0\n 7 Control         1  1959       3         0         1         0         1\n 8 Control         0  1956       3         0         1         0         1\n 9 Control         1  1968       2         0         1         0         0\n10 Control         0  1967       2         0         1         0         0\n# … with 344,074 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　以上のコードはデータを加工し、その結果を出力するだけであって、その結果を保存しない。\n\nraw_df\n\n# A tibble: 344,084 × 8\n   treatment  gender   yob hh_size voted2000 voted2002 voted2004 voted2006\n   <chr>      <chr>  <dbl>   <dbl> <chr>     <chr>     <chr>     <chr>    \n 1 Civic Duty male    1941       2 no        yes       no        no       \n 2 Civic Duty female  1947       2 no        yes       no        no       \n 3 Hawthorne  male    1951       3 no        yes       no        yes      \n 4 Hawthorne  female  1950       3 no        yes       no        yes      \n 5 Hawthorne  female  1982       3 no        yes       no        yes      \n 6 Control    male    1981       3 no        no        no        no       \n 7 Control    female  1959       3 no        yes       no        yes      \n 8 Control    male    1956       3 no        yes       no        yes      \n 9 Control    female  1968       2 no        yes       no        no       \n10 Control    male    1967       2 no        yes       no        no       \n# … with 344,074 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　このように頑張ってデータを加工したもののその結果が全く反映されていない。加工したデータを引き続き使っていくためには、加工結果を作業環境内に保存する必要がある。作業環境内にオブジェクトを保存するためには代入演算子（<-）を使い、名前を付けて作業空間内に保存する（ファイルとして保存されるわけではない）必要がある。今回は加工の結果をdfという名で保存する。raw_dfに上書きしても問題はないが、生データはとりあえず作業空間内に残しておくことを推奨する（Rに慣れれば上書きしても良い）。\n\ndf <- raw_df |>\n  rename(female = gender) |>\n  mutate(female    = if_else(female    == \"female\", 1, 0),\n         voted2000 = if_else(voted2000 == \"yes\", 1, 0),\n         voted2002 = if_else(voted2002 == \"yes\", 1, 0),\n         voted2004 = if_else(voted2004 == \"yes\", 1, 0),\n         voted2006 = if_else(voted2006 == \"yes\", 1, 0))\n\ndf\n\n# A tibble: 344,084 × 8\n   treatment  female   yob hh_size voted2000 voted2002 voted2004 voted2006\n   <chr>       <dbl> <dbl>   <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1 Civic Duty      0  1941       2         0         1         0         0\n 2 Civic Duty      1  1947       2         0         1         0         0\n 3 Hawthorne       0  1951       3         0         1         0         1\n 4 Hawthorne       1  1950       3         0         1         0         1\n 5 Hawthorne       1  1982       3         0         1         0         1\n 6 Control         0  1981       3         0         0         0         0\n 7 Control         1  1959       3         0         1         0         1\n 8 Control         0  1956       3         0         1         0         1\n 9 Control         1  1968       2         0         1         0         0\n10 Control         0  1967       2         0         1         0         0\n# … with 344,074 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　ちなみに、across()関数とラムダ式（無名関数）を組み合わせると以上のコードを効率化することもできる。across()は強力な関数だが、初心者にはやや難しいかも知れない。詳細は『私たちのR』の第13.1章を参照されたい。\n\ndf <- raw_df |>\n  rename(female = gender) |>\n  mutate(female = if_else(female == \"female\", 1, 0),\n         across(starts_with(\"voted\"), ~if_else(.x == \"yes\", 1, 0)))"
  },
  {
    "objectID": "material/intro_rct.html#記述統計量",
    "href": "material/intro_rct.html#記述統計量",
    "title": "因果推論の考え方とランダム化比較試験",
    "section": "記述統計量",
    "text": "記述統計量\n　記述統計量の計算には{summarytools}のdescr()関数が便利だ。descr(データ名)を入力するだけで各変数の記述統計量が出力される。実際にやってみると分かるが、情報量がかなり多い。しかし、実際の論文では各変数の歪度や尖度まで報告することはあまりないだろう。ここではstats引数を追加して、論文などでよく使う平均値（\"mean\"）、標準偏差（\"sd\"）、最小値（\"min\"）、最大値（\"max\"）、有効ケース数（\"n.valid\"）のみ出力する。\n\ndf |>\n  descr(stats = c(\"mean\", \"sd\", \"min\", \"max\", \"n.valid\"))\n\nDescriptive Statistics  \ndf  \nN: 344084  \n\n                   female     hh_size   voted2000   voted2002   voted2004   voted2006         yob\n------------- ----------- ----------- ----------- ----------- ----------- ----------- -----------\n         Mean        0.50        2.18        0.25        0.39        0.40        0.32     1956.21\n      Std.Dev        0.50        0.79        0.43        0.49        0.49        0.46       14.45\n          Min        0.00        1.00        0.00        0.00        0.00        0.00     1900.00\n          Max        1.00        8.00        1.00        1.00        1.00        1.00     1986.00\n      N.Valid   344084.00   344084.00   344084.00   344084.00   344084.00   344084.00   344084.00\n\n\n　ただし、descr()を使うと数値型（numeric）変数の記述統計量のみ表示される。dfだと、treatment列は文字型（character）であるため、表示されない2。各グループがサンプルの何割かを計算するためには、treatment変数をダミー変数へ変換する必要がある。ダミー変数の作成は面倒な作業であるが、{fastDummies}パッケージのdummy_cols()を使えば簡単にできる。dummy_cols()の中にはselect_columns = \"ダミー化する列名\"を入れれば、当該変数をダミー変数へ変換し、新しい列として追加してくれる。それではtreatment列をダミー化&追加し、その結果をdfに上書きしてみよう。\n\ndf <- df |>\n  dummy_cols(select_columns = \"treatment\")\n\ndf\n\n# A tibble: 344,084 × 13\n   treatm…¹ female   yob hh_size voted…² voted…³ voted…⁴ voted…⁵ treat…⁶ treat…⁷\n   <chr>     <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <int>   <int>\n 1 Civic D…      0  1941       2       0       1       0       0       1       0\n 2 Civic D…      1  1947       2       0       1       0       0       1       0\n 3 Hawthor…      0  1951       3       0       1       0       1       0       0\n 4 Hawthor…      1  1950       3       0       1       0       1       0       0\n 5 Hawthor…      1  1982       3       0       1       0       1       0       0\n 6 Control       0  1981       3       0       0       0       0       0       1\n 7 Control       1  1959       3       0       1       0       1       0       1\n 8 Control       0  1956       3       0       1       0       1       0       1\n 9 Control       1  1968       2       0       1       0       0       0       1\n10 Control       0  1967       2       0       1       0       0       0       1\n# … with 344,074 more rows, 3 more variables: treatment_Hawthorne <int>,\n#   treatment_Neighbors <int>, treatment_Self <int>, and abbreviated variable\n#   names ¹​treatment, ²​voted2000, ³​voted2002, ⁴​voted2004, ⁵​voted2006,\n#   ⁶​`treatment_Civic Duty`, ⁷​treatment_Control\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n　画面には表示されないが、出力結果の下段を見るとtreatment_で始まるいくつかの変数が追加されたことが分かる。ここでは\"tretmant\"で始まる列のみを抽出つして確認してみよう。\n\ndf |>\n  select(starts_with(\"treatment\"))\n\n# A tibble: 344,084 × 6\n   treatment  `treatment_Civic Duty` treatment_Control treatme…¹ treat…² treat…³\n   <chr>                       <int>             <int>     <int>   <int>   <int>\n 1 Civic Duty                      1                 0         0       0       0\n 2 Civic Duty                      1                 0         0       0       0\n 3 Hawthorne                       0                 0         1       0       0\n 4 Hawthorne                       0                 0         1       0       0\n 5 Hawthorne                       0                 0         1       0       0\n 6 Control                         0                 1         0       0       0\n 7 Control                         0                 1         0       0       0\n 8 Control                         0                 1         0       0       0\n 9 Control                         0                 1         0       0       0\n10 Control                         0                 1         0       0       0\n# … with 344,074 more rows, and abbreviated variable names\n#   ¹​treatment_Hawthorne, ²​treatment_Neighbors, ³​treatment_Self\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　select()関数内には抽出する列名を入力するだけで良い。たとえば、femaleとyob列を抽出するならselect(female, yob)である。また、femaleからvoted2006までの意味でfemale:voted2006のような書き方もできる。他にも上の例のようにstarts_with()やends_with()、contain()を使って特定の文字列で始まる（で終わる、を含む）列を指定することもできる。一部の列を除外する場合は変数名の前に!か-を付ける。\n　とにかく、問題なくダミー化されていることが分かる。もう一度記述統計量を出してみよう。descr()は仕様上、出力される変数の順番はアルファベット順になるが、ここでは元の順番を維持するためにorder = \"p\"を追加する。また、通常の記述統計表が、先ほど見たものとは違って、各行が変数を、列は記述統計量を表す場合が多い。このように行と列を交換するためにはtranspose = TRUEを追加する3。\n\ndf |>\n  descr(stats = c(\"mean\", \"sd\", \"min\", \"max\", \"n.valid\"),\n        order = \"p\", transpose = TRUE, headings = FALSE)\n\n\n\n\nMean\nStd.Dev\nMin\nMax\nN.Valid\n\n\n\n\nfemale\n0.50\n0.50\n0.00\n1.00\n344084.00\n\n\nyob\n1956.21\n14.45\n1900.00\n1986.00\n344084.00\n\n\nhh_size\n2.18\n0.79\n1.00\n8.00\n344084.00\n\n\nvoted2000\n0.25\n0.43\n0.00\n1.00\n344084.00\n\n\nvoted2002\n0.39\n0.49\n0.00\n1.00\n344084.00\n\n\nvoted2004\n0.40\n0.49\n0.00\n1.00\n344084.00\n\n\nvoted2006\n0.32\n0.46\n0.00\n1.00\n344084.00\n\n\ntreatment_Civic Duty\n0.11\n0.31\n0.00\n1.00\n344084.00\n\n\ntreatment_Control\n0.56\n0.50\n0.00\n1.00\n344084.00\n\n\ntreatment_Hawthorne\n0.11\n0.31\n0.00\n1.00\n344084.00\n\n\ntreatment_Neighbors\n0.11\n0.31\n0.00\n1.00\n344084.00\n\n\ntreatment_Self\n0.11\n0.31\n0.00\n1.00\n344084.00\n\n\n\n\n　他にも以下のようにdfSummary()関数を使えば、綺麗な表としてまとめてくれる。しかも文字型、factor型変数の場合も度数分布表を作成してくれるので非常に便利だ。これも{summarytools}パッケージに含まれた機能なので、別途、パッケージを読み込む必要はない。\n\ndf |>\n  select(-starts_with(\"treatment_\")) |>\n  dfSummary(headings = FALSE) |> \n  print(method = \"render\", round.digits = 3)\n\n\n\n\n  \n    \n      No\n      Variable\n      Stats / Values\n      Freqs (% of Valid)\n      Graph\n      Valid\n      Missing\n    \n  \n  \n    \n      1\n      treatment\n[character]\n      1. Civic Duty2. Control3. Hawthorne4. Neighbors5. Self\n      38218(11.1%)191243(55.6%)38204(11.1%)38201(11.1%)38218(11.1%)\n      \n      344084\n(100.0%)\n      0\n(0.0%)\n    \n    \n      2\n      female\n[numeric]\n      Min  : 0Mean : 0.5Max  : 1\n      0:172289(50.1%)1:171795(49.9%)\n      \n      344084\n(100.0%)\n      0\n(0.0%)\n    \n    \n      3\n      yob\n[numeric]\n      Mean (sd) : 1956.2 (14.4)min ≤ med ≤ max:1900 ≤ 1956 ≤ 1986IQR (CV) : 18 (0)\n      86 distinct values\n      \n      344084\n(100.0%)\n      0\n(0.0%)\n    \n    \n      4\n      hh_size\n[numeric]\n      Mean (sd) : 2.2 (0.8)min ≤ med ≤ max:1 ≤ 2 ≤ 8IQR (CV) : 0 (0.4)\n      1:47834(13.9%)2:214086(62.2%)3:57474(16.7%)4:20916(6.1%)5:3315(1.0%)6:402(0.1%)7:49(0.0%)8:8(0.0%)\n      \n      344084\n(100.0%)\n      0\n(0.0%)\n    \n    \n      5\n      voted2000\n[numeric]\n      Min  : 0Mean : 0.3Max  : 1\n      0:257464(74.8%)1:86620(25.2%)\n      \n      344084\n(100.0%)\n      0\n(0.0%)\n    \n    \n      6\n      voted2002\n[numeric]\n      Min  : 0Mean : 0.4Max  : 1\n      0:209947(61.0%)1:134137(39.0%)\n      \n      344084\n(100.0%)\n      0\n(0.0%)\n    \n    \n      7\n      voted2004\n[numeric]\n      Min  : 0Mean : 0.4Max  : 1\n      0:205934(59.8%)1:138150(40.2%)\n      \n      344084\n(100.0%)\n      0\n(0.0%)\n    \n    \n      8\n      voted2006\n[numeric]\n      Min  : 0Mean : 0.3Max  : 1\n      0:235388(68.4%)1:108696(31.6%)\n      \n      344084\n(100.0%)\n      0\n(0.0%)\n    \n  \n\nGenerated by summarytools 1.0.1 (R version 4.2.1)2022-07-28"
  },
  {
    "objectID": "material/intro_rct.html#バランスチェック",
    "href": "material/intro_rct.html#バランスチェック",
    "title": "因果推論の考え方とランダム化比較試験",
    "section": "バランスチェック",
    "text": "バランスチェック\n　バランスチェックの簡単な方法はグループごとに処置前変数（pre-treatment variables）の平均値を比較することである。無作為割当が成功しているのであれば、処置前に測定された変数の平均値は近似するはずである。ここではグループ（treatment）ごとに性別、誕生年、世帯規模、2000〜2004年の投票参加の平均値を比較してみる。\n\ndf |>\n  group_by(treatment) |>\n  summarise(female    = mean(female, na.rm = TRUE),\n            yob       = mean(yob, na.rm = TRUE),\n            hh_size   = mean(hh_size, na.rm = TRUE),\n            voted2000 = mean(voted2000, na.rm = TRUE),\n            voted2002 = mean(voted2002, na.rm = TRUE),\n            voted2004 = mean(voted2004, na.rm = TRUE))\n\n# A tibble: 5 × 7\n  treatment  female   yob hh_size voted2000 voted2002 voted2004\n  <chr>       <dbl> <dbl>   <dbl>     <dbl>     <dbl>     <dbl>\n1 Civic Duty  0.500 1956.    2.19     0.254     0.389     0.399\n2 Control     0.499 1956.    2.18     0.252     0.389     0.400\n3 Hawthorne   0.499 1956.    2.18     0.250     0.394     0.403\n4 Neighbors   0.500 1956.    2.19     0.251     0.387     0.407\n5 Self        0.500 1956.    2.18     0.251     0.392     0.402\n\n\n　それぞれの変数の平均値は非常に似ているため、無作為割当が成功したと考えられる。しかし、変数の単位によって判断が難しいかも知れない。たとえば、2つのグループがあり、年齢の平均値の差は3、世帯規模のそれは2だとする。これを見ると年齢の方がよりバランスが取れていないようにも見えるが、年齢の幅は数十であるに対し、世帯規模はせいぜい5〜6程度であろう。したがって、各変数のばらつきまで考慮した比較が適切であり、その方法の一つが標準化バイアス（=標準化差分）である。\n　標準化差分を計算する便利パッケージ、{BalanceR}を使ってみよう。第1引数はデータだから、パイプで渡せば良い。BalanceR()内にはgroup引数にグループ識別変数を、covには処置前変数のベクトルを入れる。\n\nblc_chk <- df |>\n  BalanceR(group = treatment,\n           cov   = c(female, yob, hh_size, voted2000, voted2002, voted2004))\n\nblc_chk\n\n  Covariate Mean:Civic Duty SD:Civic Duty Mean:Control SD:Control\n1    female           0.500         0.500        0.499      0.500\n2       yob        1956.341        14.465     1956.186     14.436\n3   hh_size           2.189         0.802        2.184      0.788\n4 voted2000           0.254         0.435        0.252      0.434\n5 voted2002           0.389         0.487        0.389      0.488\n6 voted2004           0.399         0.490        0.400      0.490\n  Mean:Hawthorne SD:Hawthorne Mean:Neighbors SD:Neighbors Mean:Self SD:Self\n1          0.499        0.500          0.500        0.500     0.500   0.500\n2       1956.295       14.400       1956.147       14.579  1956.207  14.416\n3          2.180        0.789          2.188        0.805     2.181   0.782\n4          0.250        0.433          0.251        0.434     0.251   0.434\n5          0.394        0.489          0.387        0.487     0.392   0.488\n6          0.403        0.491          0.407        0.491     0.402   0.490\n  SB:Civic Duty-Control SB:Civic Duty-Hawthorne SB:Civic Duty-Neighbors\n1                 0.248                   0.236                   0.024\n2                 1.069                   0.317                   1.335\n3                 0.687                   1.130                   0.169\n4                 0.388                   0.738                   0.547\n5                -0.108                  -1.123                   0.464\n6                -0.182                  -0.772                  -1.472\n  SB:Civic Duty-Self SB:Control-Hawthorne SB:Control-Neighbors SB:Control-Self\n1              0.120               -0.013               -0.225          -0.128\n2              0.924               -0.754                0.272          -0.146\n3              1.051                0.448               -0.515           0.365\n4              0.566                0.350                0.158           0.178\n5             -0.628               -1.015                0.572          -0.520\n6             -0.619               -0.590               -1.289          -0.437\n  SB:Hawthorne-Neighbors SB:Hawthorne-Self SB:Neighbors-Self\n1                 -0.212            -0.115             0.097\n2                  1.022             0.609            -0.417\n3                 -0.958            -0.085             0.878\n4                 -0.192            -0.172             0.020\n5                  1.587             0.496            -1.092\n6                 -0.700             0.153             0.853\n\n\n　ちなみに、df内にfemaleからvoted2004は連続している（names(df)で確認してみよう）。この場合は以下のように（female:voted2004）書き換えることもできる。\n\nblc_chk <- df |>\n  BalanceR(group = treatment,\n           cov   = female:voted2004)\n\nblc_chk\n\n　標準化差分（標準化バイアス）を用いたバランスチェックはそれぞれのペアごとに計算を行うため、グループが多い場合は凡例が圧迫される場合が多い。しかし、重要なのは標準化差分の最大値だろう。ペア1、2、3でバランスが取れても、ペア4のバランスが取られていない場合は無意味だからだ。また、標準化差分の場合、符号の意味はなく、絶対値が重要だ。また、バランスチェックにおいてグループごとの平均値や標準偏差は不要である。ここでsummary()関数を使うと、絶対値が最も大きい標準化差分のみ出力される。\n\nsummary(blc_chk)\n\n  Covariate Abs_Maximum_SB\n1    female          0.248\n2       yob          1.335\n3   hh_size          1.130\n4 voted2000          0.738\n5 voted2002          1.587\n6 voted2004          1.472\n\n\n　plot()関数を使えば、これらの結果を可視化することもできる。\n\nplot(blc_chk)\n\n\n\n\n図 1: ?(caption)\n\n\n\n\n　先ほど述べたようにバランスチェックで重要なのは絶対値が最も大きい標準化差分である。plot()内にsimplify = TRUEを指定すれば最大値のみ表示され、更にabs = TRUEにすると絶対値へ変換される。また、垂直のガイドラインはvline引数で変更できる。\n\n# plot() の第1引数は blc_chk なのでパイプの使える\nblc_chk |>\n  plot(vline = c(5, 10), simplify = TRUE, abs = TRUE)\n\n\n\n\n図 2: ?(caption)"
  },
  {
    "objectID": "material/intro_rct.html#処置効果の確認",
    "href": "material/intro_rct.html#処置効果の確認",
    "title": "因果推論の考え方とランダム化比較試験",
    "section": "処置効果の確認",
    "text": "処置効果の確認\n\nグループごとの応答変数の平均値\n　処置効果を確認するためには各グループごとの応答変数（ここではvoted2006）の平均値を計算し、処置群の平均値から統制群の平均値を引く必要がある。まずは、特定の変数の平均値を計算する方法について紹介する。データ内にある特定の変数の平均値を計算するためにはsummarise()関数内に平均値を求めるmean()関数を入れる。たとえば、dfのvoted2006の平均値を計算するコードは以下の通りである。\n\ndf |>\n  summarise(mean(voted2006, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  `mean(voted2006, na.rm = TRUE)`\n                            <dbl>\n1                           0.316\n\n\n　na.rm = TRUEは「欠損値があれば、それを除外する」を意味し、指定されていない場合（=既定値）はFALSEになる。今回は欠損値がないものの、念の為に入れておく。\n　出力結果を見ると、平均値が表示される列の名前が`mean(voted2006, na.rm = TRUE)`となっており、非常に見にくい。この場合、以下のようにmean()の前に出力される列名を予め指定することもできる。\n\ndf |>\n  # voted2006の平均値が表示される列名を Outcome にする。\n  summarise(Outcome = mean(voted2006, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  Outcome\n    <dbl>\n1   0.316\n\n\n　我々が知りたいのはvoted2006の平均値でなく、グループごとの平均値だろう。被験者がどのグループに属しているかわ示す変数はtreatmentであるが、summarise()にデータを渡す前にgroup_by()変数を使うと、グループごとに計算を行い、その結果を返す。\n\ndf |>\n  group_by(treatment) |>\n  summarise(Outcome = mean(voted2006, na.rm = TRUE))\n\n# A tibble: 5 × 2\n  treatment  Outcome\n  <chr>        <dbl>\n1 Civic Duty   0.315\n2 Control      0.297\n3 Hawthorne    0.322\n4 Neighbors    0.378\n5 Self         0.345\n\n\n　group_by()内でも=演算子を使うと、グループ名が出力される列名を変更することができる。\n\ndf |>\n  # グループ名が表示される列名を Group にする。\n  group_by(Groups = treatment) |>\n  summarise(Outcome = mean(voted2006, na.rm = TRUE))\n\n# A tibble: 5 × 2\n  Groups     Outcome\n  <chr>        <dbl>\n1 Civic Duty   0.315\n2 Control      0.297\n3 Hawthorne    0.322\n4 Neighbors    0.378\n5 Self         0.345\n\n\n　ここで一つ注目したいのが、グループの表示順番である。変数のデータ型が文字型だと（Rコンソール上でclass(df$treatment)を入力するか、dfの出力画面でtreatmentの下に<chr>と表示されていることで確認できる）、今のようにアルファベット順で表示される。しかし、統制群は最初か最後に来るのが通例である。この順番をアルファベット順でなく、任意の順番にするためにはtreatment変数をfactor型変数へ変換する必要がある。Factor型は「順序付きの文字型変数」だと理解しても良い4。列の追加・上書き（今回はtreatment列の上書き）の処理が必要なのでmutate()関数を使う。変数をfactor型に変換する関数はfactor()関数で、第1引数としてはfactor型へ変換する変数名を指定する。第2引数はlevelsであり、出力したい順番の文字型ベクトルを指定する。スペルミスに注意すること。\n\ndf |>\n  mutate(treatment = factor(treatment,\n                            levels = c(\"Control\", \"Civic Duty\",\n                                       \"Self\", \"Neighbors\", \"Hawthorne\")))\n\n# A tibble: 344,084 × 13\n   treatm…¹ female   yob hh_size voted…² voted…³ voted…⁴ voted…⁵ treat…⁶ treat…⁷\n   <fct>     <dbl> <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <int>   <int>\n 1 Civic D…      0  1941       2       0       1       0       0       1       0\n 2 Civic D…      1  1947       2       0       1       0       0       1       0\n 3 Hawthor…      0  1951       3       0       1       0       1       0       0\n 4 Hawthor…      1  1950       3       0       1       0       1       0       0\n 5 Hawthor…      1  1982       3       0       1       0       1       0       0\n 6 Control       0  1981       3       0       0       0       0       0       1\n 7 Control       1  1959       3       0       1       0       1       0       1\n 8 Control       0  1956       3       0       1       0       1       0       1\n 9 Control       1  1968       2       0       1       0       0       0       1\n10 Control       0  1967       2       0       1       0       0       0       1\n# … with 344,074 more rows, 3 more variables: treatment_Hawthorne <int>,\n#   treatment_Neighbors <int>, treatment_Self <int>, and abbreviated variable\n#   names ¹​treatment, ²​voted2000, ³​voted2002, ⁴​voted2004, ⁵​voted2006,\n#   ⁶​`treatment_Civic Duty`, ⁷​treatment_Control\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n　treatment列名の下が<fct>となっていることが分かる。これはtreatment列のデータ型がfactor型であることを意味する。問題なく動くことが確認できたので、dfを上書きしよう。\n\ndf <- df |>\n  mutate(treatment = factor(treatment,\n                            levels = c(\"Control\", \"Civic Duty\",\n                                       \"Self\", \"Neighbors\", \"Hawthorne\")))\n\n　それでは、改めてグループごとのvoted2006の平均値を計算してみよう。今回は計算結果をout_mean_dfという名のオブジェクトとして格納する。\n\nout_mean_df <- df |>\n  group_by(Groups = treatment) |>\n  summarise(Outcome = mean(voted2006, na.rm = TRUE))\n\nout_mean_df\n\n# A tibble: 5 × 2\n  Groups     Outcome\n  <fct>        <dbl>\n1 Control      0.297\n2 Civic Duty   0.315\n3 Self         0.345\n4 Neighbors    0.378\n5 Hawthorne    0.322\n\n\n　今回は統制群は最初に出力されていることが確認できる。\n　それではこの結果をグラフとして示してみよう。作図には{ggplot2}パッケージを使う。まずはout_mean_dfをggplot()関数に渡す。ggplot()関数以降は、+演算子を使ってレイヤーを足していくこととなる。棒グラフのレイヤーはgeom_bar()関数であり、その中にaes()関数を入れる。aes()の中には棒グラフの作図に必要な情報を入れる必要がある（これをマッピング（mapping）と呼ぶ）。棒グラフを作成するために必要な最低限の情報とは各棒の横軸上の位置（x）と棒の高さ（y）だ。今回は横軸がグループ名、縦軸が平均値となる棒グラフを作る。aes()外側にはstat = \"identity\"を忘れずに付けること。\n\nout_mean_df |>\n  ggplot() +\n  geom_bar(aes(x = Groups, y = Outcome), stat = \"identity\")\n\n\n\n\n図 3: ?(caption)\n\n\n\n\n　続いて、このグラフの見た目を調整してみよう。\n\nout_mean_df |>\n  ggplot() +\n  geom_bar(aes(x = Groups, y = Outcome), stat = \"identity\") +\n  # 縦軸（y軸）のラベルを変更する\n  labs(y = \"Mean(Outcome)\") +\n  # grayテーマ（デフォルトのテーマ）を使用し、フォントサイズは14\n  theme_gray(base_size = 14)\n\n\n\n\n図 4: ?(caption)\n\n\n\n\n　また、geom_label()レイヤーを足すと、棒の上にラベルを付けることもできる。ラベルに必要な情報は各ラベルの横軸上の位置（x）、縦軸上の位置（y）、ラベルの表示内容（label）だ。今回のラベルは平均値の具体的な数値を入れてみよう。\n\nout_mean_df |>\n  ggplot() +\n  geom_bar(aes(x = Groups, y = Outcome), stat = \"identity\") +\n  geom_label(aes(x = Groups, y = Outcome, label = Outcome)) +\n  labs(y = \"Mean(Outcome)\") +\n  theme_gray(base_size = 14)\n\n\n\n\n図 5: ?(caption)\n\n\n\n\n　小数点が長すぎるので3桁まで表示としよう。ここではsprintf()を使用する。使い方が簡単とは言えないが、覚える必要はなく、必要な時にググるか、本資料のコードをコピペすれば良い5。\n\nout_mean_df |>\n  ggplot() +\n  geom_bar(aes(x = Groups, y = Outcome), stat = \"identity\") +\n  # 2桁までなら %.3f を %.2f に変更\n  geom_label(aes(x = Groups, y = Outcome, label = sprintf(\"%.3f\", Outcome))) +\n  labs(y = \"Mean(Outcome)\") +\n  theme_gray(base_size = 14)\n\n\n\n\n図 6: ?(caption)\n\n\n\n\n　これで可視化ができた。ただし、以上のコードには改善の余地がある。geom_bar()とgeom_label()内のaes()関数に注目して欲しい。よく見るとxとyと同じだろう。geom_*()が共有するマッピングがあれば、ggplot()内で指定することでコードを効率化することもできる。\n\nout_mean_df |>\n  ggplot(aes(x = Groups, y = Outcome)) +\n  geom_bar(stat = \"identity\") +\n  geom_label(aes(label = sprintf(\"%.3f\", Outcome))) +\n  labs(y = \"Mean(Outcome)\") +\n  theme_gray(base_size = 14)\n\n\n\n\n図 7: ?(caption)\n\n\n\n\n\n\n統計的推定（単回帰分析）\n　これまでの作業はグループごとの応答変数の平均値であって、処置効果ではない。処置効果を計算するためには処置群の平均値から統制群の平均値を引く必要がある。たとえば、Civic Dutyはがき群の平均値は約0.315、統制群のそれは0.297であるため、Civic Dutyはがきの処置効果は約0.018である。しかし、これを各グループごとに計算することは面倒だし、何よりも得られた値が点推定値だという限界がある。得られた処置効果の不確実性は計算できない。\n　ここで有効なのが線形回帰分析である。回帰分析を行うことで処置効果の点推定値のみならず、不確実性の指標である標準誤差も計算され、区間推定や統計的仮説検定も可能となる。線形回帰分析の関数はlm()だ。第1引数としては回帰式であり、応答変数 ~ 説明変数と表記する。第2引数はdataであり、回帰式で指定した変数が入っているデータ名を指定する。回帰分析の結果は名前を付けてオブジェクトとして格納し、summary()関数を使うと、詳細が確認できる。\n\nfit1 <- lm(voted2006 ~ treatment, data = df)\n\nsummary(fit1)\n\n\nCall:\nlm(formula = voted2006 ~ treatment, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.3780 -0.3145 -0.2966  0.6549  0.7034 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         0.296638   0.001061 279.525  < 2e-16 ***\ntreatmentCivic Duty 0.017899   0.002600   6.884 5.85e-12 ***\ntreatmentSelf       0.048513   0.002600  18.657  < 2e-16 ***\ntreatmentNeighbors  0.081310   0.002601  31.263  < 2e-16 ***\ntreatmentHawthorne  0.025736   0.002601   9.896  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4641 on 344079 degrees of freedom\nMultiple R-squared:  0.003394,  Adjusted R-squared:  0.003383 \nF-statistic:   293 on 4 and 344079 DF,  p-value: < 2.2e-16\n\n\n　ちなみに、これもパイプ演算子を使うことができる。ただし、第1引数として渡すパイプ演算子の特徴上、そのまま使うことはできない。なぜならlm()関数の第1引数はデータでなく、回帰式（formula型）だから。この場合はプレースホルダー（place holder）を指定する必要がある。パイプ前のオブジェクトが入る位置を任意に指定することであり、_を使う。%>%演算子を使う場合は_でなく、.を使う。上記のコードと以下のコードは同じコードとなる。プレースホルダーは自分が使うパイプ演算子によって使い分けること。\n\nfit1 <- df |> # |> パイプを使う場合\n  lm(voted2006 ~ treatment, data = _)\n\nfit1 <- df %>% # %>% パイプを使う場合\n  lm(voted2006 ~ treatment, data = .)\n\n　Factor型、または文字型変数が説明変数の場合、自動的にダミー変数として処理され、Factor型の場合、最初の水準（ここでは\"Control\"）がベースカテゴリとなる。説明変数が文字型ならアルファベット順で最初の水準がベースカテゴリとなり、今回の例だと\"Civic Duty\"がベースカテゴリとなる。処置効果は「統制群に比べて〜」が重要となるので、数値型以外の説明変数は予めfactor化しておいた方が望ましい。\n　Civic Dutyの推定値は約0.018であり、これは統制群に比べ、Civic Duty群のvoted2006の平均値は約0.018高いことを意味する。応答変数が0、1であるため、これを割合（=投票率）で換算すると、約1.8%p高いことを意味する。つまり、Civic Dutyのはがきをもらった被験者はそうでない被験者に比べて投票率が約1.8%p高いことを意味する。他の推定値も同じやり方で解釈すれば良い。\n　それではこれらの処置効果が統計的に有意なものかを確認してみよう。統計的有意か否かを判定するためには有意と非有意の境界線が必要である、これは通常、有意水準（significance level; \\(\\alpha\\)）と呼ばれる。この有意水準は分析者が決めるものであるが、社会科学で広く使われる基準は\\(\\alpha = 0.05\\)、つまり5%だ。分析結果の画面にはPr(>|t|)列が表示されているが、これが\\(p\\)値と呼ばれるもので、これが0.05を下回る場合、統計的に有意と判定する。もし、\\(\\alpha = 0.1\\)を採用するなら、\\(p < 0.1\\)の場合において統計的に有意と判定する。Civic Dutyの\\(p\\)値は5.85e-12であり、これは\\(5.75 \\times 10^{-12}\\)を意味する。\\(10^{-1}\\)は0.1、\\(10^{-2}\\)は0.01であることを考えると非常に小さい数値であり、統計的に有意であると考えられる。また、\\(p\\)値が一定値以下であれば< 2e-16と表示される。4つの処置群において処置効果は統計的に有意であると判定できよう。\n　続いて、この結果を可視化してみよう。ここでも{ggplot2}パッケージを使って可視化をするが、{ggplot2}で使用可能なオブジェクトは表形式のデータである。Rコンソール上でclass(オブジェクト名)を入力すると、データのクラスが出力されるが、このクラスに\"data.frame\"があれば、{ggplot2}で使用できる。たとえば、fit1オブジェクトのクラスは\"lm\"であるため、そのまま{ggplot2}で使うことはできない。\n\nclass(fit1)\n\n[1] \"lm\"\n\n\n　推定結果を表形式に変換するためには{broom}パッケージのtidy()関数が便利だ。使い方は簡単でtidy()内に回帰分析の推定結果が格納されたオブジェクトを入れるだけである。ただし、デフォルトの設定では95%信頼区間が表示されないため、中にはconf.int = TRUEを追加しておく必要がある。\n\n# 90%信頼区間を使うのであれば conf.int = 0.9 を追加（デフォルトは0.95）\nfit1_coef <- tidy(fit1, conf.int = TRUE)\n\nfit1_coef\n\n# A tibble: 5 × 7\n  term                estimate std.error statistic   p.value conf.low conf.high\n  <chr>                  <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)           0.297    0.00106    280.   0           0.295     0.299 \n2 treatmentCivic Duty   0.0179   0.00260      6.88 5.85e- 12   0.0128    0.0230\n3 treatmentSelf         0.0485   0.00260     18.7  1.22e- 77   0.0434    0.0536\n4 treatmentNeighbors    0.0813   0.00260     31.3  2.94e-214   0.0762    0.0864\n5 treatmentHawthorne    0.0257   0.00260      9.90 4.37e- 23   0.0206    0.0308\n\nclass(fit1_coef)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\n　fit1_coefのクラスに\"data.frame\"が含まれているので、これを使って作図することができる。\n　作図する前に、fit1_coefの加工しておきたい。それぞれの係数（estimate列）は処置効果を表しているが、切片（\"(Intercept)\"）の推定値は処置効果とは無関係である。したがって、予め切片の行を除外しておきたい。特定の行を残したり、除外する関数はfilter()である。今回はterm列の値が\"(Intercept)\"ではない行を残したいので、同値演算子（==）の否定を意味する!=演算子を使用する。\n\nfit1_coef <- fit1_coef |>\n  filter(term != \"(Intercept)\")\n\nfit1_coef\n\n# A tibble: 4 × 7\n  term                estimate std.error statistic   p.value conf.low conf.high\n  <chr>                  <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 treatmentCivic Duty   0.0179   0.00260      6.88 5.85e- 12   0.0128    0.0230\n2 treatmentSelf         0.0485   0.00260     18.7  1.22e- 77   0.0434    0.0536\n3 treatmentNeighbors    0.0813   0.00260     31.3  2.94e-214   0.0762    0.0864\n4 treatmentHawthorne    0.0257   0.00260      9.90 4.37e- 23   0.0206    0.0308\n\n\n　それでは作図に入ろう。処置効果を示す場合は、点推定値以外にもその不確実性を示すのは一般的である。不確実性の指標として幅広く使われるのは標準誤差（standard error; 標準偏差ではない）であるが、可視化の際にはこの標準誤差に基づき計算した信頼区間を示すのが一般的だ。有意水準が5%であれば、95%信頼区間を示し、10%なら90%信頼区間を用いる。\n　点と区間を同時に示すプロットがpoint-rangeプロットであり、{ggplot2}ではgeom_pointrange()レイヤーを使う。必要な情報はpoint-rangeの横軸上の位置（x）、点の縦軸上の位置（y）、区間の上限（ymax）と下限（ymin）である。これらの情報は全てfit1_coefに入っているため、fit1_coefをそのままggplot()関数に渡して作図することができる。\n\nfit1_coef |>\n  ggplot() +\n  geom_pointrange(aes(x = term, y = estimate,\n                      ymin = conf.low, ymax = conf.high))\n\n\n\n\n図 8: ?(caption)\n\n\n\n\n　それでは図をカスタマイズしてみよう。図内の様々なラベルを修正するlabs()レイヤーでラベルを修正する。テーマはデフォルトのtheme_gray()の代わりに白黒テーマ（theme_bw()）を使用し、フォントサイズは12とする。また、y = 0の水平線を追加する。95%信頼区間内に0が含まれる場合、「5%水準で統計的に有意でない」と判断できる。水平線を描くにはgeom_hline()レイヤーを追加し、yintercept = 0を指定することで、0のところに水平線が描ける。\n\nfit1_coef |>\n  ggplot() +\n  geom_hline(yintercept = 0) +\n  geom_pointrange(aes(x = term, y = estimate,\n                      ymin = conf.low, ymax = conf.high)) +\n  labs(x = \"Treatments\", y = \"Average Treatment Effects\") +\n  theme_bw(base_size = 12)\n\n\n\n\n図 9: ?(caption)\n\n\n\n\n　まだ気になる点がある。それは横軸の目盛りラベルにtreatmentという不要な情報がある点だ。これは作図の時点で修正することも可能だが、まずはdfのterm変数の値を修正する方法を紹介する。変数の値を修正する時にはrecode()関数を使用する。第1引数はリコーディングする変数名であり、引き続き\"元の値\" = \"新しい値\"を指定すれば良い。スペルミスに注意すること。\n\nfit1_coef <- fit1_coef |>\n  mutate(term = recode(term,\n                       \"treatmentCivic Duty\" = \"Civic Duty\",\n                       \"treatmentHawthorne\"  = \"Hawthorne\",\n                       \"treatmentNeighbors\"  = \"Neighbors\",\n                       \"treatmentSelf\"       = \"Self\"))\n\nfit1_coef\n\n# A tibble: 4 × 7\n  term       estimate std.error statistic   p.value conf.low conf.high\n  <chr>         <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 Civic Duty   0.0179   0.00260      6.88 5.85e- 12   0.0128    0.0230\n2 Self         0.0485   0.00260     18.7  1.22e- 77   0.0434    0.0536\n3 Neighbors    0.0813   0.00260     31.3  2.94e-214   0.0762    0.0864\n4 Hawthorne    0.0257   0.00260      9.90 4.37e- 23   0.0206    0.0308\n\n\n　以上の作業はterm列の各値から\"treatment\"文字を\"\"に置換することなので、文字列を置換する関数であるstr_replace()を使えば、より短くすることができる。\n\nfit1_coef <- fit1_coef |>\n  mutate(term = str_replace(term, \"treatment\", \"\"))\n\n　fit1_coefも修正できたので、 図 9 と同じコードでもう一度作図してみよう。\n\nfit1_coef |>\n  ggplot() +\n  geom_hline(yintercept = 0) +\n  geom_pointrange(aes(x = term, y = estimate,\n                      ymin = conf.low, ymax = conf.high)) +\n  labs(x = \"Treatments\", y = \"Average Treatment Effects\") +\n  theme_bw(base_size = 12)\n\n\n\n\n図 10: ?(caption)\n\n\n\n\n　最後に横軸の順番を修正してみよう。fit1_coefのterm列は文字型変数であるため、アルファベット順になる。これをdfのtreatment列と同様、Civic Duty、Self、Neighbors、Hawthorneの順にしたい。この場合fit1_coefのterm列をfactor化すれば良い。factor()関数を使っても良いが、ここではまた便利な技を紹介しよう。それはfct_inorder()関数だ。これは表示されている順番をfactorの順番とする関数だ。実際、fit1_coefの中身を見ると、表示順番はCivic Duty、Self、Neighbors、Hawthorneだ。非常に嬉しい状況なので、fct_inorder()を使ってみよう。\n\nfit1_coef <- fit1_coef |>\n  mutate(term = fct_inorder(term))\n\nfit1_coef\n\n# A tibble: 4 × 7\n  term       estimate std.error statistic   p.value conf.low conf.high\n  <fct>         <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 Civic Duty   0.0179   0.00260      6.88 5.85e- 12   0.0128    0.0230\n2 Self         0.0485   0.00260     18.7  1.22e- 77   0.0434    0.0536\n3 Neighbors    0.0813   0.00260     31.3  2.94e-214   0.0762    0.0864\n4 Hawthorne    0.0257   0.00260      9.90 4.37e- 23   0.0206    0.0308\n\n\n　それでは、 図 10 と同じコードでもう一度作図してみよう。\n\nfit1_coef |>\n  ggplot() +\n  geom_hline(yintercept = 0) +\n  geom_pointrange(aes(x = term, y = estimate,\n                      ymin = conf.low, ymax = conf.high)) +\n  labs(x = \"Treatments\", y = \"Average Treatment Effects\") +\n  theme_bw(base_size = 12)\n\n\n\n\n図 11: ?(caption)\n\n\n\n\n　これで処置効果の可視化もバッチリだ。\n\n\n多重比較の問題\n　グループが2つ、つまり統制群と統制群のみが存在する場合、我々が比較を行う回数は1回のみである（統制群 - 処置群）。しかし、今回のデータの場合、処置群は4つである。これは比較を4回行うことを意味する。具体的には「統制群 - 処置群1」、「統制群 - 処置群2」、「統制群 - 処置群3」、「統制群 - 処置群4」だ。比較を繰り返すほど、統計的に有意な結果が得られる可能性は高い。極端な話、1000回程度検定を繰り返せば、本当は効果がなくてもたまたま統計的に有意な結果が何回かは得られるだろう。これが多重検定（multiple testing）の問題である。したがって、比較の回数が多くなるにつれ、統計的有意性検定にも何らかのペナルティーを課す必要がある。\n　多重比較におけるペナルティーの付け方はいくつかあるが、ここでは最も保守的な（=研究者にとって都合の悪い）補正法であるボンフェローニ補正（Bonferroni correction）を紹介する。これは非常に単純で、\\(p\\)値や信頼区間を計算する際、「統計的有意」と判定されるハードルを上げる方法である。予め決めておいた有意水準（\\(\\alpha\\)）が0.05で、比較の回数が4回であれば、\\(p\\)値が\\(0.05 \\times \\frac{1}{4} = 0.0125\\)を下回る場合において「5%水準で有意である」と判定する。信頼区間でいえば通常の95%信頼区間（1 - 0.05）でなく、98.75%信頼区間（1 - 0.0125）を使うこととなる。この結果、統計的に有意な結果が得られたら「1.25%水準で〜」と解釈するのではなく、「5%水準で〜」と解釈する必要がある。\n　95%以外の信頼区間を求めるのは簡単で、tidy()関数内にconf.levelを修正すれば良い。指定されていない場合はデフォルトで0.95が割り当てられているが、これを0.9875と修正する。\n\nfit1_coef <- tidy(fit1, conf.int = TRUE, conf.level = 0.9875)\n\nfit1_coef\n\n# A tibble: 5 × 7\n  term                estimate std.error statistic   p.value conf.low conf.high\n  <chr>                  <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)           0.297    0.00106    280.   0           0.294     0.299 \n2 treatmentCivic Duty   0.0179   0.00260      6.88 5.85e- 12   0.0114    0.0244\n3 treatmentSelf         0.0485   0.00260     18.7  1.22e- 77   0.0420    0.0550\n4 treatmentNeighbors    0.0813   0.00260     31.3  2.94e-214   0.0748    0.0878\n5 treatmentHawthorne    0.0257   0.00260      9.90 4.37e- 23   0.0192    0.0322\n\n\n　それでは 図 11 と同じ図を作ってみよう。まず、切片の行を除外するが、ここではfilter()を使わず、slice()の使った方法を紹介する。slice()は()内に指定した行を残す関数だ。たとえば、slice(fit1_coef, 2)ならfit1_coefの2行目のみを残す。fit1_coefはslice()の第1引数だから、パイプ演算子を使うことも可能で、こちらの方を推奨する。そうすれば()内には残す行のみの指定で済む。slice(2)のみなら2行目を残し、slice(1, 3, 5)なら1、3、5行目を残す。:を使うと「〜行目から〜行目まで」の指定ができる。処置効果の係数はfit1_coefの2行目から5行目までなので、2:5と指定すれば良い。\n\nfit1_coef <- fit1_coef |>\n  slice(2:5)\n\nfit1_coef\n\n# A tibble: 4 × 7\n  term                estimate std.error statistic   p.value conf.low conf.high\n  <chr>                  <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 treatmentCivic Duty   0.0179   0.00260      6.88 5.85e- 12   0.0114    0.0244\n2 treatmentSelf         0.0485   0.00260     18.7  1.22e- 77   0.0420    0.0550\n3 treatmentNeighbors    0.0813   0.00260     31.3  2.94e-214   0.0748    0.0878\n4 treatmentHawthorne    0.0257   0.00260      9.90 4.37e- 23   0.0192    0.0322\n\n\n　続いて、term変数の値から\"treatment\"の文字を除去し、fit1_coefでの出力順番でtermをfactor化する。\n\nfit1_coef <- fit1_coef |>\n  mutate(term = recode(term,\n                       \"treatmentCivic Duty\" = \"Civic Duty\",\n                       \"treatmentHawthorne\"  = \"Hawthorne\",\n                       \"treatmentNeighbors\"  = \"Neighbors\",\n                       \"treatmentSelf\"       = \"Self\"),\n         term = fct_inorder(term))\n\nfit1_coef\n\n# A tibble: 4 × 7\n  term       estimate std.error statistic   p.value conf.low conf.high\n  <fct>         <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 Civic Duty   0.0179   0.00260      6.88 5.85e- 12   0.0114    0.0244\n2 Self         0.0485   0.00260     18.7  1.22e- 77   0.0420    0.0550\n3 Neighbors    0.0813   0.00260     31.3  2.94e-214   0.0748    0.0878\n4 Hawthorne    0.0257   0.00260      9.90 4.37e- 23   0.0192    0.0322\n\n\n　最後に 図 11 と同じコードで作図する。\n\nfit1_coef |>\n  ggplot() +\n  geom_hline(yintercept = 0) +\n  geom_pointrange(aes(x = term, y = estimate,\n                      ymin = conf.low, ymax = conf.high)) +\n  labs(x = \"Treatments\", \n       y = \"Average Treatment Effects (w/ 98.75% CI)\") +\n  theme_bw(base_size = 12)\n\n\n\n\n図 12: ?(caption)\n\n\n\n\n\n\n統計的推定（重回帰分析）\n　今回の例は無作為割当が成功しており、処置前変数の偏りは見られない。しかし、何らかの理由で処置前変数の偏りが生じる場合がある。その「何らかの理由」が応答変数にまで影響を与えるのであれば、それは交絡変数（confounder）となり、バイアスの原因となる。この場合、偏りが生じている処置前変数を統制（control）することによってバイアスを小さくすることができる。今回は不要であるが、性別や誕生年などの共変量を統制した推定をしてみよう。\n　やり方は簡単で、lm()内の回帰式を応答変数 ~ 説明変数1 + 説明変数2 + ...のように説明変数を+で足していけば良い。\n\nfit2 <-lm(voted2006 ~ treatment + female + yob + hh_size +\n            voted2000 + voted2002 + voted2004, data = df)\n\nsummary(fit2)\n\n\nCall:\nlm(formula = voted2006 ~ treatment + female + yob + hh_size + \n    voted2000 + voted2002 + voted2004, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.7679 -0.3344 -0.1953  0.5300  0.9359 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          5.845e+00  1.099e-01  53.202  < 2e-16 ***\ntreatmentCivic Duty  1.839e-02  2.504e-03   7.343 2.10e-13 ***\ntreatmentSelf        4.797e-02  2.504e-03  19.157  < 2e-16 ***\ntreatmentNeighbors   8.064e-02  2.504e-03  32.199  < 2e-16 ***\ntreatmentHawthorne   2.506e-02  2.504e-03  10.005  < 2e-16 ***\nfemale              -5.783e-03  1.525e-03  -3.791  0.00015 ***\nyob                 -2.913e-03  5.642e-05 -51.633  < 2e-16 ***\nhh_size              5.075e-03  1.018e-03   4.986 6.17e-07 ***\nvoted2000            9.401e-02  1.783e-03  52.722  < 2e-16 ***\nvoted2002            1.414e-01  1.590e-03  88.876  < 2e-16 ***\nvoted2004            1.578e-01  1.564e-03 100.838  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4469 on 344073 degrees of freedom\nMultiple R-squared:  0.07592,   Adjusted R-squared:  0.0759 \nF-statistic:  2827 on 10 and 344073 DF,  p-value: < 2.2e-16\n\n\n　{modelsummary}パッケージのmodelsummary()関数を使えば、推定結果がより見やすくなる。\n\nmodelsummary(fit2)\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    5.845 \n  \n  \n     \n    (0.110) \n  \n  \n    treatmentCivic Duty \n    0.018 \n  \n  \n     \n    (0.003) \n  \n  \n    treatmentSelf \n    0.048 \n  \n  \n     \n    (0.003) \n  \n  \n    treatmentNeighbors \n    0.081 \n  \n  \n     \n    (0.003) \n  \n  \n    treatmentHawthorne \n    0.025 \n  \n  \n     \n    (0.003) \n  \n  \n    female \n    −0.006 \n  \n  \n     \n    (0.002) \n  \n  \n    yob \n    −0.003 \n  \n  \n     \n    (0.000) \n  \n  \n    hh_size \n    0.005 \n  \n  \n     \n    (0.001) \n  \n  \n    voted2000 \n    0.094 \n  \n  \n     \n    (0.002) \n  \n  \n    voted2002 \n    0.141 \n  \n  \n     \n    (0.002) \n  \n  \n    voted2004 \n    0.158 \n  \n  \n     \n    (0.002) \n  \n  \n    Num.Obs. \n    344084 \n  \n  \n    R2 \n    0.076 \n  \n  \n    R2 Adj. \n    0.076 \n  \n  \n    AIC \n    422192.9 \n  \n  \n    BIC \n    422321.9 \n  \n  \n    Log.Lik. \n    −211084.453 \n  \n  \n    F \n    2826.942 \n  \n  \n    RMSE \n    0.45 \n  \n\n\n\n\n\n　また、複数のモデルをlist()関数でまとめると、モデル間比較もできる。\n\nmodelsummary(list(\"w/o Covariates\" = fit1, \"w/ Covariates\" = fit2))\n\n\n\n \n  \n      \n    w/o Covariates \n    w/ Covariates \n  \n \n\n  \n    (Intercept) \n    0.297 \n    5.845 \n  \n  \n     \n    (0.001) \n    (0.110) \n  \n  \n    treatmentCivic Duty \n    0.018 \n    0.018 \n  \n  \n     \n    (0.003) \n    (0.003) \n  \n  \n    treatmentSelf \n    0.049 \n    0.048 \n  \n  \n     \n    (0.003) \n    (0.003) \n  \n  \n    treatmentNeighbors \n    0.081 \n    0.081 \n  \n  \n     \n    (0.003) \n    (0.003) \n  \n  \n    treatmentHawthorne \n    0.026 \n    0.025 \n  \n  \n     \n    (0.003) \n    (0.003) \n  \n  \n    female \n     \n    −0.006 \n  \n  \n     \n     \n    (0.002) \n  \n  \n    yob \n     \n    −0.003 \n  \n  \n     \n     \n    (0.000) \n  \n  \n    hh_size \n     \n    0.005 \n  \n  \n     \n     \n    (0.001) \n  \n  \n    voted2000 \n     \n    0.094 \n  \n  \n     \n     \n    (0.002) \n  \n  \n    voted2002 \n     \n    0.141 \n  \n  \n     \n     \n    (0.002) \n  \n  \n    voted2004 \n     \n    0.158 \n  \n  \n     \n     \n    (0.002) \n  \n  \n    Num.Obs. \n    344084 \n    344084 \n  \n  \n    R2 \n    0.003 \n    0.076 \n  \n  \n    R2 Adj. \n    0.003 \n    0.076 \n  \n  \n    AIC \n    448179.9 \n    422192.9 \n  \n  \n    BIC \n    448244.4 \n    422321.9 \n  \n  \n    Log.Lik. \n    −224083.935 \n    −211084.453 \n  \n  \n    F \n    292.976 \n    2826.942 \n  \n  \n    RMSE \n    0.46 \n    0.45 \n  \n\n\n\n\n\n　modelsummary()は推定値と標準誤差（カッコ内）が別々の行として出力する。これを一行でまとめるためには、以下のようにコードを修正する。\n\nmodelsummary(list(\"w/o Covariates\" = fit1, \"w/ Covariates\" = fit2),\n             estimate  = \"{estimate} ({std.error})\",\n             statistic = NULL)\n\n\n\n \n  \n      \n    w/o Covariates \n    w/ Covariates \n  \n \n\n  \n    (Intercept) \n    0.297 (0.001) \n    5.845 (0.110) \n  \n  \n    treatmentCivic Duty \n    0.018 (0.003) \n    0.018 (0.003) \n  \n  \n    treatmentSelf \n    0.049 (0.003) \n    0.048 (0.003) \n  \n  \n    treatmentNeighbors \n    0.081 (0.003) \n    0.081 (0.003) \n  \n  \n    treatmentHawthorne \n    0.026 (0.003) \n    0.025 (0.003) \n  \n  \n    female \n     \n    −0.006 (0.002) \n  \n  \n    yob \n     \n    −0.003 (0.000) \n  \n  \n    hh_size \n     \n    0.005 (0.001) \n  \n  \n    voted2000 \n     \n    0.094 (0.002) \n  \n  \n    voted2002 \n     \n    0.141 (0.002) \n  \n  \n    voted2004 \n     \n    0.158 (0.002) \n  \n  \n    Num.Obs. \n    344084 \n    344084 \n  \n  \n    R2 \n    0.003 \n    0.076 \n  \n  \n    R2 Adj. \n    0.003 \n    0.076 \n  \n  \n    AIC \n    448179.9 \n    422192.9 \n  \n  \n    BIC \n    448244.4 \n    422321.9 \n  \n  \n    Log.Lik. \n    −224083.935 \n    −211084.453 \n  \n  \n    F \n    292.976 \n    2826.942 \n  \n  \n    RMSE \n    0.46 \n    0.45 \n  \n\n\n\n\n\n　また、alignで各列を左寄せや右寄せに（文字列は左寄せ、数値は右寄せが一般的）、coef_rename引数で表示される変数名を変更することもできる。\n\nmodelsummary(list(\"w/o Covariates\" = fit1, \"w/ Covariates\" = fit2),\n             estimate  = \"{estimate} ({std.error})\",\n             statistic = NULL,\n             align = \"lrr\", # 1列は左寄せ、2列は右寄せ、3列は右寄せ\n             coef_rename = c(\"treatmentCivic Duty\" = \"Civic Duty\",\n                             \"treatmentSelf\"       = \"Self\",\n                             \"treatmentNeighbors\"  = \"Neighbors\",\n                             \"treatmentHawthorne\"  = \"Hawthorne\",\n                             \"female\"              = \"Female\",\n                             \"yob\"                 = \"Year of Birth\",\n                             \"hh_size\"             = \"Household Size\",\n                             \"voted2000\"           = \"Voted (2000)\",\n                             \"voted2002\"           = \"Voted (2002)\",\n                             \"voted2004\"           = \"Voted (2004)\"))\n\n\n\n \n  \n      \n    w/o Covariates \n    w/ Covariates \n  \n \n\n  \n    (Intercept) \n    0.297 (0.001) \n    5.845 (0.110) \n  \n  \n    Civic Duty \n    0.018 (0.003) \n    0.018 (0.003) \n  \n  \n    Self \n    0.049 (0.003) \n    0.048 (0.003) \n  \n  \n    Neighbors \n    0.081 (0.003) \n    0.081 (0.003) \n  \n  \n    Hawthorne \n    0.026 (0.003) \n    0.025 (0.003) \n  \n  \n    Female \n     \n    −0.006 (0.002) \n  \n  \n    Year of Birth \n     \n    −0.003 (0.000) \n  \n  \n    Household Size \n     \n    0.005 (0.001) \n  \n  \n    Voted (2000) \n     \n    0.094 (0.002) \n  \n  \n    Voted (2002) \n     \n    0.141 (0.002) \n  \n  \n    Voted (2004) \n     \n    0.158 (0.002) \n  \n  \n    Num.Obs. \n    344084 \n    344084 \n  \n  \n    R2 \n    0.003 \n    0.076 \n  \n  \n    R2 Adj. \n    0.003 \n    0.076 \n  \n  \n    AIC \n    448179.9 \n    422192.9 \n  \n  \n    BIC \n    448244.4 \n    422321.9 \n  \n  \n    Log.Lik. \n    −224083.935 \n    −211084.453 \n  \n  \n    F \n    292.976 \n    2826.942 \n  \n  \n    RMSE \n    0.46 \n    0.45 \n  \n\n\n\n\n\n　処置効果に注目すると、共変量の有無が推定結果に影響をほぼ与えないことが分かる。これは無作為割当に成功したことを意味する。"
  },
  {
    "objectID": "material/intro_rct.html#番外編",
    "href": "material/intro_rct.html#番外編",
    "title": "因果推論の考え方とランダム化比較試験",
    "section": "番外編",
    "text": "番外編\n　modelsummary()を使えば、複数のモデルの推定結果を一つの表としてまとめられる。しかし、図の場合はどうだろう。共変量なしモデルとありモデルを 図 12 のように一つにまとめることはできるだろうか。もちろん出来る。\n　まず、重回帰分析を行った結果（fit2）から処置効果の推定値情報を抽出し、fit1_coefと同じ構造のデータとしてまとめる。\n\nfit2_coef <- tidy(fit2, conf.int = TRUE, conf.level = 0.9875)\n\nfit2_coef <- fit2_coef |>\n  slice(2:5) |>\n  mutate(term = recode(term,\n                       \"treatmentCivic Duty\" = \"Civic Duty\",\n                       \"treatmentHawthorne\"  = \"Hawthorne\",\n                       \"treatmentNeighbors\"  = \"Neighbors\",\n                       \"treatmentSelf\"       = \"Self\"),\n         term = fct_inorder(term))\n\nfit2_coef\n\n# A tibble: 4 × 7\n  term       estimate std.error statistic   p.value conf.low conf.high\n  <fct>         <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 Civic Duty   0.0184   0.00250      7.34 2.10e- 13   0.0121    0.0246\n2 Self         0.0480   0.00250     19.2  9.27e- 82   0.0417    0.0542\n3 Neighbors    0.0806   0.00250     32.2  3.92e-227   0.0744    0.0869\n4 Hawthorne    0.0251   0.00250     10.0  1.45e- 23   0.0188    0.0313\n\n\n　処置効果の推定値や標準誤差などが異なるが、構造としては同じである。続いて、bind_rows()を用い、この2つのデータを一つの表として結合する。2つの表はlist()関数でまとめるが、それぞれ\"モデル名\" = データ名と指定する。最後に、.id = \"Model\"を追加する。\n\nbind_rows(list(\"Model 1\" = fit1_coef, \n               \"Model 2\" = fit2_coef),\n          .id = \"Model\")\n\n# A tibble: 8 × 8\n  Model   term       estimate std.error statistic   p.value conf.low conf.high\n  <chr>   <fct>         <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 Model 1 Civic Duty   0.0179   0.00260      6.88 5.85e- 12   0.0114    0.0244\n2 Model 1 Self         0.0485   0.00260     18.7  1.22e- 77   0.0420    0.0550\n3 Model 1 Neighbors    0.0813   0.00260     31.3  2.94e-214   0.0748    0.0878\n4 Model 1 Hawthorne    0.0257   0.00260      9.90 4.37e- 23   0.0192    0.0322\n5 Model 2 Civic Duty   0.0184   0.00250      7.34 2.10e- 13   0.0121    0.0246\n6 Model 2 Self         0.0480   0.00250     19.2  9.27e- 82   0.0417    0.0542\n7 Model 2 Neighbors    0.0806   0.00250     32.2  3.92e-227   0.0744    0.0869\n8 Model 2 Hawthorne    0.0251   0.00250     10.0  1.45e- 23   0.0188    0.0313\n\n\n　2つの表が1つとなり、Modelという列が追加される（これは.idで指定した名前）。そして、fit1_coefだった行は\"Model 1\"、fit2_coefだった行は\"Model 2\"が付く。ただし、これだけだと表が結合されて出力されるだけなので、fit_coefという名のオブジェクトとして作業環境内に格納しておく。\n\nfit_coef <- bind_rows(list(\"Model 1\" = fit1_coef, \n                           \"Model 2\" = fit2_coef),\n                      .id = \"Model\")\n\n　それではfit_coefを使って、作図をしてみよう。コードは 図 12 と同じであるが、facet_wrap()レイヤーを追加する。これはグラフのファセット（facet）分割を意味し、ファセットとは「面」を意味する。()内には~分割の基準となる変数名を入れる。2つのモデルがあり、fit_coefだとModel列がどのモデルの推定値かを示している。\n\nfit_coef |>\n  ggplot() +\n  geom_hline(yintercept = 0) +\n  geom_pointrange(aes(x = term, y = estimate,\n                      ymin = conf.low, ymax = conf.high)) +\n  labs(x = \"Treatments\", y = \"Average Treatment Effects\") +\n  facet_wrap(~ Model) +\n  theme_bw(base_size = 12)\n\n\n\n\n図 13: ?(caption)\n\n\n\n\n　今回の結果だとモデル1もモデル2も推定値がほぼ同じである。ファセット分割の場合、小さい差の比較が難しいというデメリットがある。この場合、ファセット分割をせず、一つのファセットにpoint-rangeの色分けした方が読みやすくなる。point-rangeをModelの値に応じて色分けする場合、aes()内にcolor = Modelを追加する。\n\nfit_coef |>\n  ggplot() +\n  geom_hline(yintercept = 0) +\n  geom_pointrange(aes(x = term, y = estimate,\n                      ymin = conf.low, ymax = conf.high,\n                      color = Model)) +\n  labs(x = \"Treatments\", y = \"Average Treatment Effects\") +\n  theme_bw(base_size = 12)\n\n\n\n\n図 14: ?(caption)\n\n\n\n\n　何かおかしい。point-rangeの横軸上の位置が同じということから重なってしまい、モデル1のpoint-rangeがよく見えない。これをずらすためにaes()の外側にposition = position_dodge2(1/2)を追加する。\n\nfit_coef |>\n  ggplot() +\n  geom_hline(yintercept = 0) +\n  geom_pointrange(aes(x = term, y = estimate,\n                      ymin = conf.low, ymax = conf.high,\n                      color = Model),\n                  position = position_dodge2(1/2)) +\n  labs(x = \"Treatments\", y = \"Average Treatment Effects\") +\n  theme_bw(base_size = 12)\n\n\n\n\n図 15: ?(caption)\n\n\n\n\n　これで図は完成だが、少し修正してみよう。{ggplot2}の場合、凡例は右側に表示されるが、これを下側へ移動させるためにはtheme()レイヤーを追加し、legend.position = \"bottom\"を指定する。また、モデル1とモデル2が具体的に何を意味するのかを明確に示したい。これはfit_coefのModel列を修正しても良いが、今回はscale_color_discrete()レイヤーで修正する例を紹介する。\n\nfit_coef |>\n  ggplot() +\n  geom_hline(yintercept = 0) +\n  geom_pointrange(aes(x = term, y = estimate,\n                      ymin = conf.low, ymax = conf.high,\n                      color = Model),\n                  position = position_dodge2(1/2)) +\n  labs(x = \"Treatments\", y = \"Average Treatment Effects\") +\n  scale_color_discrete(labels = c(\"Model 1\" = \"w/o Covariates\",\n                                  \"Model 2\" = \"w/ Covariates\")) +\n  theme_bw(base_size = 12) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n図 16: ?(caption)"
  },
  {
    "objectID": "material/matching.html",
    "href": "material/matching.html",
    "title": "マッチング",
    "section": "",
    "text": "新しいタブで開く"
  },
  {
    "objectID": "material/iv.html",
    "href": "material/iv.html",
    "title": "操作変数法",
    "section": "",
    "text": "新しいタブで開く"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "実習用データ集",
    "section": "",
    "text": "ファイル名\n備考\n\n\n\n\n1\nintro_data2.csv\n第1回スライド\n\n\n1\nmatching_data1.csv\n第2回スライド\n\n\n1\nmatching_data2.csv\n第2回スライド\n\n\n1\nmatching_data3.csv\n第2回スライド\n\n\n1\nmatching_data4.csv\n第2回スライド"
  },
  {
    "objectID": "material/matching.html#因果推論と内生性",
    "href": "material/matching.html#因果推論と内生性",
    "title": "マッチング",
    "section": "因果推論と内生性",
    "text": "因果推論と内生性\n内生性: 処置変数と誤差項間の相関関係\n\n内生性は因果推論の敵\n例\n\n処置変数 = ソンさんの講義を履修するか否か\n結果変数 = 10年後の年収\nもし、やる気のある学生が履修する傾向があるとしたら?\nやる気のある学生は履修の有無と関係なく、高所得者になりやすい。\n\\(\\rightarrow\\) 「やる気」は処置と結果、両方と連関している\n\n\n\n.center[内生性を除去する最良の手法 \\(\\rightarrow\\) RCT]"
  },
  {
    "objectID": "material/matching.html#rctの限界",
    "href": "material/matching.html#rctの限界",
    "title": "マッチング",
    "section": "RCTの限界",
    "text": "RCTの限界\n\n高費用\n\n数万〜数億円\n\n倫理的な問題による実行不可能性\n\n喫煙と健康\nPhilip Zimbardo. 2008. The Lucifer Effect: How Good People Turn Evil. Rider.\n\n外的妥当性の問題\n\nMichael G. Findley, Kyosuke Kikuta, and Michael Denly. 2021. “External Validity,” Annual Review of Political Science, 24:365-393.\n\n回顧的因果推論には不向き\n\n主に介入 (intervention)の効果が推定対象"
  },
  {
    "objectID": "material/matching.html#因果推論",
    "href": "material/matching.html#因果推論",
    "title": "マッチング",
    "section": "因果推論",
    "text": "因果推論\nThe Three Layer Causal Hierarchy (Pearl 2009: p. 29)\n\nLevel1: Association\n\\(P(y|x)\\)\nTypical Activity: Seeing\nTypical Question:\n\nWhat is?\nHow would seeing \\(X\\) change my belief in \\(Y\\) ?\n\nExamples:\n\nWhat does a symptom tell me about a disease?\nWhat does a survey tell us about the election results?"
  },
  {
    "objectID": "material/matching.html#因果推論-1",
    "href": "material/matching.html#因果推論-1",
    "title": "マッチング",
    "section": "因果推論",
    "text": "因果推論\nThe Three Layer Causal Hierarchy (Pearl 2009: p. 29)\n\nLevel2: Intervention\n\\(P(y|do(x), z)\\)\nTypical Activity: Doing\nTypical Question:\n\nWhat if?\nWhat if I do \\(X\\) ?\n\nExamples:\n\nWhat if I take asprin, will my headache be cured?\nWhat if we ban cigarettes?"
  },
  {
    "objectID": "material/matching.html#因果推論-2",
    "href": "material/matching.html#因果推論-2",
    "title": "マッチング",
    "section": "因果推論",
    "text": "因果推論\nThe Three Layer Causal Hierarchy (Pearl 2009: p. 29)\n\nLevel3: Counterfactual\n\\(P(y_x|x^{\\prime}, y^{\\prime})\\)\nTypical Activity: Imagining and Retrospection\nTypical Question:\n\nWhy?\nWhat is \\(X\\) that caused \\(Y\\) ?\nWhat if I had acted differently?\n\nExamples:\n\nWas it the asprin that stopped my headache?\nWould Kennedy be alive had Oswald not shot him?\nWhat if I had not been smoking the past 2 years?"
  },
  {
    "objectID": "material/matching.html#観察データを用いた因果推論",
    "href": "material/matching.html#観察データを用いた因果推論",
    "title": "マッチング",
    "section": "観察データを用いた因果推論",
    "text": "観察データを用いた因果推論\nもし、 \\(X\\) をしたら.font70[(did)] \\(Y\\) はどうなった.font70[(would)]だろうか\n\n過去を対象にRCTを行うことは不可能\n過去に収集された観察データが使用することに\nマッチング、回帰不連続デザイン、差分の差分法、操作変数法など\n\n\n割当メカニズム (assignment mechanism)\n\nユニットが処置を受けるか否かを規定するメカニズム\n例) 「やる気」が「履修」を規定\n無作為割当なら無作為に処置を受けるか否かが決まるため、考える必要がない。"
  },
  {
    "objectID": "material/matching.html#内生性への対処",
    "href": "material/matching.html#内生性への対処",
    "title": "マッチング",
    "section": "内生性への対処",
    "text": "内生性への対処\nmatching_data1.csvの例 (架空データ)\n\n明らかに「やる気」と「履修」は連関\n履修有無による平均年収の差は約265.333万円\n\n.font90[]"
  },
  {
    "objectID": "material/matching.html#内生性への対処-1",
    "href": "material/matching.html#内生性への対処-1",
    "title": "マッチング",
    "section": "内生性への対処",
    "text": "内生性への対処\n方法: 処置変数と結果変数に影響を与える要因(交絡要因)を揃える\n\n「やる気」のない学生（Yaruki == 0）だけに絞ってみる\n履修有無による平均年収の差は209万円\n\n.left-column[] .right-column[ .font80[]]"
  },
  {
    "objectID": "material/matching.html#内生性への対処-2",
    "href": "material/matching.html#内生性への対処-2",
    "title": "マッチング",
    "section": "内生性への対処",
    "text": "内生性への対処\n方法: 処置変数と結果変数に影響を与える要因(交絡要因)を揃える\n\n「やる気」のある学生（Yaruki == 1）だけに絞ってみる\n履修有無による平均年収の差は176.3万円\n\n.left-column[] .right-column[ .font80[]]"
  },
  {
    "objectID": "material/matching.html#内生性への対処-3",
    "href": "material/matching.html#内生性への対処-3",
    "title": "マッチング",
    "section": "内生性への対処",
    "text": "内生性への対処\n.pull-left[ .center[ やる気のない学生 ( \\(Z_i = 0\\) )]] .pull-right[ .center[ やる気のある学生 ( \\(Z_i = 1\\) )]]\n\n講義履修の因果効果は約191.6万円\n.center[]"
  },
  {
    "objectID": "material/matching.html#マッチングの考え方-1",
    "href": "material/matching.html#マッチングの考え方-1",
    "title": "マッチング",
    "section": "マッチングの考え方",
    "text": "マッチングの考え方\n割当メカニズムを想定し、交絡要因が同じユニット同士を比較\n\n交絡要因: 処置変数と結果変数、両方と関係のある変数\n以下の条件が満たされる場合、マッチングで因果効果の推定が可能\n条件付き独立の仮定 (Conditional Independece Assumption; CIA)\n\n\\(\\{Y_i(T_i = 1),Y_i(T_i = 0)\\} \\perp T_i∣X_i\\)\n\\(T_i\\) : 学生 \\(i\\) の履修有無、 \\(X_i\\) : 学生 \\(i\\) のやる気\nやる気(=交絡要因)が同じ場合、学生 \\(i\\) がソンさんの講義を履修するか否か(=処置変数)は彼(女)の将来収入(=結果変数)と関係なく決まる\n\\(\\rightarrow\\) 処置変数を外生変数として扱うことが可能に\n\nCIAが満たされるためには、割当メカニズム上のすべての交絡要因が必要"
  },
  {
    "objectID": "material/matching.html#条件付き独立の仮定とは",
    "href": "material/matching.html#条件付き独立の仮定とは",
    "title": "マッチング",
    "section": "条件付き独立の仮定とは",
    "text": "条件付き独立の仮定とは\n.left-column[ .font70[]] .right-column[]"
  },
  {
    "objectID": "material/matching.html#条件付き独立の仮定とは-1",
    "href": "material/matching.html#条件付き独立の仮定とは-1",
    "title": "マッチング",
    "section": "条件付き独立の仮定とは",
    "text": "条件付き独立の仮定とは\n.left-column[ .font70[]] .right-column[]"
  },
  {
    "objectID": "material/matching.html#条件付き独立の仮定とは-2",
    "href": "material/matching.html#条件付き独立の仮定とは-2",
    "title": "マッチング",
    "section": "条件付き独立の仮定とは",
    "text": "条件付き独立の仮定とは\n.left-column[ .font70[]] .right-column[]"
  },
  {
    "objectID": "material/matching.html#条件付き独立の仮定とは-3",
    "href": "material/matching.html#条件付き独立の仮定とは-3",
    "title": "マッチング",
    "section": "条件付き独立の仮定とは",
    "text": "条件付き独立の仮定とは\n.left-column[ .font70[]] .right-column[ 条件付き独立が成立するということは]"
  },
  {
    "objectID": "material/matching.html#重回帰分析との比較",
    "href": "material/matching.html#重回帰分析との比較",
    "title": "マッチング",
    "section": "重回帰分析との比較",
    "text": "重回帰分析との比較\n.pull-left[ 重回帰分析における回帰係数の解釈] .pull-right[ .font80[]]"
  },
  {
    "objectID": "material/matching.html#重回帰分析との比較-1",
    "href": "material/matching.html#重回帰分析との比較-1",
    "title": "マッチング",
    "section": "重回帰分析との比較",
    "text": "重回帰分析との比較\n実質的にマッチングと回帰分析は同じという見解も (Angrist and Pischke 2009)\n\n具体的に言えば、回帰分析はマッチングの特殊な形態\n\n強い仮定を置いたマッチング\n回帰分析は \\(Y = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\\) の関数型を仮定 (parametric)\n\n回帰分析において誤差項の平均値は必ず0を仮定 ( \\(\\mathbb{E}(\\varepsilon|T, X) = 0\\) )\n\nマッチングの場合、( \\(\\mathbb{E}(\\varepsilon|T = 0, X) = \\mathbb{E}(\\varepsilon|T = 1, X)\\) )\n\n回帰分析はオーバーラップ条件を無視する\n\nマッチングされないケースでも、線形関数によって予測されてしまう\nマッチングはオーバーラップされないケースを分析から除外する\n\n結論: 回帰分析より柔軟、拡張性がある"
  },
  {
    "objectID": "material/matching.html#ate-att-atc",
    "href": "material/matching.html#ate-att-atc",
    "title": "マッチング",
    "section": "ATE, ATT, ATC",
    "text": "ATE, ATT, ATC\n3種類の因果効果\n\nATE (Average Treatment Effect): 平均処置効果\nATT (ATE for the Treated): 処置群における平均処置効果\n\n潜在結果: 処置群が処置を受けなかった場合の応答変数\n\nATC (ATE for the Control): 統制群における平均処置効果\n\n潜在結果: 統制群が処置を受けた場合の応答変数\n\n\n\n\n因果効果は一般的に母集団ではなく、サンプルから推定されるため、「SATE/SATT/SATC」と呼ばれる場合も\n他にもRIE (Retrospective Intervention Effect) なども (Samii et al. 2016)\nRCTでは主にATEが推定対象（マッチングでは区分するケースが多い）\n統計ソフトウェアによってはATTを因果効果の推定値として表示する場合もある。"
  },
  {
    "objectID": "material/matching.html#ate-att-atc-1",
    "href": "material/matching.html#ate-att-atc-1",
    "title": "マッチング",
    "section": "ATE, ATT, ATC",
    "text": "ATE, ATT, ATC\nATT: 処置群における平均処置効果 * 処置群の潜在的結果を統制群から割り当てる。 * 処置群は \\(Y_i(T_i = 1)\\) が観察済みであり、潜在的結果は \\(Y_i(T_i = 0)\\) * やる気のない学生の \\(Y_i(T_i = 0)\\) は193.5、ある学生は394.2\n.font80[]"
  },
  {
    "objectID": "material/matching.html#ate-att-atc-2",
    "href": "material/matching.html#ate-att-atc-2",
    "title": "マッチング",
    "section": "ATE, ATT, ATC",
    "text": "ATE, ATT, ATC\nATC: 統制群における平均処置効果 * 統制群の潜在的結果を処置群から割り当てる。 * 統制群は \\(Y_i(T_i = 0)\\) が観察済みであり、潜在的結果は \\(Y_i(T_i = 1)\\) * やる気のない学生の \\(Y_i(T_i = 1)\\) は402.5、ある学生は570.5\n.font80[]"
  },
  {
    "objectID": "material/matching.html#ate-att-atc-3",
    "href": "material/matching.html#ate-att-atc-3",
    "title": "マッチング",
    "section": "ATE, ATT, ATC",
    "text": "ATE, ATT, ATC\nATE: 平均処置効果\n\nATTとATCの加重平均\n今回は処置群と統制群が15:15 \\(\\rightarrow\\) 単純平均でOK\n\n\\(\\frac{1}{2}(185.1 + 198.1) = 191.6\\)\n手計算マッチングとと同じ結果\n\n\n\\[\\text{ATE} = \\frac{N_{\\text{treated}}}{N_{\\text{all}}} \\text{ATT} + \\frac{N_{\\text{controlled}}}{N_{\\text{all}}} \\text{ATC}.\\]"
  },
  {
    "objectID": "material/matching.html#マッチングいろいろ",
    "href": "material/matching.html#マッチングいろいろ",
    "title": "マッチング",
    "section": "マッチングいろいろ",
    "text": "マッチングいろいろ\n\nExact Matching\nNearest-neighbor Matching\n\nk-nearest Neighbor Matching\nCaliper Matching (Radius Matching)\n\nCoarsened Exact Matching\nPropensity Score Matching\n\nInverse Probability Weighting\nEnsemble Matching"
  },
  {
    "objectID": "material/matching.html#exact-matching",
    "href": "material/matching.html#exact-matching",
    "title": "マッチング",
    "section": "Exact Matching",
    "text": "Exact Matching\nExact Matching\n\n「正確マッチング」、「厳格なマッチング」などで訳される\nこれまで見てきた方法が Exact Matching\n\nデータ内の共変量 (交絡要因) が完全に一致するケース同士の比較\n\n共変量が少数、かつ、名目or順序変数の場合、使用可\n共変量が多数、または連続変数の場合は実質的に無理\n\n次元の呪い or 次元爆発\n\n\n.center[]"
  },
  {
    "objectID": "material/matching.html#nearest-neighbor-matching",
    "href": "material/matching.html#nearest-neighbor-matching",
    "title": "マッチング",
    "section": "Nearest-neighbor Matching",
    "text": "Nearest-neighbor Matching\nNearest-neighbor Matching\n\n「最近傍マッチング」と訳される。\n共変量が連続変数、多次元の場合、「完全に一致」ケースはない場合がほとんど\n\n\\(\\rightarrow\\) 「一致」ではなく、「最も似ている」ケース同士と比較\n共変量を座標（超）平面に位置づけた場合、最も近いケースをマッチング\n\n\n\n「近さ」の基準\n\nManhattan Distance\nStandardized Euclidean Distance\nMahalanobis Distance (\\(\\leftarrow\\) 最もよく使われる基準)"
  },
  {
    "objectID": "material/matching.html#nearest-neighbor-matching-1",
    "href": "material/matching.html#nearest-neighbor-matching-1",
    "title": "マッチング",
    "section": "Nearest-neighbor Matching",
    "text": "Nearest-neighbor Matching\nManhattan Distance (City-block Distance)\n\\[d(i, j) = |X_i - X_j| + |Y_i - Y_j| \\text{ where } i \\neq j.\\]\n.pull-left[ .center[]] .pull-right[]"
  },
  {
    "objectID": "material/matching.html#nearest-neighbor-matching-2",
    "href": "material/matching.html#nearest-neighbor-matching-2",
    "title": "マッチング",
    "section": "Nearest-neighbor Matching",
    "text": "Nearest-neighbor Matching\nStandardized Euclidean Distance\n\\[d(i, j) = \\sqrt{\\Bigg(\\frac{X_i - X_j}{\\sigma_X}\\Bigg)^2 + \\Bigg(\\frac{Y_i - Y_j}{\\sigma_Y}\\Bigg)^2} \\text{ where } i \\neq j.\\]\n.pull-left[ .center[]] .pull-right[]"
  },
  {
    "objectID": "material/matching.html#nearest-neighbor-matching-3",
    "href": "material/matching.html#nearest-neighbor-matching-3",
    "title": "マッチング",
    "section": "Nearest-neighbor Matching",
    "text": "Nearest-neighbor Matching\nMahalanobis Distance\n\n共変量間の相関が0の場合、Standardized Euclidean Distanceと同じ\n\n.font70[]\n.pull-left[ .center[]] .pull-right[]"
  },
  {
    "objectID": "material/matching.html#マッチング方法",
    "href": "material/matching.html#マッチング方法",
    "title": "マッチング",
    "section": "マッチング方法",
    "text": "マッチング方法\nATTの場合、処置群のケースに統制群の中で最も近いケース1個を割当\n\n近さの測定はマハラノビス距離が一般的\n処置群内の1ケースに複数の統制群ケースを割り当てる場合も\n\nk-nearest Neighbor Matching\nCaliper Matching"
  },
  {
    "objectID": "material/matching.html#k-nearest-neighbor-matching",
    "href": "material/matching.html#k-nearest-neighbor-matching",
    "title": "マッチング",
    "section": "k-nearest Neighbor Matching",
    "text": "k-nearest Neighbor Matching\nk-最近傍マッチング\n\n最も近い1個ケースを潜在的結果として使うのではなく、最も近い \\(k\\) 個のケースの平均値を潜在的結果として用いる。\n\n\\(j(m)\\) : \\(i\\) から \\(m\\) 番目に近いケース \\(j\\)\n\n\n\\[Y_i(T_i = 0) = \\begin{cases}Y_i & \\text{ if } T_i = 0\\\\ \\frac{1}{K} \\sum_{m = 1}^K Y_{j(m)} & \\text{ if } T_i = 1\\end{cases}\\]\n\n最適 \\(k\\) を決める理論的基準は無し\n\n\\(k\\) を大きくすると、モデルの分散が小さくなる\nただし、モデルの分散が小さい = バイアスが拡大\n\nBias–variance trade-off\n\n\\(k\\) を変化させることによって結果がどのように変わるか観察"
  },
  {
    "objectID": "material/matching.html#caliper-matching",
    "href": "material/matching.html#caliper-matching",
    "title": "マッチング",
    "section": "Caliper Matching",
    "text": "Caliper Matching\n「カリパーマッチング」と訳される.font70[(訳されてない…?)]\n\n半径 \\(h\\) の中にある全てのケースの平均値を潜在的結果として使用\n\n\\[Y_i(T_i = 0) = \\begin{cases}Y_i & \\text{ if } T_i = 0\\\\ \\frac{\\sum_{j=1}^N I(T_j = 0, d(i, j) < h)\\cdot Y_i}{\\sum_{j=1}^N I(T_j = 0, d(i, j) < h)} & \\text{ if } T_i = 1\\end{cases}\\]\n.pull-left[ .center[]] .pull-right[]"
  },
  {
    "objectID": "material/matching.html#coarsened-exact-matching",
    "href": "material/matching.html#coarsened-exact-matching",
    "title": "マッチング",
    "section": "Coarsened Exact Matching",
    "text": "Coarsened Exact Matching\nCEMはマハラノビス最近傍マッチング同様、matchit()関数を使います。ただし、cemパッケージをインストールしておく必要があります。まず、Exact Matching類の手法は距離を図る必要がないので、distance引数は不要です。method = ...引数を\"nearest\" (最近傍)から\"cem\"に替えるだけです。推定可能な処置効果はATE、ATT、ATCですが、ここではATTを推定してみましょう。\nマッチングをしたらmatch.data()でマッチングされたデータを抽出します。\n\nCEM_Matching <- matchit(treat ~ age + educ + black + hispanic + married + \n                            nodegree + re74 + re75, data = la_df,\n                       method = \"cem\", estimand = \"ATT\")\n\nsummary(CEM_Matching)\n\n\nCall:\nmatchit(formula = treat ~ age + educ + black + hispanic + married + \n    nodegree + re74 + re75, data = la_df, method = \"cem\", estimand = \"ATT\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\nage            25.8162       28.0303         -0.3094     0.4400    0.0813\neduc           10.3459       10.2354          0.0550     0.4959    0.0347\nblack           0.8432        0.2028          1.7615          .    0.6404\nhispanic        0.0595        0.1422         -0.3498          .    0.0827\nmarried         0.1892        0.5128         -0.8263          .    0.3236\nnodegree        0.7081        0.5967          0.2450          .    0.1114\nre74         2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75         1532.0553     2466.4844         -0.2903     0.9563    0.1342\n         eCDF Max\nage        0.1577\neduc       0.1114\nblack      0.6404\nhispanic   0.0827\nmarried    0.3236\nnodegree   0.1114\nre74       0.4470\nre75       0.2876\n\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\nage            21.6462       21.2936          0.0493     0.8909    0.0130\neduc           10.3692       10.2795          0.0446     0.8894    0.0100\nblack           0.8615        0.8615          0.0000          .    0.0000\nhispanic        0.0308        0.0308          0.0000          .    0.0000\nmarried         0.0308        0.0308          0.0000          .    0.0000\nnodegree        0.6769        0.6769          0.0000          .    0.0000\nre74          489.0612      697.6115         -0.0427     1.3916    0.0435\nre75          362.4365      520.8378         -0.0492     0.8501    0.0403\n         eCDF Max Std. Pair Dist.\nage        0.0897          0.1373\neduc       0.1000          0.1949\nblack      0.0000          0.0000\nhispanic   0.0000          0.0000\nmarried    0.0000          0.0000\nnodegree   0.0000          0.0000\nre74       0.3118          0.0968\nre75       0.1598          0.1635\n\nSample Sizes:\n              Control Treated\nAll            429.       185\nMatched (ESS)   41.29      65\nMatched         75.        65\nUnmatched      354.       120\nDiscarded        0.         0\n\n\nCEMの場合、マッチングされないブロックは捨てられるため、マハラノビス距離最近傍マッチングよりもサンプルサイズが小さくなります。また、処置群と統制群のサンプルサイズも不均衡になります。マッチング結果を見ると、処置群からは65ケース、統制群からは75サンプルのみ残ることになりました。\nつづいて、cobalt::love.plot()を使用して、バランスチェックを行います。\n\nlove.plot(CEM_Matching, thresholds = 0.25, abs = TRUE)\n\n\n\n\n\n\n\n\nCEMの場合、マハラノビス距離最近傍マッチングよりもバランスが大きく改善されたことが分かります。その理由は簡単です。最近傍マッチングの場合、最も近いケースであれば、どれほど離れていてもマッチングされます。一方、CEMは正確マッチングの一種であるため、ある程度離れているケースを捨ててしまうからです。結局は共変量が非常に近いケースのみを残すことになります。\nそれでは、match.data()関数を使ってマッチング後のデータを抽出してみましょう。\n\nCEM_Data <- match.data(CEM_Matching)\n\n処置効果の推定の前に、抽出されたデータの中身を見てましょう。元のデータにはweightsという列はありませんでしたが、いきなりできました。実はこれが、CEMに使う重み変数です。自動的に計算してデータセットに追加してくれたわけです6。\n\nCEM_Data\n\n\n\n\n\n\n\n\n処置効果の推定は重み付き回帰分析 (WLS)で行います。既存のlm()関数にweight = ...引数を追加し、データ内の重み列をしてするだけです。\n\n# 処置効果の確認\n# weight = Wが追加されていることを確認\nCEM_Fit <- lm(re78 ~ treat, data = CEM_Data, weights = weights)\n\n# 処置効果の確認\nmodelsummary(CEM_Fit)\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    5265.785 \n  \n  \n     \n    (850.457) \n  \n  \n    treat \n    1070.907 \n  \n  \n     \n    (1248.129) \n  \n  \n    Num.Obs. \n    140 \n  \n  \n    R2 \n    0.005 \n  \n  \n    R2 Adj. \n    −0.002 \n  \n  \n    AIC \n    2924.7 \n  \n  \n    BIC \n    2933.5 \n  \n  \n    F \n    0.736 \n  \n  \n    RMSE \n    7169.24 \n  \n\n\n\n\n\n今回、処置効果の推定値は約1070.907ドルです。"
  },
  {
    "objectID": "material/matching.html#cemの例-matching_data2.csv",
    "href": "material/matching.html#cemの例-matching_data2.csv",
    "title": "マッチング",
    "section": "CEMの例 (matching_data2.csv)",
    "text": "CEMの例 (matching_data2.csv)\n年齢は10歳刻み、学歴は大卒以上・未満に層化\n\n.font80[]"
  },
  {
    "objectID": "material/matching.html#cemの例-matching_data2.csv-1",
    "href": "material/matching.html#cemの例-matching_data2.csv-1",
    "title": "マッチング",
    "section": "CEMの例 (matching_data2.csv)",
    "text": "CEMの例 (matching_data2.csv)\n年齢は10歳刻み、学歴は大卒以上・未満に層化 * カテゴリが少なくなり、マッチングされやすくなる\n.font80[]"
  },
  {
    "objectID": "material/matching.html#cemの例-matching_data2.csv-2",
    "href": "material/matching.html#cemの例-matching_data2.csv-2",
    "title": "マッチング",
    "section": "CEMの例 (matching_data2.csv)",
    "text": "CEMの例 (matching_data2.csv)\n層ごとにケースをマッチング (ペアが組めない層もあり)\n.font80[]"
  },
  {
    "objectID": "material/matching.html#cemの例-matching_data2.csv-3",
    "href": "material/matching.html#cemの例-matching_data2.csv-3",
    "title": "マッチング",
    "section": "CEMの例 (matching_data2.csv)",
    "text": "CEMの例 (matching_data2.csv)\nペアが組めない層を除外\n.font80[]"
  },
  {
    "objectID": "material/matching.html#cemの例-matching_data2.csv-4",
    "href": "material/matching.html#cemの例-matching_data2.csv-4",
    "title": "マッチング",
    "section": "CEMの例 (matching_data2.csv)",
    "text": "CEMの例 (matching_data2.csv)\n各ユニットの重みを計算 * \\(m_{C,T}\\) : 統制・処置ケースの数、 \\(m^s_{C,T}\\) : 層 \\(s\\) 内の統制・処置ケースの数\n\\[w_i = \\begin{cases} 1 & \\text{ if } T_i = 1, \\\\ \\frac{m_C}{m_T} \\cdot \\frac{m^s_T}{m^s_C} & \\text{ if } T_i = 0.\\end{cases}\\]\n.font70[]"
  },
  {
    "objectID": "material/matching.html#cemの例-matching_data2matching_data2.csv",
    "href": "material/matching.html#cemの例-matching_data2matching_data2.csv",
    "title": "マッチング",
    "section": "CEMの例 (matching_data2matching_data2.csv)",
    "text": "CEMの例 (matching_data2matching_data2.csv)\n重み付け回帰分析\n\n\\(W = \\{2.200, 0.367, 1.000, 1.100, 0.000, 0.000, ..., 1.100\\}^{\\top}\\)\n\nマッチングされないケースの重みは0にするか、分析から除外\n\\(\\beta = (X^{\\top}WX)^{-1}X^{\\top}WY\\)\n\nRの場合、lm(formula, data, weight = ...)で推定可能\n\n{cem} or {MatchIt}パッケージならもっと簡単\n\n\\(\\widehat{\\text{Outcome}} = 4.567 + 2.033 \\cdot \\text{Treat}\\)\n\n処置群における因果効果 (ATT) = 2.033"
  },
  {
    "objectID": "material/matching.html#傾向スコアとは",
    "href": "material/matching.html#傾向スコアとは",
    "title": "マッチング",
    "section": "傾向スコアとは",
    "text": "傾向スコアとは\nPropensity Score\n\n簡単にいうと「あるユニット \\(i\\) が処置を受ける確率」\n\n\\(e_i = Pr(T_i = 1 | X_i)\\)\n\n\n\nなぜ傾向スコア?\n\nマッチングの限界\n\n次元の問題 (dimension problem)\n恣意性\nカテゴリ変数の扱い方\nスケールの問題"
  },
  {
    "objectID": "material/matching.html#傾向スコアの計算",
    "href": "material/matching.html#傾向スコアの計算",
    "title": "マッチング",
    "section": "傾向スコアの計算",
    "text": "傾向スコアの計算\n処置変数 ( \\(T_i\\) ) を応答変数とし、共変量 ( \\(X_i\\) ) を説明変数とする\n\n一般的に、ロジットやプロビット回帰分析で推定する。\n\n他にも色々ある.font80[(Support Vector Machine, Decision Tree, Neural Network, …)]\n色んな手法で算出した傾向スコアを重み付けして合成することも可能\n\nEnsemble Method (Samii, Paler, and Zukerman 2016)\n\n\n\n\n\n推定された予測確率 \\(\\rightarrow\\) 傾向スコア\n\nRではオブジェクト名$fitted.valueで抽出可\n傾向スコアは多くの共変量を.kenten2[一つ]の変数に集約したもの\n\\(\\rightarrow\\) 傾向スコアを統制した回帰分析で因果効果を推定\n\\(\\rightarrow\\) 傾向スコアを用いて最近傍マッチング"
  },
  {
    "objectID": "material/matching.html#傾向スコアマッチングの手順",
    "href": "material/matching.html#傾向スコアマッチングの手順",
    "title": "マッチング",
    "section": "傾向スコア・マッチングの手順",
    "text": "傾向スコア・マッチングの手順\n割り当てメカニズムを仮定\n\\[Pr(\\text{処置}) \\propto \\beta_0 + \\beta_1 \\cdot \\text{年齢} + \\beta_2 \\cdot \\text{教育}\\]\n.font80[]"
  },
  {
    "objectID": "material/matching.html#傾向スコアの算出",
    "href": "material/matching.html#傾向スコアの算出",
    "title": "マッチング",
    "section": "傾向スコアの算出",
    "text": "傾向スコアの算出\n傾向スコアの算出\n\nPS_Fit <- glm(処置 ~ 年齢 + 学歴, data = データ, family = binomial(\"logit\"))\nsummary(PS_Fit)\n\n\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    3.839 \n  \n  \n     \n    (2.364) \n  \n  \n    Age \n    −0.059 \n  \n  \n     \n    (0.039) \n  \n  \n    Edu \n    −0.420 \n  \n  \n     \n    (0.304) \n  \n  \n    Num.Obs. \n    24 \n  \n  \n    AIC \n    35.5 \n  \n  \n    BIC \n    39.0 \n  \n  \n    Log.Lik. \n    −14.748 \n  \n  \n    F \n    1.486 \n  \n  \n    RMSE \n    0.46"
  },
  {
    "objectID": "material/matching.html#傾向スコアの算出-1",
    "href": "material/matching.html#傾向スコアの算出-1",
    "title": "マッチング",
    "section": "傾向スコアの算出",
    "text": "傾向スコアの算出\n傾向スコアの抽出\n\nデータ$PS <- PS_Fit$fitted.value\n\n.font80[]"
  },
  {
    "objectID": "material/matching.html#傾向スコアの算出-2",
    "href": "material/matching.html#傾向スコアの算出-2",
    "title": "マッチング",
    "section": "傾向スコアの算出",
    "text": "傾向スコアの算出\nATT: 傾向スコアが最も近い統制群を割り当てる\n\n一回マッチングされたケースを除外する vs. しない\n傾向スコアが同じケースが複数ある場合の対処\n\n.font80[]"
  },
  {
    "objectID": "material/matching.html#傾向スコアの算出-3",
    "href": "material/matching.html#傾向スコアの算出-3",
    "title": "マッチング",
    "section": "傾向スコアの算出",
    "text": "傾向スコアの算出\nATC: 傾向スコアが最も近い処置群を割り当てる\n.font80[]"
  },
  {
    "objectID": "material/matching.html#傾向スコアの算出-4",
    "href": "material/matching.html#傾向スコアの算出-4",
    "title": "マッチング",
    "section": "傾向スコアの算出",
    "text": "傾向スコアの算出\nATE: ATTとATCの.kenten[加重]平均\n\\[\\begin{align}\\text{ATE} & = \\frac{N_\\text{Treat}}{N_\\text{All}}\\text{ATT} + \\frac{N_\\text{Control}}{N_\\text{All}}\\text{ATC} \\\\ & = \\frac{11}{24} 1.818 + \\frac{13}{24} 3.923 = 2.958\\end{align}\\]\n\nweighted.mean(c(1.818, 3.923), c(11, 13))\n\n[1] 2.958208"
  },
  {
    "objectID": "material/matching.html#処置を受ける確率の計算",
    "href": "material/matching.html#処置を受ける確率の計算",
    "title": "マッチング",
    "section": "処置を受ける確率の計算",
    "text": "処置を受ける確率の計算\n処置を受ける確率 = 傾向スコア\n\n一般的にはロジスティック/プロビット回帰分析が使われる\nただし、確率が予測できるなら他の手法でも良い\n\nCovariate Balancing Propensity Score\nEntropy Balancing\nNeural Network\nSupport Vector Machine\nRandom Forestなど\n\n\n\n\n複数の手法を組み合わせる(= ensemble)することも可能\n\n\\(\\rightarrow\\) Super Learner Algorithm"
  },
  {
    "objectID": "material/matching.html#傾向スコアのもう一つの使い方",
    "href": "material/matching.html#傾向スコアのもう一つの使い方",
    "title": "マッチング",
    "section": "傾向スコアのもう一つの使い方",
    "text": "傾向スコアのもう一つの使い方\nIPW: Inverse Probability Weighting (Rubin 1985)\n\n「逆確率重み付け」と訳される\n実際に処置を受けた( \\(T_i = 1\\) )にもかかわらず、処置を受ける傾向が 小さい場合は分析において大きい重み\n\n傾向スコアを重み変数として用いる。\n\\(e_i\\) が1または0に近い場合、一部のケースに非常に大きい重みを付け ることになるため、注意が必要\n\n\n\\[w_i = \\begin{cases}\\frac{1}{e_i} & \\text{ if } T_i = 1, \\\\ \\frac{1}{1 - e_i} & \\text{ if } T_i = 0.\\end{cases}\\]\n\n\\(e_i\\) : \\(i\\) の傾向スコア; \\(T_i\\) : \\(i\\) の処置有無 ( \\(\\in \\{0, 1\\}\\) )"
  },
  {
    "objectID": "material/matching.html#傾向スコアのもう一つの使い方-1",
    "href": "material/matching.html#傾向スコアのもう一つの使い方-1",
    "title": "マッチング",
    "section": "傾向スコアのもう一つの使い方",
    "text": "傾向スコアのもう一つの使い方\nIPW: Inverse Probability Weighting (Rubin 1985)\n\n「逆確率重み付け」と訳される\n実際に処置を受けた( \\(T_i = 1\\) )にもかかわらず、処置を受ける傾向が 小さい場合は分析において大きい重み\n\n傾向スコアを重み変数として用いる。\n\\(e_i\\) が1または0に近い場合、一部のケースに非常に大きい重みを付け ることになるため、注意が必要\n\n\n\\[w_i = T_i \\frac{1}{e_i} + (1 - T_i) \\frac{1}{1 - e_i}.\\]\n\n\\(e_i\\) : \\(i\\) の傾向スコア; \\(T_i\\) : \\(i\\) の処置有無 ( \\(\\in \\{0, 1\\}\\) )"
  },
  {
    "objectID": "material/matching.html#ipw-の考え方",
    "href": "material/matching.html#ipw-の考え方",
    "title": "マッチング",
    "section": "IPW の考え方",
    "text": "IPW の考え方\n.left-column[ .font70[]] .right-column[ 条件付き独立の例のデータ * \\(Pr(Z = 0) = 0.4\\) 、 \\(Pr(Z = 1) = 0.6\\) * \\(Z = 0\\) の場合 * \\(Pr(T = 0) = 0.5\\) 、 \\(Pr(T = 1) = 0.5\\) * \\(Z = 1\\) の場合 * \\(Pr(T = 0) = 0.25\\) 、 \\(Pr(T = 1) = 0.75\\) * 観察されたデータからは約0.1の処置効果が推定されるが、本当の処置効果は0]"
  },
  {
    "objectID": "material/matching.html#ipwの考え方",
    "href": "material/matching.html#ipwの考え方",
    "title": "マッチング",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "material/matching.html#ipwの考え方-1",
    "href": "material/matching.html#ipwの考え方-1",
    "title": "マッチング",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "material/matching.html#ipwの考え方-2",
    "href": "material/matching.html#ipwの考え方-2",
    "title": "マッチング",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "material/matching.html#ipwの考え方-3",
    "href": "material/matching.html#ipwの考え方-3",
    "title": "マッチング",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "material/matching.html#ipwの考え方-4",
    "href": "material/matching.html#ipwの考え方-4",
    "title": "マッチング",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "material/matching.html#ipwの考え方-5",
    "href": "material/matching.html#ipwの考え方-5",
    "title": "マッチング",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "material/matching.html#他の考え方",
    "href": "material/matching.html#他の考え方",
    "title": "マッチング",
    "section": "他の考え方",
    "text": "他の考え方\n.left-column[ .font70[]] .right-column[ もし、全ケースが統制群なら?]"
  },
  {
    "objectID": "material/matching.html#他の考え方-1",
    "href": "material/matching.html#他の考え方-1",
    "title": "マッチング",
    "section": "他の考え方",
    "text": "他の考え方\n.left-column[ .font70[]] .right-column[ もし、全ケースが処置群なら?]"
  },
  {
    "objectID": "material/matching.html#他の考え方-2",
    "href": "material/matching.html#他の考え方-2",
    "title": "マッチング",
    "section": "他の考え方",
    "text": "他の考え方\n.left-column[ .font70[]] .right-column[ * 統制群におけるWの和: 20 * 処置群におけるWの和: 20 * \\(\\rightarrow\\) 各群におけるWの和はサンプルサイズと一致する * \\(\\rightarrow\\) 全サンプルが統制/処置群の場合の 結果変数の期待値を計算 (加重平均)]"
  },
  {
    "objectID": "material/matching.html#他の考え方-3",
    "href": "material/matching.html#他の考え方-3",
    "title": "マッチング",
    "section": "他の考え方",
    "text": "他の考え方\n.left-column[ .font70[]] .right-column[ * 統制群の加重平均 * \\(0 \\cdot 2 + 1 \\cdot 2 + 0 \\cdot 2 + 0 \\cdot 2 + \\dots + 0 \\cdot 4\\) * \\(\\mathbb{E}^w[Y_0] = 10\\) * 処置群の加重平均 * \\(0 \\cdot 2 + 0 \\cdot 2 + 0 \\cdot 2 + 1 \\cdot 2 + \\dots + 0 \\cdot 1.33\\) * \\(\\mathbb{E}^w[Y_1] = 10\\)]"
  },
  {
    "objectID": "material/matching.html#共変量の選択-1",
    "href": "material/matching.html#共変量の選択-1",
    "title": "マッチング",
    "section": "共変量の選択",
    "text": "共変量の選択\n共変量選択の基準は (星野 2009; Imbens and Rubin 2015など) 1. 処置変数と結果変数、.kenten[両方と連関]があること * OVBと関係 2. 処置.kenten[前]変数と処置.kenten[後]変数の区別 * 処置変数に時間的に先行しているか否か 3. 処置前変数 (pre treatment variable) は必ず投入する 4. 処置後変数 (post treatment variable) は目的による * というものの、基本的に投入しない * 応答変数よりも時間的に後なら絶対に投入しない"
  },
  {
    "objectID": "material/matching.html#共変量の選択-2",
    "href": "material/matching.html#共変量の選択-2",
    "title": "マッチング",
    "section": "共変量の選択",
    "text": "共変量の選択\nVanderWeele (2019) のmodified disjunctive cause criterion\n\n処置変数と応答変数.kenten[どちらかの原因]となる変数\n処置変数と応答変数.kenten[両方の原因]となる変数\n.kenten[操作変数]は共変量として投入しない\n上記の基準を満たさない場合でも、観察されていない共変量の.kenten[代理変数]は 統制しても良い\n\nしかし、慎重に選択しないとバイアスが拡大\n2.の該当する変数の代理変数が望ましい\n\n\n\n.font80[詳細は https://www.slideshare.net/tintstyle/ss-141543274 を参照]"
  },
  {
    "objectID": "material/matching.html#ダイアグラムを使った例",
    "href": "material/matching.html#ダイアグラムを使った例",
    "title": "マッチング",
    "section": "ダイアグラムを使った例",
    "text": "ダイアグラムを使った例\n.pull-left[] .pull-right[ * T \\(\\rightarrow\\) Yの効果は11 * Z: TとYの原因 \\(\\leftarrow\\) 投入 * A: 操作変数 \\(\\leftarrow\\) 除外 * X: Y の原因 \\(\\leftarrow\\) 投入 * V :Tの結果 \\(\\leftarrow\\) 除外 * Wは…?]"
  },
  {
    "objectID": "material/matching.html#ダイアグラムを使った例-1",
    "href": "material/matching.html#ダイアグラムを使った例-1",
    "title": "マッチング",
    "section": "ダイアグラムを使った例",
    "text": "ダイアグラムを使った例\n.pull-left[] .pull-right[ * T \\(\\rightarrow\\) Y: 直接効果 * T \\(\\rightarrow\\) W \\(\\rightarrow\\) Y: 間接効果 * Wは中間変数.font80[(mediate variable)] * 因果推論では主に全効果 (total effect) に関心があるためWは投入しない * 全効果: 直接効果 + 間接効果 * Tが変動したらWも必ず変わるため、Tのみの効果はあまり意味なし * 直接効果のみ推定する場合、Wも統制 * 結論: ZとXのみ統制 * 実はXは入れなくてもOK]"
  },
  {
    "objectID": "material/matching.html#ダイアグラムのツール",
    "href": "material/matching.html#ダイアグラムのツール",
    "title": "マッチング",
    "section": "ダイアグラムのツール",
    "text": "ダイアグラムのツール\nDAGitty — draw and analyze causal diagrams\n\nウェーブページ or Rパッケージ({dagitty})\n\nhttp://www.dagitty.net/\n\n\n.left-column[] .right-column[]"
  },
  {
    "objectID": "material/matching.html#ダイアグラムのツール-1",
    "href": "material/matching.html#ダイアグラムのツール-1",
    "title": "マッチング",
    "section": "ダイアグラムのツール",
    "text": "ダイアグラムのツール\n{ggdag}を用いた可視化: 『私たちのR』第20章も参照\n\nlibrary(ggdag)\ncoordinates(DAG1) <- list(\n  x = c(T = 1, Y = 5, Z = 3, W = 3, A = 2, V = 2, X = 4),\n  y = c(T = 2, Y = 2, Z = 3, W = 4, A = 1, V = 4, X = 1)\n)\nggdag(DAG1, node_size = 12, stylized = TRUE) +\n  theme_dag_blank()"
  },
  {
    "objectID": "material/matching.html#利用データ",
    "href": "material/matching.html#利用データ",
    "title": "マッチング",
    "section": "利用データ",
    "text": "利用データ\n定番のlalondeデータセット: 職業訓練の有無と所得 * data(\"lalonde\", package = \"cobalt\")で読み込み * その前に{cobalt}パッケージを読み込む\n\n\n\n\n変数名\n説明\n\n\n\n\n1\ntreat\n職業訓練の履修有無 (Treatment)\n\n\n2\nage\n年齢\n\n\n3\neduc\n教育年数\n\n\n4\nrace\n人種(白人、黒人、ヒスパニック)\n\n\n5\nmarried\n結婚有無\n\n\n6\nnodegree\n学位有無\n\n\n7\nre74\n1974 年の所得\n\n\n8\nre75\n1975 年の所得\n\n\n9\nre78\n1978 年の所得 (Outcome)"
  },
  {
    "objectID": "material/matching.html#matching-手法",
    "href": "material/matching.html#matching-手法",
    "title": "マッチング",
    "section": "Matching 手法",
    "text": "Matching 手法\nExact Matching は現実的に不可能であるため省略\n\nMahalanobis Matching\nCoarsened Exact Matching\nPropensity Score Matching\nInverse Probability Weighting\n\n\n使用パッケージ * 1〜3: {MatchIt} * 事前に{cem}パッケージをインストールしておく * 4: {WeightIt} * バランスチェック: {cobalt}"
  },
  {
    "objectID": "material/matching.html#バランスチェック",
    "href": "material/matching.html#バランスチェック",
    "title": "マッチング",
    "section": "バランスチェック",
    "text": "バランスチェック\n　ここでは標準化差分以外のバランスチェック方法について紹介する。まず、特定の共変量の分布をヒストグラムを用い、マッチング前後で比較する方法だ。これもまた{coblat}パッケージを使用するが、今回はlove.plot()でなく、bal.plot()を使う。\n　第1引数はmatchit()から得られたオブジェクト名、続いてvar.nameにはバランスをチェックする共変量名（傾向スコアの場合は\"distance\"）、他は以下のコードの通りに打てばよい。\n\nbal.plot(ps_mat, var.name = \"distance\", which = \"both\",\n         type = \"histogram\", mirror = TRUE)\n\n\n\n\n\n\n\n\n　左がマッチング前、右が後である。また、上部の赤いヒストグラムは統制群、下部の青は処置群を意味する。もし、傾向スコアのバランスが取れているならヒストグラムは上下対称となる。マッチング前だと統制群は傾向スコアの値が小さく、処置群のそれは大きい傾向があったが、マッチング後はほぼ上下対称となっていることからバランスが改善されたことが分かる。\n　ちなみに、ヒストグラムが作成できないダミー変数の場合、棒グラフが表示される。読み方は同じであるが、今回は上下対称ではなく、赤い棒と青い棒の高さが一致すればバランスが取れていると確認できる。\n\nbal.plot(ps_mat, var.name = \"black\", which = \"both\",\n         type = \"histogram\", mirror = TRUE)\n\n\n\n\n\n\n\n\n　このようにヒストグラム（棒グラフ）を使うと、一つ一つの変数の図が必要となってくるので、実際の論文には掲載しにくい。それでも分析の段階では一つ一つのバランスを詳細に見ることは重要である。たとえば、傾向スコアマッチングの場合、回答者の年齢（age）のバランスは改善されている。本当にそうだろうか。ageのバランスを確認してみよう。\n\nbal.plot(ps_mat, var.name = \"age\", which = \"both\",\n         type = \"histogram\", mirror = TRUE)\n\n\n\n\n\n\n\n\n　改善ところか、改悪されているとも読み取れる。標準化差分は平均値と標準誤差のみに依存するため、分布の情報までは分からない。このような場合は、処置効果の推定の際、共変量を投入して更に調整が必要であることを示唆する。"
  },
  {
    "objectID": "material/matching.html#qq-plotの出し方",
    "href": "material/matching.html#qq-plotの出し方",
    "title": "マッチング",
    "section": "QQ Plotの出し方",
    "text": "QQ Plotの出し方\nplot(マッチング後のオブジェクト名)で出力\n\n点が45度線上に位置する場合、バランス\n\n.pull-left[] .pull-right[]"
  },
  {
    "objectID": "material/matching.html#qq-plotとは",
    "href": "material/matching.html#qq-plotとは",
    "title": "マッチング",
    "section": "QQ Plotとは",
    "text": "QQ Plotとは\nQuantile–Quantile Plot\n\n(簡単にいうと)2つの変数を小さい値から大きい値の順に並び替え、 散布図を作成し、45度線を引いたもの\nもし2つのデータが完全に同じ分布をしているのなら、全ての点は 45 度線上に位置\n長所\n\n分布の形まで比較可能\n平均値と分散のみに依存するため、サンプルサイズに鈍感\n\n短所\n\n離散変数においては使いにくい"
  },
  {
    "objectID": "material/matching.html#同じ平均値分散相関異なる分布",
    "href": "material/matching.html#同じ平均値分散相関異なる分布",
    "title": "マッチング",
    "section": "同じ平均値・分散・相関、異なる分布",
    "text": "同じ平均値・分散・相関、異なる分布\nAlberto CairoのDatasaurus dataset\n\n以下の2つの散布図のXとYは平均値、分散、相関係数が同じ\n\n記述統計量だけでなく、可視化も必要\n\n\n.pull-left[] .pull-left[]"
  },
  {
    "objectID": "material/matching.html#qq-plotの例-1",
    "href": "material/matching.html#qq-plotの例-1",
    "title": "マッチング",
    "section": "QQ Plotの例 (1)",
    "text": "QQ Plotの例 (1)\nもし2つのデータが完全に同じなら…\n\nX: -5, -4, -2, -1, 0, 1, 2, 3, 4, 5\nY: -5, -4, -2, -1, 0, 1, 2, 3, 4, 5"
  },
  {
    "objectID": "material/matching.html#qq-plotの例-2",
    "href": "material/matching.html#qq-plotの例-2",
    "title": "マッチング",
    "section": "QQ Plotの例 (2)",
    "text": "QQ Plotの例 (2)\nもし2つのデータが同じ分布から生成されたなら… * \\(X \\sim \\text{Normal}(5, 2)\\) * \\(Y \\sim \\text{Normal}(5, 2)\\)"
  },
  {
    "objectID": "material/matching.html#qq-plotの例-3",
    "href": "material/matching.html#qq-plotの例-3",
    "title": "マッチング",
    "section": "QQ Plotの例 (3)",
    "text": "QQ Plotの例 (3)\nもし2つのデータが分散が異なる分布から生成されたなら… * \\(X \\sim \\text{Normal}(5, 1)\\) * \\(Y \\sim \\text{Normal}(5, 2)\\)"
  },
  {
    "objectID": "material/matching.html#qq-plotの例-4",
    "href": "material/matching.html#qq-plotの例-4",
    "title": "マッチング",
    "section": "QQ Plotの例 (4)",
    "text": "QQ Plotの例 (4)\nもし2つのデータが平均が異なる分布から生成されたなら * \\(X \\sim \\text{Normal}(2, 2)\\) * \\(Y \\sim \\text{Normal}(5, 2)\\)"
  },
  {
    "objectID": "material/matching.html#qq-plotの例-5",
    "href": "material/matching.html#qq-plotの例-5",
    "title": "マッチング",
    "section": "QQ Plotの例 (5)",
    "text": "QQ Plotの例 (5)\nもし2つのデータが平均と分散が異なる分布から生成されたなら… * \\(X \\sim \\text{Normal}(2, 2)\\) * \\(Y \\sim \\text{Normal}(5, 5)\\)"
  },
  {
    "objectID": "material/matching.html#jitter-plot",
    "href": "material/matching.html#jitter-plot",
    "title": "マッチング",
    "section": "Jitter Plot",
    "text": "Jitter Plot\n傾向スコアの分布を確認する\n\nplot(マッチング・オブジェクト名, type = \"jitter\")で出力"
  },
  {
    "objectID": "material/matching.html#cobaltの利用-1",
    "href": "material/matching.html#cobaltの利用-1",
    "title": "マッチング",
    "section": "{cobalt}の利用 (1)",
    "text": "{cobalt}の利用 (1)\n傾向スコアのヒストグラム: cobalt::bal.plot() * ヒストグラムが上下対称ならバランス"
  },
  {
    "objectID": "material/matching.html#cobaltの利用-2",
    "href": "material/matching.html#cobaltの利用-2",
    "title": "マッチング",
    "section": "{cobalt}の利用 (2)",
    "text": "{cobalt}の利用 (2)\nラブ・プロット: cobalt::love.plot()\n\n標準化差分によるバランス・チェック"
  },
  {
    "objectID": "material/matching.html#真の因果効果は",
    "href": "material/matching.html#真の因果効果は",
    "title": "マッチング",
    "section": "真の因果効果は?",
    "text": "真の因果効果は?\n手法ごとに異なる処置効果 * .kenten[真の]因果効果が分からないため、どの推定値が正しいかは分からない * 最近傍マッチングはランダム要素があるため、分析の度に変化 * 様々な手法で推定値、または傾向が安定しているかを確認"
  },
  {
    "objectID": "slide/matching.html#因果推論と内生性",
    "href": "slide/matching.html#因果推論と内生性",
    "title": "方法論特殊講義III",
    "section": "因果推論と内生性",
    "text": "因果推論と内生性\n内生性: 処置変数と誤差項間の相関関係\n\n内生性は因果推論の敵\n例\n\n処置変数 = ソンさんの講義を履修するか否か\n結果変数 = 10年後の年収\nもし、やる気のある学生が履修する傾向があるとしたら?\nやる気のある学生は履修の有無と関係なく、高所得者になりやすい。\n\\(\\rightarrow\\) 「やる気」は処置と結果、両方と連関している\n\n\n\n\n内生性を除去する最良の手法 \\(\\rightarrow\\) RCT"
  },
  {
    "objectID": "slide/matching.html#rctの限界",
    "href": "slide/matching.html#rctの限界",
    "title": "方法論特殊講義III",
    "section": "RCTの限界",
    "text": "RCTの限界\n\n高費用\n\n数万〜数億円\n\n倫理的な問題による実行不可能性\n\n喫煙と健康\nPhilip Zimbardo. 2008. The Lucifer Effect: How Good People Turn Evil. Rider.\n\n外的妥当性の問題\n\nMichael G. Findley, Kyosuke Kikuta, and Michael Denly. 2021. “External Validity,” Annual Review of Political Science, 24:365-393.\n\n回顧的因果推論には不向き\n\n主に介入 (intervention)の効果が推定対象"
  },
  {
    "objectID": "slide/matching.html#因果推論",
    "href": "slide/matching.html#因果推論",
    "title": "方法論特殊講義III",
    "section": "因果推論",
    "text": "因果推論\nThe Three Layer Causal Hierarchy (Pearl 2009: p. 29)\n\nLevel1: Association\n\\(P(y|x)\\)\nTypical Activity: Seeing\nTypical Question:\n\nWhat is?\nHow would seeing \\(X\\) change my belief in \\(Y\\) ?\n\nExamples:\n\nWhat does a symptom tell me about a disease?\nWhat does a survey tell us about the election results?"
  },
  {
    "objectID": "slide/matching.html#因果推論-1",
    "href": "slide/matching.html#因果推論-1",
    "title": "方法論特殊講義III",
    "section": "因果推論",
    "text": "因果推論\nThe Three Layer Causal Hierarchy (Pearl 2009: p. 29)\n\nLevel2: Intervention\n\\(P(y|do(x), z)\\)\nTypical Activity: Doing\nTypical Question:\n\nWhat if?\nWhat if I do \\(X\\) ?\n\nExamples:\n\nWhat if I take asprin, will my headache be cured?\nWhat if we ban cigarettes?"
  },
  {
    "objectID": "slide/matching.html#因果推論-2",
    "href": "slide/matching.html#因果推論-2",
    "title": "方法論特殊講義III",
    "section": "因果推論",
    "text": "因果推論\nThe Three Layer Causal Hierarchy (Pearl 2009: p. 29)\n\nLevel3: Counterfactual\n\\(P(y_x|x^{\\prime}, y^{\\prime})\\)\nTypical Activity: Imagining and Retrospection\nTypical Question:\n\nWhy?\nWhat is \\(X\\) that caused \\(Y\\) ?\nWhat if I had acted differently?\n\nExamples:\n\nWas it the asprin that stopped my headache?\nWould Kennedy be alive had Oswald not shot him?\nWhat if I had not been smoking the past 2 years?"
  },
  {
    "objectID": "slide/matching.html#観察データを用いた因果推論",
    "href": "slide/matching.html#観察データを用いた因果推論",
    "title": "方法論特殊講義III",
    "section": "観察データを用いた因果推論",
    "text": "観察データを用いた因果推論\nもし、\\(X\\)をしたら（did）\\(Y\\)はどうなった（would）だろうか\n\n過去を対象にRCTを行うことは不可能\n過去に収集された観察データが使用することに\nマッチング、回帰不連続デザイン、差分の差分法、操作変数法など\n\n\n割当メカニズム (assignment mechanism)\n\nユニットが処置を受けるか否かを規定するメカニズム\n例) 「やる気」が「履修」を規定\n無作為割当なら無作為に処置を受けるか否かが決まるため、考える必要がない。"
  },
  {
    "objectID": "slide/matching.html#内生性への対処",
    "href": "slide/matching.html#内生性への対処",
    "title": "方法論特殊講義III",
    "section": "内生性への対処",
    "text": "内生性への対処\nmatching_data1.csvの例 (架空データ; 30行 \\(\\times\\) 4列)\n\n明らかに「やる気」と「履修」は連関\n履修有無による平均年収の差は約265.333万円\n\n\n\n\n\n# A tibble: 10 × 4\n      ID Income Yaruki Rishu\n   <dbl>  <dbl>  <dbl> <dbl>\n 1     1    659      0     1\n 2     2    587      1     1\n 3     3    628      1     1\n 4     4    563      1     1\n 5     5    531      1     1\n 6     6     79      0     0\n 7     7    356      0     1\n 8     8    176      0     0\n 9     9    339      0     0\n10    10    520      1     1\n\n\n\n\n# A tibble: 10 × 4\n      ID Income Yaruki Rishu\n   <dbl>  <dbl>  <dbl> <dbl>\n 1    11    239      0     0\n 2    12    276      1     0\n 3    13    609      1     1\n 4    14    254      0     0\n 5    15    423      0     1\n 6    16    172      0     1\n 7    17     20      0     0\n 8    18    447      1     0\n 9    19    498      1     1\n10    20    648      1     1\n\n\n\n\n# A tibble: 10 × 4\n      ID Income Yaruki Rishu\n   <dbl>  <dbl>  <dbl> <dbl>\n 1    21    155      0     0\n 2    22    768      1     1\n 3    23    463      1     0\n 4    24    309      1     0\n 5    25    304      0     0\n 6    26    408      1     1\n 7    27    259      0     0\n 8    28    516      1     1\n 9    29    476      1     0\n10    30    110      0     0"
  },
  {
    "objectID": "slide/matching.html#内生性への対処-1",
    "href": "slide/matching.html#内生性への対処-1",
    "title": "方法論特殊講義III",
    "section": "内生性への対処",
    "text": "内生性への対処\n方法: 処置変数と結果変数に影響を与える要因(交絡要因)を揃える\n\n「やる気」のない学生（Yaruki == 0）だけに絞ってみる\n履修有無による平均年収の差は209万円\n\n\n\n\ndf1 |>\n  filter(Yaruki == 0) |>\n  group_by(Rishu) |>\n  summarise(Inc = mean(Income)) |>\n  pull(Inc)\n\n[1] 193.5 402.5\n\n\n\n402.5 - 193.5\n\n[1] 209\n\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID\n      所得\n      やる気\n      履修\n      　\n      ID\n      所得\n      やる気\n      履修\n    \n  \n  \n    1\n659\n0\n1\n\n6\n79\n0\n0\n    7\n356\n0\n1\n\n8\n176\n0\n0\n    15\n423\n0\n1\n\n9\n339\n0\n0\n    16\n172\n0\n1\n\n11\n239\n0\n0\n    \n\n\n\n\n14\n254\n0\n0\n    \n\n\n\n\n17\n20\n0\n0\n    \n\n\n\n\n21\n155\n0\n0\n    \n\n\n\n\n25\n304\n0\n0\n    \n\n\n\n\n27\n259\n0\n0\n    \n\n\n\n\n30\n110\n0\n0\n    Mean\n402.5\n\n\n\nMean\n193.5"
  },
  {
    "objectID": "slide/matching.html#内生性への対処-2",
    "href": "slide/matching.html#内生性への対処-2",
    "title": "方法論特殊講義III",
    "section": "内生性への対処",
    "text": "内生性への対処\n方法: 処置変数と結果変数に影響を与える要因(交絡要因)を揃える\n\n「やる気」のある学生（Yaruki == 1）だけに絞ってみる\n履修有無による平均年収の差は176.3万円\n\n\n\n\ndf1 |>\n  filter(Yaruki == 1) |>\n  group_by(Rishu) |>\n  summarise(Inc = mean(Income)) |>\n  pull(Inc)\n\n[1] 394.2000 570.5455\n\n\n\n570.5455 - 394.2000\n\n[1] 176.3455\n\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID\n      所得\n      やる気\n      履修\n      　\n      ID\n      所得\n      やる気\n      履修\n    \n  \n  \n    2\n587\n1\n1\n\n12\n276\n1\n0\n    3\n628\n1\n1\n\n18\n447\n1\n0\n    4\n563\n1\n1\n\n23\n463\n1\n0\n    5\n531\n1\n1\n\n24\n309\n1\n0\n    10\n520\n1\n1\n\n29\n476\n1\n0\n    13\n609\n1\n1\n\n\n\n\n\n    19\n498\n1\n1\n\n\n\n\n\n    20\n648\n1\n1\n\n\n\n\n\n    22\n768\n1\n1\n\n\n\n\n\n    26\n408\n1\n1\n\n\n\n\n\n    28\n516\n1\n1\n\n\n\n\n\n    Mean\n570.5\n\n\n\nMean\n394.2"
  },
  {
    "objectID": "slide/matching.html#内生性への対処-3",
    "href": "slide/matching.html#内生性への対処-3",
    "title": "方法論特殊講義III",
    "section": "内生性への対処",
    "text": "内生性への対処\n\n\n\n\n\n\n\n\n  \n  \n    \n      履修 (T)\n      平均年収 (Y)\n    \n  \n  \n    1\n\n402.5\n    0\n193.5\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n      履修 (T)\n      平均年収 (Y)\n    \n  \n  \n    1\n\n570.5455\n    0\n394.2000\n  \n  \n  \n\n\n\n\n\n\n\nやる気のある（ない）被験者を一人の被験者として考える場合、差分はITEと解釈可能。\n\nITEの加重平均 \\(\\rightarrow\\) 講義履修の因果効果 \\(\\rightarrow\\) 約191.6万円\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      N\n      やる気 (Zi)\n      Yi(Ti = 1)\n      Yi(Ti = -)\n      ITEi\n    \n  \n  \n    1\n\n14\n\n0\n\n402.5000\n\n193.5000\n\n209.0000\n\n    2\n\n16\n\n1\n\n570.5455\n\n394.2000\n\n176.3455\n\n    加重平均\n\n\n\n\n\n191.5843"
  },
  {
    "objectID": "slide/matching.html#マッチングの考え方-1",
    "href": "slide/matching.html#マッチングの考え方-1",
    "title": "方法論特殊講義III",
    "section": "マッチングの考え方",
    "text": "マッチングの考え方\n割当メカニズムを想定し、交絡要因が同じユニット同士を比較\n\n交絡要因: 処置変数と結果変数、両方と関係のある変数\n以下の条件が満たされる場合、マッチングで因果効果の推定が可能\n条件付き独立の仮定 (Conditional Independece Assumption; CIA)\n\n\\(\\{Y_i(T_i = 1),Y_i(T_i = 0)\\} \\perp T_i∣X_i\\)\n\\(T_i\\) : 学生 \\(i\\) の履修有無、 \\(X_i\\) : 学生 \\(i\\) のやる気\nやる気(=交絡要因)が同じ場合、学生 \\(i\\) がソンさんの講義を履修するか否か(=処置変数)は彼(女)の将来収入(=結果変数)と関係なく決まる\n\\(\\rightarrow\\) 処置変数を外生変数として扱うことが可能に\n\nCIAが満たされるためには、割当メカニズム上のすべての交絡要因が必要"
  },
  {
    "objectID": "slide/matching.html#条件付き独立の仮定とは",
    "href": "slide/matching.html#条件付き独立の仮定とは",
    "title": "方法論特殊講義III",
    "section": "条件付き独立の仮定とは",
    "text": "条件付き独立の仮定とは\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Zi\n      Ti\n      Y0, i\n      Y1, i\n    \n  \n  \n    1\n0\n0\n0\n1\n    2\n0\n0\n1\n0\n    3\n0\n0\n0\n0\n    4\n0\n0\n0\n0\n    5\n0\n1\n0\n0\n    6\n0\n1\n1\n0\n    7\n0\n1\n0\n0\n    8\n0\n1\n0\n1\n    9\n1\n0\n1\n1\n    10\n1\n0\n1\n0\n    11\n1\n0\n0\n1\n    12\n1\n1\n1\n1\n    13\n1\n1\n1\n1\n    14\n1\n1\n0\n1\n    15\n1\n1\n0\n1\n    16\n1\n1\n0\n1\n    17\n1\n1\n1\n1\n    18\n1\n1\n1\n0\n    19\n1\n1\n1\n0\n    20\n1\n1\n1\n0\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n  \n    \n      X\n      Y0\n      Y1\n    \n  \n  \n    T = 0\n\n(A) 0.429\n\n(B) 0.429\n\n    T = 1\n\n(C) 0.538\n\n(D) 0.538\n\n  \n  \n  \n\n\n\n\n\n処置効果は0.538 − 0.429 = 0.109\nもし、統制群と処置群が同質なら\nA = C、そしてB = Dのはず\n処置群がもし統制群になっても、今の統制群と同じ\n\\(\\Rightarrow\\) 交換可能性が成立せず"
  },
  {
    "objectID": "slide/matching.html#条件付き独立の仮定とは-1",
    "href": "slide/matching.html#条件付き独立の仮定とは-1",
    "title": "方法論特殊講義III",
    "section": "条件付き独立の仮定とは",
    "text": "条件付き独立の仮定とは\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Zi\n      Ti\n      Y0, i\n      Y1, i\n    \n  \n  \n    1\n0\n0\n0\n1\n    2\n0\n0\n1\n0\n    3\n0\n0\n0\n0\n    4\n0\n0\n0\n0\n    5\n0\n1\n0\n0\n    6\n0\n1\n1\n0\n    7\n0\n1\n0\n0\n    8\n0\n1\n0\n1\n    9\n1\n0\n1\n1\n    10\n1\n0\n1\n0\n    11\n1\n0\n0\n1\n    12\n1\n1\n1\n1\n    13\n1\n1\n1\n1\n    14\n1\n1\n0\n1\n    15\n1\n1\n0\n1\n    16\n1\n1\n0\n1\n    17\n1\n1\n1\n1\n    18\n1\n1\n1\n0\n    19\n1\n1\n1\n0\n    20\n1\n1\n1\n0\n  \n  \n  \n\n\n\n\n\n\n\n\\(Z\\) で条件づけた場合 ( \\(Z = 0\\) )\n\n\n\n\n\n\n  \n  \n    \n      X\n      Y0\n      Y1\n    \n  \n  \n    T = 0\n\n(A) 0.250\n\n(B) 0.250\n\n    T = 1\n\n(C) 0.250\n\n(D) 0.250\n\n  \n  \n  \n\n\n\n\n\n処置効果は0.250 − 0.250 = 0.000\nもし、統制群と処置群が同質なら\nA = C、そしてB = Dが成立\n\\(\\Rightarrow\\) 交換可能性が成立"
  },
  {
    "objectID": "slide/matching.html#条件付き独立の仮定とは-2",
    "href": "slide/matching.html#条件付き独立の仮定とは-2",
    "title": "方法論特殊講義III",
    "section": "条件付き独立の仮定とは",
    "text": "条件付き独立の仮定とは\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Zi\n      Ti\n      Y0, i\n      Y1, i\n    \n  \n  \n    1\n0\n0\n0\n1\n    2\n0\n0\n1\n0\n    3\n0\n0\n0\n0\n    4\n0\n0\n0\n0\n    5\n0\n1\n0\n0\n    6\n0\n1\n1\n0\n    7\n0\n1\n0\n0\n    8\n0\n1\n0\n1\n    9\n1\n0\n1\n1\n    10\n1\n0\n1\n0\n    11\n1\n0\n0\n1\n    12\n1\n1\n1\n1\n    13\n1\n1\n1\n1\n    14\n1\n1\n0\n1\n    15\n1\n1\n0\n1\n    16\n1\n1\n0\n1\n    17\n1\n1\n1\n1\n    18\n1\n1\n1\n0\n    19\n1\n1\n1\n0\n    20\n1\n1\n1\n0\n  \n  \n  \n\n\n\n\n\n\n\n\\(Z\\) で条件づけた場合 ( \\(Z = 1\\) )\n\n\n\n\n\n\n  \n  \n    \n      X\n      Y0\n      Y1\n    \n  \n  \n    T = 0\n\n(A) 0.667\n\n(B) 0.667\n\n    T = 1\n\n(C) 0.667\n\n(D) 0.667\n\n  \n  \n  \n\n\n\n\n\n処置効果は0.667 − 0.667 = 0.000\nもし、統制群と処置群が同質なら\nA = C、そしてB = Dが成立\n\\(\\Rightarrow\\) 交換可能性が成立"
  },
  {
    "objectID": "slide/matching.html#条件付き独立の仮定とは-3",
    "href": "slide/matching.html#条件付き独立の仮定とは-3",
    "title": "方法論特殊講義III",
    "section": "条件付き独立の仮定とは",
    "text": "条件付き独立の仮定とは\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Zi\n      Ti\n      Y0, i\n      Y1, i\n    \n  \n  \n    1\n0\n0\n0\n1\n    2\n0\n0\n1\n0\n    3\n0\n0\n0\n0\n    4\n0\n0\n0\n0\n    5\n0\n1\n0\n0\n    6\n0\n1\n1\n0\n    7\n0\n1\n0\n0\n    8\n0\n1\n0\n1\n    9\n1\n0\n1\n1\n    10\n1\n0\n1\n0\n    11\n1\n0\n0\n1\n    12\n1\n1\n1\n1\n    13\n1\n1\n1\n1\n    14\n1\n1\n0\n1\n    15\n1\n1\n0\n1\n    16\n1\n1\n0\n1\n    17\n1\n1\n1\n1\n    18\n1\n1\n1\n0\n    19\n1\n1\n1\n0\n    20\n1\n1\n1\n0\n  \n  \n  \n\n\n\n\n\n\n\n条件付き独立が成立するということは\n\n交換可能性が成立\n処置群を統制群に、統制群を処置群にしても同じ結果が得られること"
  },
  {
    "objectID": "slide/matching.html#重回帰分析との比較",
    "href": "slide/matching.html#重回帰分析との比較",
    "title": "方法論特殊講義III",
    "section": "重回帰分析との比較",
    "text": "重回帰分析との比較\n重回帰分析における回帰係数の解釈\n\n他の変数すべてが同じ場合、ある変数が1単位変化する時の応答変数の変化量\nマッチングと同じ?\n\n重回帰分析とマッチングの結果が近似することも \\(\\bigcirc\\)\n\n手計算マッチング: 191.5843\n\n\n\n\n# df1 は matching_data1.csv\n# 単回帰分析\nFit1 <- lm(Income ~ Rishu, data = df1)\n# 重回帰分析\nFit2 <- lm(Income ~ Rishu + Yaruki, \n           data = df1)\n\n\n\n\n\n\n \n  \n      \n    単回帰分析 \n    重回帰分析 \n  \n \n\n  \n    (Intercept) \n    260.400 (36.459) \n    198.595 (32.737) \n  \n  \n    Rishu \n    265.333 (51.561) \n    191.167 (44.915) \n  \n  \n    Yaruki \n     \n    185.415 (45.015) \n  \n  \n    Num.Obs. \n    30 \n    30 \n  \n  \n    R2 \n    0.486 \n    0.684"
  },
  {
    "objectID": "slide/matching.html#重回帰分析との比較-1",
    "href": "slide/matching.html#重回帰分析との比較-1",
    "title": "方法論特殊講義III",
    "section": "重回帰分析との比較",
    "text": "重回帰分析との比較\n実質的にマッチングと回帰分析は同じという見解も (Angrist and Pischke 2009)\n\n具体的に言えば、回帰分析はマッチングの特殊な形態\n\n強い仮定を置いたマッチング\n回帰分析は \\(Y = \\beta_0 + \\beta_1 X_1 + ... + \\beta_k X_k\\) の関数型を仮定 (parametric)\n\n回帰分析において誤差項の平均値は必ず0を仮定 ( \\(\\mathbb{E}(\\varepsilon|T, X) = 0\\) )\n\nマッチングの場合、( \\(\\mathbb{E}(\\varepsilon|T = 0, X) = \\mathbb{E}(\\varepsilon|T = 1, X)\\) )\n\n回帰分析はオーバーラップ条件を無視する\n\nマッチングされないケースでも、線形関数によって予測されてしまう\nマッチングはオーバーラップされないケースを分析から除外する\n\n結論: 回帰分析より柔軟、拡張性がある"
  },
  {
    "objectID": "slide/matching.html#ate-att-atc",
    "href": "slide/matching.html#ate-att-atc",
    "title": "方法論特殊講義III",
    "section": "ATE, ATT, ATC",
    "text": "ATE, ATT, ATC\n3種類の因果効果\n\nATE (Average Treatment Effect): 平均処置効果\nATT (ATE for the Treated): 処置群における平均処置効果\n\n潜在結果: 処置群が処置を受けなかった場合の応答変数\n\nATC (ATE for the Control): 統制群における平均処置効果\n\n潜在結果: 統制群が処置を受けた場合の応答変数\n\n\n\n\n因果効果は一般的に母集団ではなく、サンプルから推定されるため、「SATE/SATT/SATC」と呼ばれる場合も\n他にもRIE (Retrospective Intervention Effect) なども (Samii et al. 2016)\nRCTでは主にATEが推定対象（マッチングでは区分するケースが多い）\n統計ソフトウェアによってはATTを因果効果の推定値として表示する場合もある。"
  },
  {
    "objectID": "slide/matching.html#ate-att-atc-1",
    "href": "slide/matching.html#ate-att-atc-1",
    "title": "方法論特殊講義III",
    "section": "ATE, ATT, ATC",
    "text": "ATE, ATT, ATC\nATT: 処置群における平均処置効果\n\n処置群の潜在的結果を統制群から割り当てる。\n処置群は \\(Y_i(T_i = 1)\\) が観察済みであり、潜在的結果は \\(Y_i(T_i = 0)\\)\nやる気のない学生の \\(Y_i(T_i = 0)\\) は193.5、ある学生は394.2\n\n\n\n\n\n\n\n  \n  \n    \n      ID\n      Yaruki\n      Y(T = 1)\n      Y(T = 0)\n      ITE\n    \n  \n  \n    1\n0\n659\n193.5\n465.5\n    2\n1\n587\n394.2\n192.8\n    3\n1\n628\n394.2\n233.8\n    4\n1\n563\n394.2\n168.8\n    5\n1\n531\n394.2\n136.8\n    7\n0\n356\n193.5\n162.5\n    ...\n...\n...\n...\n...\n    20\n1\n648\n394.2\n253.8\n    22\n1\n768\n394.2\n373.8\n    26\n1\n408\n394.2\n13.8\n    28\n1\n516\n394.2\n121.8\n    平均\n\n\n\n185.1"
  },
  {
    "objectID": "slide/matching.html#ate-att-atc-2",
    "href": "slide/matching.html#ate-att-atc-2",
    "title": "方法論特殊講義III",
    "section": "ATE, ATT, ATC",
    "text": "ATE, ATT, ATC\nATC: 統制群における平均処置効果\n\n統制群の潜在的結果を処置群から割り当てる。\n統制群は \\(Y_i(T_i = 0)\\) が観察済みであり、潜在的結果は \\(Y_i(T_i = 1)\\)\nやる気のない学生の \\(Y_i(T_i = 1)\\) は402.5、ある学生は570.5\n\n\n\n\n\n\n\n  \n  \n    \n      ID\n      Yaruki\n      Y(T = 1)\n      Y(T = 0)\n      ITE\n    \n  \n  \n    6\n0\n402.5\n79\n323.5\n    8\n0\n402.5\n176\n226.5\n    9\n0\n402.5\n339\n63.5\n    11\n0\n402.5\n239\n163.5\n    12\n1\n570.5\n276\n294.5\n    14\n0\n402.5\n254\n148.5\n    ...\n...\n...\n...\n...\n    25\n0\n402.5\n304\n98.5\n    27\n0\n402.5\n259\n143.5\n    29\n1\n570.5\n476\n94.5\n    30\n0\n402.5\n110\n292.5\n    平均\n\n\n\n185.1"
  },
  {
    "objectID": "slide/matching.html#ate-att-atc-3",
    "href": "slide/matching.html#ate-att-atc-3",
    "title": "方法論特殊講義III",
    "section": "ATE, ATT, ATC",
    "text": "ATE, ATT, ATC\nATE: 平均処置効果\n\nATTとATCの加重平均\n今回は処置群と統制群が15:15 \\(\\rightarrow\\) 単純平均でOK\n\n\\(\\frac{1}{2}(185.1 + 198.1) = 191.6\\)\n手計算マッチングとと同じ結果\n\n\n\\[\\text{ATE} = \\frac{N_{\\text{treated}}}{N_{\\text{all}}} \\text{ATT} + \\frac{N_{\\text{controlled}}}{N_{\\text{all}}} \\text{ATC}.\\]"
  },
  {
    "objectID": "slide/matching.html#マッチングいろいろ",
    "href": "slide/matching.html#マッチングいろいろ",
    "title": "方法論特殊講義III",
    "section": "マッチングいろいろ",
    "text": "マッチングいろいろ\n\nExact Matching\nNearest-neighbor Matching\n\nk-nearest Neighbor Matching\nCaliper Matching (Radius Matching)\n\nCoarsened Exact Matching\nPropensity Score Matching\n\nInverse Probability Weighting\nEnsemble Matching"
  },
  {
    "objectID": "slide/matching.html#exact-matching",
    "href": "slide/matching.html#exact-matching",
    "title": "方法論特殊講義III",
    "section": "Exact Matching",
    "text": "Exact Matching\n\n「正確マッチング」、「厳格なマッチング」などで訳される\nこれまで見てきた方法が Exact Matching\n\nデータ内の共変量 (交絡要因) が完全に一致するケース同士の比較\n\n共変量が少数、かつ、名目or順序変数の場合、使用可\n共変量が多数、または連続変数の場合は実質的に無理\n\n次元の呪い or 次元爆発"
  },
  {
    "objectID": "slide/matching.html#nearest-neighbor-matching",
    "href": "slide/matching.html#nearest-neighbor-matching",
    "title": "方法論特殊講義III",
    "section": "Nearest-neighbor Matching",
    "text": "Nearest-neighbor Matching\nNearest-neighbor Matching\n\n「最近傍マッチング」と訳される。\n共変量が連続変数、多次元の場合、「完全に一致」ケースはない場合がほとんど\n\n\\(\\rightarrow\\) 「一致」ではなく、「最も似ている」ケース同士と比較\n共変量を座標（超）平面に位置づけた場合、最も近いケースをマッチング\n\n\n\n「近さ」の基準\n\nManhattan Distance\nStandardized Euclidean Distance\nMahalanobis Distance (\\(\\leftarrow\\) 最もよく使われる基準)"
  },
  {
    "objectID": "slide/matching.html#nearest-neighbor-matching-1",
    "href": "slide/matching.html#nearest-neighbor-matching-1",
    "title": "方法論特殊講義III",
    "section": "Nearest-neighbor Matching",
    "text": "Nearest-neighbor Matching\nManhattan Distance (City-block Distance)\n\\[d(i, j) = |X_i - X_j| + |Y_i - Y_j| \\text{ where } i \\neq j.\\]"
  },
  {
    "objectID": "slide/matching.html#nearest-neighbor-matching-2",
    "href": "slide/matching.html#nearest-neighbor-matching-2",
    "title": "方法論特殊講義III",
    "section": "Nearest-neighbor Matching",
    "text": "Nearest-neighbor Matching\nStandardized Euclidean Distance\n\\[d(i, j) = \\sqrt{\\Bigg(\\frac{X_i - X_j}{\\sigma_X}\\Bigg)^2 + \\Bigg(\\frac{Y_i - Y_j}{\\sigma_Y}\\Bigg)^2} \\text{ where } i \\neq j.\\]"
  },
  {
    "objectID": "slide/matching.html#nearest-neighbor-matching-3",
    "href": "slide/matching.html#nearest-neighbor-matching-3",
    "title": "方法論特殊講義III",
    "section": "Nearest-neighbor Matching",
    "text": "Nearest-neighbor Matching\nMahalanobis Distance\n\n共変量間の相関が0の場合、Standardized Euclidean Distanceと同じ\n\n\\[d(i, j) = \\sqrt{\\frac{1}{1 - \\rho^2_{X, Y}} \\Bigg[\\Bigg(\\frac{X_i - X_j}{\\sigma_X}\\Bigg)^2 + \\Bigg(\\frac{Y_i - Y_j}{\\sigma_Y}\\Bigg)^2 - 2\\rho_{X,Y}\\Bigg(\\frac{X_i - X_j}{\\sigma_X}\\Bigg) \\Bigg(\\frac{Y_i - Y_j}{\\sigma_Y}\\Bigg)\\Bigg]} \\text{ where } i \\neq j.\\]"
  },
  {
    "objectID": "slide/matching.html#マッチング方法",
    "href": "slide/matching.html#マッチング方法",
    "title": "方法論特殊講義III",
    "section": "マッチング方法",
    "text": "マッチング方法\nATTの場合、処置群のケースに統制群の中で最も近いケース1個を割当\n\n近さの測定はマハラノビス距離が一般的\n処置群内の1ケースに複数の統制群ケースを割り当てる場合も\n\nk-nearest Neighbor Matching\nCaliper Matching\n\n復元マッチングと非復元マッチング"
  },
  {
    "objectID": "slide/matching.html#k-nearest-neighbor-matching",
    "href": "slide/matching.html#k-nearest-neighbor-matching",
    "title": "方法論特殊講義III",
    "section": "k-nearest Neighbor Matching",
    "text": "k-nearest Neighbor Matching\nk-最近傍マッチング\n\n最も近い1個ケースを潜在的結果として使うのではなく、最も近い \\(k\\) 個のケースの平均値を潜在的結果として用いる。\n\n\\(j(m)\\) : \\(i\\) から \\(m\\) 番目に近いケース \\(j\\)\n\n\n\\[Y_i(T_i = 0) = \\begin{cases}Y_i & \\text{ if } T_i = 0\\\\ \\frac{1}{K} \\sum_{m = 1}^K Y_{j(m)} & \\text{ if } T_i = 1\\end{cases}\\]\n\n最適 \\(k\\) を決める理論的基準は無し\n\n\\(k\\) を大きくすると、モデルの分散が小さくなる\nただし、モデルの分散が小さい = バイアスが拡大\n\nBias–variance trade-off\n\n\\(k\\) を変化させることによって結果がどのように変わるか観察"
  },
  {
    "objectID": "slide/matching.html#caliper-matching",
    "href": "slide/matching.html#caliper-matching",
    "title": "方法論特殊講義III",
    "section": "Caliper Matching",
    "text": "Caliper Matching\n「カリパーマッチング」と訳される（訳されてない…?）\n\n半径 \\(h\\) の中にある全てのケースの平均値を潜在的結果として使用\n\n\n\n\\[Y_i(T_i = 0) = \\begin{cases}Y_i & \\text{ if } T_i = 0\\\\ \\frac{\\sum_{j=1}^N I(T_j = 0, d(i, j) < h)\\cdot Y_i}{\\sum_{j=1}^N I(T_j = 0, d(i, j) < h)} & \\text{ if } T_i = 1\\end{cases}\\]"
  },
  {
    "objectID": "slide/matching.html#coarsened-exact-matching",
    "href": "slide/matching.html#coarsened-exact-matching",
    "title": "方法論特殊講義III",
    "section": "Coarsened Exact Matching",
    "text": "Coarsened Exact Matching\nCoarsened Exact Matching (Iacus, King, and Porro 2011)\n\n定訳はなく、「CEM」で呼ばれる(粗い正確マッチング?)\nアルゴリズムは簡単\n\n共変量をいくつかの層 (strata) へ分割する。\n各層にそれぞれ該当する処置・統制ユニットを入れる。\n最低一つ以上の処置・統制ユニットがない層は捨てる。\n各層の処置・統制ユニットの結果変数の差分を計算し、すべての層に対して加重平均\n\n層を細かくするほどExact Matchingへ近づく\n\nただし、マッチングされないケースが多くなり、分析に使えるケースが減少\nバイアス\\(\\downarrow\\); 分散\\(\\uparrow\\)"
  },
  {
    "objectID": "slide/matching.html#cemの例-matching_data2.csv",
    "href": "slide/matching.html#cemの例-matching_data2.csv",
    "title": "方法論特殊講義III",
    "section": "CEMの例 (matching_data2.csv)",
    "text": "CEMの例 (matching_data2.csv)\n年齢は10歳刻み、学歴は大卒以上・未満に層化\n\n\n\n\n\n\n  \n  \n    \n      ID\n      年齢\n      教育\n      処置\n      結果\n      　\n      ID\n      年齢\n      教育\n      処置\n      結果\n    \n  \n  \n    1\n29\n院\n0\n6\n\n13\n57\n高\n0\n4\n    2\n41\n大\n0\n3\n\n14\n25\n院\n1\n5\n    3\n31\n院\n1\n7\n\n15\n55\n中\n1\n9\n    4\n39\n院\n0\n5\n\n16\n48\n院\n0\n2\n    5\n53\n大\n0\n6\n\n17\n23\n専\n0\n2\n    6\n59\n大\n0\n1\n\n18\n34\n大\n1\n4\n    7\n37\n高\n1\n8\n\n19\n42\n大\n1\n9\n    8\n44\n中\n0\n4\n\n20\n23\n高\n0\n4\n    9\n51\n中\n0\n2\n\n21\n22\n高\n1\n8\n    10\n59\n小\n1\n8\n\n22\n49\n大\n0\n9\n    11\n21\n大\n1\n4\n\n23\n45\n高\n1\n6\n    12\n24\n中\n1\n6\n\n24\n33\n大\n0\n8"
  },
  {
    "objectID": "slide/matching.html#cemの例-matching_data2.csv-1",
    "href": "slide/matching.html#cemの例-matching_data2.csv-1",
    "title": "方法論特殊講義III",
    "section": "CEMの例 (matching_data2.csv)",
    "text": "CEMの例 (matching_data2.csv)\n年齢は10歳刻み、学歴は大卒以上・未満に層化 * カテゴリが少なくなり、マッチングされやすくなる\n.font80[]"
  },
  {
    "objectID": "slide/matching.html#cemの例-matching_data2.csv-2",
    "href": "slide/matching.html#cemの例-matching_data2.csv-2",
    "title": "方法論特殊講義III",
    "section": "CEMの例 (matching_data2.csv)",
    "text": "CEMの例 (matching_data2.csv)\n層ごとにケースをマッチング (ペアが組めない層もあり)\n.font80[]"
  },
  {
    "objectID": "slide/matching.html#cemの例-matching_data2.csv-3",
    "href": "slide/matching.html#cemの例-matching_data2.csv-3",
    "title": "方法論特殊講義III",
    "section": "CEMの例 (matching_data2.csv)",
    "text": "CEMの例 (matching_data2.csv)\nペアが組めない層を除外\n.font80[]"
  },
  {
    "objectID": "slide/matching.html#cemの例-matching_data2.csv-4",
    "href": "slide/matching.html#cemの例-matching_data2.csv-4",
    "title": "方法論特殊講義III",
    "section": "CEMの例 (matching_data2.csv)",
    "text": "CEMの例 (matching_data2.csv)\n各ユニットの重みを計算 * \\(m_{C,T}\\) : 統制・処置ケースの数、 \\(m^s_{C,T}\\) : 層 \\(s\\) 内の統制・処置ケースの数\n\\[w_i = \\begin{cases} 1 & \\text{ if } T_i = 1, \\\\ \\frac{m_C}{m_T} \\cdot \\frac{m^s_T}{m^s_C} & \\text{ if } T_i = 0.\\end{cases}\\]\n.font70[]"
  },
  {
    "objectID": "slide/matching.html#cemの例-matching_data2matching_data2.csv",
    "href": "slide/matching.html#cemの例-matching_data2matching_data2.csv",
    "title": "方法論特殊講義III",
    "section": "CEMの例 (matching_data2matching_data2.csv)",
    "text": "CEMの例 (matching_data2matching_data2.csv)\n重み付け回帰分析\n\n\\(W = \\{2.200, 0.367, 1.000, 1.100, 0.000, 0.000, ..., 1.100\\}^{\\top}\\)\n\nマッチングされないケースの重みは0にするか、分析から除外\n\\(\\beta = (X^{\\top}WX)^{-1}X^{\\top}WY\\)\n\nRの場合、lm(formula, data, weight = ...)で推定可能\n\n{cem} or {MatchIt}パッケージならもっと簡単\n\n\\(\\widehat{\\text{Outcome}} = 4.567 + 2.033 \\cdot \\text{Treat}\\)\n\n処置群における因果効果 (ATT) = 2.033"
  },
  {
    "objectID": "slide/matching.html#傾向スコアとは",
    "href": "slide/matching.html#傾向スコアとは",
    "title": "方法論特殊講義III",
    "section": "傾向スコアとは",
    "text": "傾向スコアとは\nPropensity Score\n\n簡単にいうと「あるユニット \\(i\\) が処置を受ける確率」\n\n\\(e_i = Pr(T_i = 1 | X_i)\\)\n\n\n\nなぜ傾向スコア?\n\nマッチングの限界\n\n次元の問題 (dimension problem)\n恣意性\nカテゴリ変数の扱い方\nスケールの問題"
  },
  {
    "objectID": "slide/matching.html#傾向スコアの計算",
    "href": "slide/matching.html#傾向スコアの計算",
    "title": "方法論特殊講義III",
    "section": "傾向スコアの計算",
    "text": "傾向スコアの計算\n処置変数 ( \\(T_i\\) ) を応答変数とし、共変量 ( \\(X_i\\) ) を説明変数とする\n\n一般的に、ロジットやプロビット回帰分析で推定する。\n\n他にも色々ある\n\nSupport Vector Machine, Decision Tree, Neural Network, …\n\n色んな手法で算出した傾向スコアを重み付けして合成することも可能\n\nEnsemble Method (Samii, Paler, and Zukerman 2016)\n\n\n\n\n\n推定された予測確率 \\(\\rightarrow\\) 傾向スコア\n\nRではオブジェクト名$fitted.valueで抽出可\n傾向スコアは多くの共変量を一つの変数に集約したもの\n\\(\\rightarrow\\) 傾向スコアを統制した回帰分析で因果効果を推定\n\\(\\rightarrow\\) 傾向スコアを用いて最近傍マッチング"
  },
  {
    "objectID": "slide/matching.html#傾向スコアマッチングの手順",
    "href": "slide/matching.html#傾向スコアマッチングの手順",
    "title": "方法論特殊講義III",
    "section": "傾向スコア・マッチングの手順",
    "text": "傾向スコア・マッチングの手順\n割り当てメカニズムを仮定\n\n\n\\[Pr(\\text{処置}) \\propto \\beta_0 + \\beta_1 \\cdot \\text{年齢} + \\beta_2 \\cdot \\text{教育}\\]\n\n\n\n\n\n\n\n  \n  \n    \n      ID\n      年齢\n      教育\n      処置\n      結果\n      　\n      ID\n      年齢\n      教育\n      処置\n      結果\n    \n  \n  \n    1\n29\n院\n0\n6\n\n13\n57\n高\n0\n4\n    2\n41\n大\n0\n3\n\n14\n25\n院\n1\n5\n    3\n31\n院\n1\n7\n\n15\n55\n中\n1\n9\n    4\n39\n院\n0\n5\n\n16\n48\n院\n0\n2\n    5\n53\n大\n0\n6\n\n17\n23\n専\n0\n2\n    6\n59\n大\n0\n1\n\n18\n34\n大\n1\n4\n    7\n37\n高\n1\n8\n\n19\n42\n大\n1\n9\n    8\n44\n中\n0\n4\n\n20\n23\n高\n0\n4\n    9\n51\n中\n0\n2\n\n21\n22\n高\n1\n8\n    10\n59\n小\n1\n8\n\n22\n49\n大\n0\n9\n    11\n21\n大\n1\n4\n\n23\n45\n高\n1\n6\n    12\n24\n中\n1\n6\n\n24\n33\n大\n0\n8"
  },
  {
    "objectID": "slide/matching.html#傾向スコアの算出",
    "href": "slide/matching.html#傾向スコアの算出",
    "title": "方法論特殊講義III",
    "section": "傾向スコアの算出",
    "text": "傾向スコアの算出\n傾向スコアの算出\n\nPS_Fit <- glm(処置 ~ 年齢 + 学歴, data = データ, family = binomial(\"logit\"))\nsummary(PS_Fit)\n\n\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    3.839 (2.364) \n  \n  \n    Age \n    −0.059 (0.039) \n  \n  \n    Edu \n    −0.420 (0.304) \n  \n  \n    Num.Obs. \n    24 \n  \n  \n    AIC \n    35.5 \n  \n  \n    F \n    1.486"
  },
  {
    "objectID": "slide/matching.html#傾向スコアの算出-1",
    "href": "slide/matching.html#傾向スコアの算出-1",
    "title": "方法論特殊講義III",
    "section": "傾向スコアの算出",
    "text": "傾向スコアの算出\n傾向スコアの抽出\n\n\n\nデータ$PS <- PS_Fit$fitted.value\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID\n      年齢\n      教育\n      処置\n      結果\n      PS\n      　\n      ID\n      年齢\n      教育\n      処置\n      結果\n      PS\n    \n  \n  \n    1\n29\n院\n0\n6\n0.406\n\n13\n57\n高\n0\n4\n0.317\n    2\n41\n大\n0\n3\n0.339\n\n14\n25\n院\n1\n5\n0.463\n    3\n31\n院\n1\n7\n0.378\n\n15\n55\n中\n1\n9\n0.443\n    4\n39\n院\n0\n5\n0.275\n\n16\n48\n院\n0\n2\n0.183\n    5\n53\n大\n0\n6\n0.203\n\n17\n23\n専\n0\n2\n0.692\n    6\n59\n大\n0\n1\n0.152\n\n18\n34\n大\n1\n4\n0.437\n    7\n37\n高\n1\n8\n0.601\n\n19\n42\n大\n1\n9\n0.326\n    8\n44\n中\n0\n4\n0.603\n\n20\n23\n高\n0\n4\n0.774\n    9\n51\n中\n0\n2\n0.502\n\n21\n22\n高\n1\n8\n0.784\n    10\n59\n小\n1\n8\n0.489\n\n22\n49\n大\n0\n9\n0.243\n    11\n21\n大\n1\n4\n0.624\n\n23\n45\n高\n1\n6\n0.485\n    12\n24\n中\n1\n6\n0.831\n\n24\n33\n大\n0\n8\n0.451"
  },
  {
    "objectID": "slide/matching.html#傾向スコアの算出-2",
    "href": "slide/matching.html#傾向スコアの算出-2",
    "title": "方法論特殊講義III",
    "section": "傾向スコアの算出",
    "text": "傾向スコアの算出\n\n\nATT: 傾向スコアが最も近い統制群を割り当てる\n\n一回マッチングされたケースを除外する vs. しない\n傾向スコアが同じケースが複数ある場合の対処\n\n\n\n\n\n\n\n\n  \n  \n    \n      \n        処置群\n      \n      　\n      \n        統制群\n      \n      差分\n    \n    \n      ID\n      結果\n      PS\n      ID\n      結果\n      PS\n    \n  \n  \n    3\n7\n0.378\n\n1\n6\n0.406\n1\n    7\n8\n0.601\n\n8\n4\n0.603\n4\n    10\n8\n0.489\n\n9\n2\n0.502\n6\n    11\n4\n0.624\n\n8\n4\n0.603\n0\n    12\n6\n0.831\n\n20\n4\n0.774\n2\n    14\n5\n0.463\n\n24\n8\n0.451\n-3\n    15\n9\n0.443\n\n24\n8\n0.451\n1\n    18\n4\n0.437\n\n24\n8\n0.451\n-4\n    19\n9\n0.326\n\n13\n4\n0.317\n5\n    21\n8\n0.784\n\n20\n4\n0.774\n4\n    23\n6\n0.485\n\n9\n2\n0.502\n4\n  \n  \n  \n    \n       差分の平均値 (ATT): 1.818"
  },
  {
    "objectID": "slide/matching.html#傾向スコアの算出-3",
    "href": "slide/matching.html#傾向スコアの算出-3",
    "title": "方法論特殊講義III",
    "section": "傾向スコアの算出",
    "text": "傾向スコアの算出\n\n\nATC: 傾向スコアが最も近い処置群を割り当てる\n\n\n\n\n\n\n\n  \n  \n    \n      \n        処置群\n      \n      　\n      \n        統制群\n      \n      差分\n    \n    \n      ID\n      結果\n      PS\n      ID\n      結果\n      PS\n    \n  \n  \n    3\n7\n0.378\n\n1\n6\n0.406\n1\n    19\n9\n0.326\n\n2\n3\n0.339\n6\n    19\n9\n0.326\n\n4\n5\n0.275\n4\n    19\n9\n0.326\n\n5\n6\n0.203\n3\n    19\n9\n0.326\n\n6\n1\n0.152\n8\n    7\n8\n0.601\n\n8\n4\n0.603\n4\n    10\n8\n0.489\n\n9\n2\n0.502\n6\n    19\n9\n0.326\n\n13\n4\n0.317\n5\n    19\n9\n0.326\n\n16\n2\n0.183\n7\n    11\n4\n0.624\n\n17\n2\n0.692\n2\n    21\n8\n0.784\n\n20\n4\n0.774\n4\n    19\n9\n0.326\n\n22\n9\n0.243\n0\n    15\n9\n0.443\n\n24\n8\n0.451\n1\n  \n  \n  \n    \n       差分の平均値 (ATC): 3.923"
  },
  {
    "objectID": "slide/matching.html#傾向スコアの算出-4",
    "href": "slide/matching.html#傾向スコアの算出-4",
    "title": "方法論特殊講義III",
    "section": "傾向スコアの算出",
    "text": "傾向スコアの算出\nATE: ATTとATCの加重平均\n\\[\\begin{align}\\text{ATE} & = \\frac{N_\\text{Treat}}{N_\\text{All}}\\text{ATT} + \\frac{N_\\text{Control}}{N_\\text{All}}\\text{ATC} \\\\ & = \\frac{11}{24} 1.818 + \\frac{13}{24} 3.923 = 2.958\\end{align}\\]\n\n# 第1引数は平均値を求める値のベクトル、第2引数は重みのベクトル\n# 重みは合計1になるように c(0.4583333, 0.5416667) でもOK\nweighted.mean(c(1.818, 3.923), c(11, 13))\n\n[1] 2.958208"
  },
  {
    "objectID": "slide/matching.html#処置を受ける確率の計算",
    "href": "slide/matching.html#処置を受ける確率の計算",
    "title": "方法論特殊講義III",
    "section": "処置を受ける確率の計算",
    "text": "処置を受ける確率の計算\n処置を受ける確率 = 傾向スコア\n\n一般的にはロジスティック/プロビット回帰分析が使われる\nただし、確率が予測できるなら他の手法でも良い\n\nCovariate Balancing Propensity Score\nEntropy Balancing\nNeural Network\nSupport Vector Machine（SVM）\nRandom Forest（RF）など\n\n\n\n\n複数の手法を組み合わせる(= ensemble)することも可能\n\n\\(\\rightarrow\\) Super Learner Algorithm"
  },
  {
    "objectID": "slide/matching.html#傾向スコアのもう一つの使い方",
    "href": "slide/matching.html#傾向スコアのもう一つの使い方",
    "title": "方法論特殊講義III",
    "section": "傾向スコアのもう一つの使い方",
    "text": "傾向スコアのもう一つの使い方\nIPW: Inverse Probability Weighting (Rubin 1985)\n\n「逆確率重み付け」と訳される\n実際に処置を受けた( \\(T_i = 1\\) )にもかかわらず、処置を受ける傾向が 小さい場合は分析において大きい重み\n\n傾向スコアを重み変数として用いる。\n\\(e_i\\) が1または0に近い場合、一部のケースに非常に大きい重みを付け ることになるため、注意が必要\n\n\n\\[w_i = \\begin{cases}\\frac{1}{e_i} & \\text{ if } T_i = 1, \\\\ \\frac{1}{1 - e_i} & \\text{ if } T_i = 0.\\end{cases}\\]\n\n\\(e_i\\) : \\(i\\) の傾向スコア; \\(T_i\\) : \\(i\\) の処置有無 ( \\(\\in \\{0, 1\\}\\) )"
  },
  {
    "objectID": "slide/matching.html#傾向スコアのもう一つの使い方-1",
    "href": "slide/matching.html#傾向スコアのもう一つの使い方-1",
    "title": "方法論特殊講義III",
    "section": "傾向スコアのもう一つの使い方",
    "text": "傾向スコアのもう一つの使い方\nIPW: Inverse Probability Weighting (Rubin 1985)\n\n「逆確率重み付け」と訳される\n実際に処置を受けた( \\(T_i = 1\\) )にもかかわらず、処置を受ける傾向が 小さい場合は分析において大きい重み\n\n傾向スコアを重み変数として用いる。\n\\(e_i\\) が1または0に近い場合、一部のケースに非常に大きい重みを付け ることになるため、注意が必要\n\n\n\\[w_i = T_i \\frac{1}{e_i} + (1 - T_i) \\frac{1}{1 - e_i}.\\]\n\n\\(e_i\\) : \\(i\\) の傾向スコア\n\\(T_i\\) : \\(i\\) の処置有無 ( \\(T_i \\in \\{0, 1\\}\\) )"
  },
  {
    "objectID": "slide/matching.html#ipw-の考え方",
    "href": "slide/matching.html#ipw-の考え方",
    "title": "方法論特殊講義III",
    "section": "IPW の考え方",
    "text": "IPW の考え方\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Zi\n      Ti\n      Y0, i\n      Y1, i\n    \n  \n  \n    1\n0\n0\n0\n1\n    2\n0\n0\n1\n0\n    3\n0\n0\n0\n0\n    4\n0\n0\n0\n0\n    5\n0\n1\n0\n0\n    6\n0\n1\n1\n0\n    7\n0\n1\n0\n0\n    8\n0\n1\n0\n1\n    9\n1\n0\n1\n1\n    10\n1\n0\n1\n0\n    11\n1\n0\n0\n1\n    12\n1\n1\n1\n1\n    13\n1\n1\n1\n1\n    14\n1\n1\n0\n1\n    15\n1\n1\n0\n1\n    16\n1\n1\n0\n1\n    17\n1\n1\n1\n1\n    18\n1\n1\n1\n0\n    19\n1\n1\n1\n0\n    20\n1\n1\n1\n0\n  \n  \n  \n\n\n\n\n\n条件付き独立の例のデータ（matching_data3.csv）\n\n\\(Pr(Z = 0) = 0.4\\) 、 \\(Pr(Z = 1) = 0.6\\)\n\\(Z = 0\\) の場合\n\n\\(Pr(T = 0) = 0.5\\) 、 \\(Pr(T = 1) = 0.5\\)\n\n\\(Z = 1\\) の場合\n\n\\(Pr(T = 0) = 0.25\\) 、 \\(Pr(T = 1) = 0.75\\)\n\n観察されたデータからは約0.1の処置効果が推定されるが、本当の処置効果は0"
  },
  {
    "objectID": "slide/matching.html#ipwの考え方",
    "href": "slide/matching.html#ipwの考え方",
    "title": "方法論特殊講義III",
    "section": "IPWの考え方",
    "text": "IPWの考え方"
  },
  {
    "objectID": "slide/matching.html#ipwの考え方-1",
    "href": "slide/matching.html#ipwの考え方-1",
    "title": "方法論特殊講義III",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "slide/matching.html#ipwの考え方-2",
    "href": "slide/matching.html#ipwの考え方-2",
    "title": "方法論特殊講義III",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "slide/matching.html#ipwの考え方-3",
    "href": "slide/matching.html#ipwの考え方-3",
    "title": "方法論特殊講義III",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "slide/matching.html#ipwの考え方-4",
    "href": "slide/matching.html#ipwの考え方-4",
    "title": "方法論特殊講義III",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "slide/matching.html#ipwの考え方-5",
    "href": "slide/matching.html#ipwの考え方-5",
    "title": "方法論特殊講義III",
    "section": "IPWの考え方",
    "text": "IPWの考え方\n.center[]"
  },
  {
    "objectID": "slide/matching.html#他の考え方",
    "href": "slide/matching.html#他の考え方",
    "title": "方法論特殊講義III",
    "section": "他の考え方",
    "text": "他の考え方\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Zi\n      Ti\n      Y0, i\n      Y1, i\n      ei\n      Wi\n    \n  \n  \n    1\n0\n0\n0\n?\n0.50\n2.00\n    2\n0\n0\n1\n?\n0.50\n2.00\n    3\n0\n0\n0\n?\n0.50\n2.00\n    4\n0\n0\n0\n?\n0.50\n2.00\n    5\n0\n1\n?\n0\n0.50\n \n    6\n0\n1\n?\n0\n0.50\n \n    7\n0\n1\n?\n0\n0.50\n \n    8\n0\n1\n?\n1\n0.50\n \n    9\n1\n0\n1\n?\n0.75\n4.00\n    10\n1\n0\n1\n?\n0.75\n4.00\n    11\n1\n0\n0\n?\n0.75\n4.00\n    12\n1\n1\n?\n1\n0.75\n \n    13\n1\n1\n?\n1\n0.75\n \n    14\n1\n1\n?\n1\n0.75\n \n    15\n1\n1\n?\n1\n0.75\n \n    16\n1\n1\n?\n1\n0.75\n \n    17\n1\n1\n?\n1\n0.75\n \n    18\n1\n1\n?\n0\n0.75\n \n    19\n1\n1\n?\n0\n0.75\n \n    20\n1\n1\n?\n0\n0.75\n \n  \n  \n  \n\n\n\n\n\nもし、全ケースが統制群なら?\n\n\\(Z = 0\\) の統制群は4ケース（ID = 1, 2, 3, 4）\n\\(Z = 0\\) は全部で8ケース（2倍）\n\\(\\rightarrow\\) ケース1〜4の重みを2倍に（\\(W = 2\\)）\n\n\n\n\\(Z = 1\\) の統制群は3ケース（ID = 9, 10, 11）\n\\(Z = 1\\) は全部で12ケース（4倍）\n\\(\\rightarrow\\) ケース9〜11の重みを4倍に（\\(W = 4\\)）"
  },
  {
    "objectID": "slide/matching.html#他の考え方-1",
    "href": "slide/matching.html#他の考え方-1",
    "title": "方法論特殊講義III",
    "section": "他の考え方",
    "text": "他の考え方\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Zi\n      Ti\n      Y0, i\n      Y1, i\n      ei\n      Wi\n    \n  \n  \n    1\n0\n0\n0\n?\n0.50\n \n    2\n0\n0\n1\n?\n0.50\n \n    3\n0\n0\n0\n?\n0.50\n \n    4\n0\n0\n0\n?\n0.50\n \n    5\n0\n1\n?\n0\n0.50\n2.00\n    6\n0\n1\n?\n0\n0.50\n2.00\n    7\n0\n1\n?\n0\n0.50\n2.00\n    8\n0\n1\n?\n1\n0.50\n2.00\n    9\n1\n0\n1\n?\n0.75\n \n    10\n1\n0\n1\n?\n0.75\n \n    11\n1\n0\n0\n?\n0.75\n \n    12\n1\n1\n?\n1\n0.75\n1.33\n    13\n1\n1\n?\n1\n0.75\n1.33\n    14\n1\n1\n?\n1\n0.75\n1.33\n    15\n1\n1\n?\n1\n0.75\n1.33\n    16\n1\n1\n?\n1\n0.75\n1.33\n    17\n1\n1\n?\n1\n0.75\n1.33\n    18\n1\n1\n?\n0\n0.75\n1.33\n    19\n1\n1\n?\n0\n0.75\n1.33\n    20\n1\n1\n?\n0\n0.75\n1.33\n  \n  \n  \n\n\n\n\n\nもし、全ケースが処置群なら?\n\n\\(Z = 0\\) の処置群は4ケース（ID = 5, 6, 7, 8）\n\\(Z = 0\\) は全部で8ケース（2倍）\n\\(\\rightarrow\\) ケース5〜8の重みを2倍に（\\(W = 2\\)）\n\n\n\n\\(Z = 1\\) の処置群は9ケース（ID = 12, 13, 14, … 20）\n\\(Z = 1\\) は全部で12ケース（1.333倍）\n\\(\\rightarrow\\) ケース12〜20の重みを1.333倍に（\\(W = 1.333\\)）"
  },
  {
    "objectID": "slide/matching.html#他の考え方-2",
    "href": "slide/matching.html#他の考え方-2",
    "title": "方法論特殊講義III",
    "section": "他の考え方",
    "text": "他の考え方\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Zi\n      Ti\n      Y0, i\n      Y1, i\n      ei\n      Wi\n    \n  \n  \n    1\n0\n0\n0\n?\n0.50\n2.00\n    2\n0\n0\n1\n?\n0.50\n2.00\n    3\n0\n0\n0\n?\n0.50\n2.00\n    4\n0\n0\n0\n?\n0.50\n2.00\n    5\n0\n1\n?\n0\n0.50\n2.00\n    6\n0\n1\n?\n0\n0.50\n2.00\n    7\n0\n1\n?\n0\n0.50\n2.00\n    8\n0\n1\n?\n1\n0.50\n2.00\n    9\n1\n0\n1\n?\n0.75\n4.00\n    10\n1\n0\n1\n?\n0.75\n4.00\n    11\n1\n0\n0\n?\n0.75\n4.00\n    12\n1\n1\n?\n1\n0.75\n1.33\n    13\n1\n1\n?\n1\n0.75\n1.33\n    14\n1\n1\n?\n1\n0.75\n1.33\n    15\n1\n1\n?\n1\n0.75\n1.33\n    16\n1\n1\n?\n1\n0.75\n1.33\n    17\n1\n1\n?\n1\n0.75\n1.33\n    18\n1\n1\n?\n0\n0.75\n1.33\n    19\n1\n1\n?\n0\n0.75\n1.33\n    20\n1\n1\n?\n0\n0.75\n1.33\n  \n  \n  \n\n\n\n\n\n\n統制群におけるWの和: 20\n処置群におけるWの和: 20\n\\(\\rightarrow\\) 各群におけるWの和はサンプルサイズと一致する\n\\(\\rightarrow\\) 全サンプルが統制/処置群の場合の結果変数の期待値を計算（加重平均）"
  },
  {
    "objectID": "slide/matching.html#他の考え方-3",
    "href": "slide/matching.html#他の考え方-3",
    "title": "方法論特殊講義III",
    "section": "他の考え方",
    "text": "他の考え方\n\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Zi\n      Ti\n      Y0, i\n      Y1, i\n      ei\n      Wi\n    \n  \n  \n    1\n0\n0\n0\n?\n0.50\n2.00\n    2\n0\n0\n1\n?\n0.50\n2.00\n    3\n0\n0\n0\n?\n0.50\n2.00\n    4\n0\n0\n0\n?\n0.50\n2.00\n    5\n0\n1\n?\n0\n0.50\n2.00\n    6\n0\n1\n?\n0\n0.50\n2.00\n    7\n0\n1\n?\n0\n0.50\n2.00\n    8\n0\n1\n?\n1\n0.50\n2.00\n    9\n1\n0\n1\n?\n0.75\n4.00\n    10\n1\n0\n1\n?\n0.75\n4.00\n    11\n1\n0\n0\n?\n0.75\n4.00\n    12\n1\n1\n?\n1\n0.75\n1.33\n    13\n1\n1\n?\n1\n0.75\n1.33\n    14\n1\n1\n?\n1\n0.75\n1.33\n    15\n1\n1\n?\n1\n0.75\n1.33\n    16\n1\n1\n?\n1\n0.75\n1.33\n    17\n1\n1\n?\n1\n0.75\n1.33\n    18\n1\n1\n?\n0\n0.75\n1.33\n    19\n1\n1\n?\n0\n0.75\n1.33\n    20\n1\n1\n?\n0\n0.75\n1.33\n  \n  \n  \n\n\n\n\n\n\n統制群の加重平均\n\n\\(0 \\cdot 2 + 1 \\cdot 2 + 0 \\cdot 2 + 0 \\cdot 2 + \\dots + 0 \\cdot 4\\)\n\\(\\mathbb{E}^w[Y_0] = 10\\)\n\n処置群の加重平均\n\n\\(0 \\cdot 2 + 0 \\cdot 2 + 0 \\cdot 2 + 1 \\cdot 2 + \\dots + 0 \\cdot 1.33\\)\n\\(\\mathbb{E}^w[Y_1] = 10\\)\n\n\n\n\\[\\mathbb{E}^w[Y_1] - \\mathbb{E}^w[Y_0] = 0\\]"
  },
  {
    "objectID": "slide/matching.html#共変量の選択-1",
    "href": "slide/matching.html#共変量の選択-1",
    "title": "方法論特殊講義III",
    "section": "共変量の選択",
    "text": "共変量の選択\n共変量選択の基準は (星野 2009; Imbens and Rubin 2015など)\n\n処置変数と結果変数、両方と連関があること\n\nOVBと関係\n\n処置前変数と処置後変数の区別\n\n処置変数に時間的に先行しているか否か\n\n処置前変数 (pre-treatment variable) は必ず投入する\n処置後変数 (post-treatment variable) は目的による\n\nというものの、基本的に投入しない\n応答変数よりも時間的に後なら絶対に投入しない"
  },
  {
    "objectID": "slide/matching.html#共変量の選択-2",
    "href": "slide/matching.html#共変量の選択-2",
    "title": "方法論特殊講義III",
    "section": "共変量の選択",
    "text": "共変量の選択\nVanderWeele (2019) のmodified disjunctive cause criterion\n\n処置変数と応答変数どちらかの原因となる変数\n処置変数と応答変数両方の原因となる変数\n操作変数は共変量として投入しない\n上記の基準を満たさない場合でも、観察されていない共変量の代理変数は統制しても良い\n\nしかし、慎重に選択しないとバイアスが拡大\n2.の該当する変数の代理変数が望ましい\n\n\n\n\n詳細はhttps://www.slideshare.net/tintstyle/ss-141543274を参照"
  },
  {
    "objectID": "slide/matching.html#ダイアグラムを使った例",
    "href": "slide/matching.html#ダイアグラムを使った例",
    "title": "方法論特殊講義III",
    "section": "ダイアグラムを使った例",
    "text": "ダイアグラムを使った例\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(T \\rightarrow Y\\)の効果は11\n\\(Z\\): \\(T\\)と\\(Y\\)の原因 \\(\\leftarrow\\) 投入\n\\(A\\): 操作変数 \\(\\leftarrow\\) 除外\n\\(X\\): \\(Y\\)の原因 \\(\\leftarrow\\) 投入\n\\(V\\): \\(T\\)の結果 \\(\\leftarrow\\) 除外\n\\(W\\)は…?"
  },
  {
    "objectID": "slide/matching.html#ダイアグラムを使った例-1",
    "href": "slide/matching.html#ダイアグラムを使った例-1",
    "title": "方法論特殊講義III",
    "section": "ダイアグラムを使った例",
    "text": "ダイアグラムを使った例\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(T \\rightarrow Y\\): 直接効果\n\\(T \\rightarrow W \\rightarrow Y\\): 間接効果\n\n\\(W\\)は中間変数（mediate variable）\n\n因果推論では主に全効果 (total effect) に関心があるため\\(W\\)は投入しない\n\n全効果: 直接効果 + 間接効果\n\\(T\\)が変動したら\\(W\\)も必ず変わるため、\\(T\\)のみの効果はあまり意味なし\n直接効果のみ推定する場合、\\(W\\)も統制\n\n結論: \\(Z\\)と\\(X\\)のみ統制\n\n実は\\(X\\)は入れなくてもOK"
  },
  {
    "objectID": "slide/matching.html#ダイアグラムのツール",
    "href": "slide/matching.html#ダイアグラムのツール",
    "title": "方法論特殊講義III",
    "section": "ダイアグラムのツール",
    "text": "ダイアグラムのツール\nDAGitty — draw and analyze causal diagrams\n\nウェーブページ or Rパッケージ{dagitty}\n\nhttp://www.dagitty.net/\n\n\n\n\n\npacman::p_load(ggdag)\nDAG1 <- dagitty(\n  \"dag {\n  T -> Y\n  T -> W -> Y\n  T <- Z -> Y\n  A -> T\n  X -> Y\n  T -> V\n  }\"\n)\n\n\n\n# Total Effect推定のための共変量\nadjustmentSets(DAG1, \n               exposure = \"T\", outcome = \"Y\",\n               effect = \"total\")\n\n{ Z }\n\n# Direct Effect推定のための共変量\nadjustmentSets(DAG1, \n               exposure = \"T\", outcome = \"Y\",\n               effect = \"direct\")\n\n{ W, Z }"
  },
  {
    "objectID": "slide/matching.html#ダイアグラムのツール-1",
    "href": "slide/matching.html#ダイアグラムのツール-1",
    "title": "方法論特殊講義III",
    "section": "ダイアグラムのツール",
    "text": "ダイアグラムのツール\n{ggdag}を用いた可視化\n\n詳細は『私たちのR』第20章を参照\n\n\n\n\ncoordinates(DAG1) <- list(\n  x = c(T = 1, Y = 5, Z = 3, W = 3, \n        A = 2, V = 2, X = 4),\n  y = c(T = 2, Y = 2, Z = 3, W = 4, \n        A = 1, V = 4, X = 1)\n)\nggdag(DAG1, stylized = TRUE) +\n  theme_dag_blank()"
  },
  {
    "objectID": "slide/matching.html#利用データ",
    "href": "slide/matching.html#利用データ",
    "title": "方法論特殊講義III",
    "section": "利用データ",
    "text": "利用データ\n定番のlalondeデータセット: 職業訓練の有無と所得 * data(\"lalonde\", package = \"cobalt\")で読み込み * その前に{cobalt}パッケージを読み込む\n\n\n\n\n変数名\n説明\n\n\n\n\n1\ntreat\n職業訓練の履修有無 (Treatment)\n\n\n2\nage\n年齢\n\n\n3\neduc\n教育年数\n\n\n4\nrace\n人種(白人、黒人、ヒスパニック)\n\n\n5\nmarried\n結婚有無\n\n\n6\nnodegree\n学位有無\n\n\n7\nre74\n1974 年の所得\n\n\n8\nre75\n1975 年の所得\n\n\n9\nre78\n1978 年の所得 (Outcome)"
  },
  {
    "objectID": "slide/matching.html#matching-手法",
    "href": "slide/matching.html#matching-手法",
    "title": "方法論特殊講義III",
    "section": "Matching 手法",
    "text": "Matching 手法\nExact Matching は現実的に不可能であるため省略\n\nMahalanobis Matching\nCoarsened Exact Matching\nPropensity Score Matching\nInverse Probability Weighting\n\n\n使用パッケージ * 1〜3: {MatchIt} * 事前に{cem}パッケージをインストールしておく * 4: {WeightIt} * バランスチェック: {cobalt}"
  },
  {
    "objectID": "slide/matching.html#バランスチェック",
    "href": "slide/matching.html#バランスチェック",
    "title": "方法論特殊講義III",
    "section": "バランスチェック",
    "text": "バランスチェック\n{MatchIt}パッケージが提供する方法\n\nRCTの講義で紹介したように、標準化差分を利用\n\n{MatchIt}のsummary(..., standardize = TRUE)を利用\n{cobalt}、{BalanceR}パッケージなど\n\nQQ Plot\n\n統制—処置群の共変量の分布に違いがあるか否か\n\nHistogram\n\nマッチング前後ごとに共変量のヒストグラム\n\nJitter Plot\n\nヒストグラムと似ているが棒の高さより密度で判断"
  },
  {
    "objectID": "slide/matching.html#qq-plotの出し方",
    "href": "slide/matching.html#qq-plotの出し方",
    "title": "方法論特殊講義III",
    "section": "QQ Plotの出し方",
    "text": "QQ Plotの出し方\nplot(マッチング後のオブジェクト名)で出力\n\n点が45度線上に位置する場合、バランス\n\n.pull-left[] .pull-right[]"
  },
  {
    "objectID": "slide/matching.html#qq-plotとは",
    "href": "slide/matching.html#qq-plotとは",
    "title": "方法論特殊講義III",
    "section": "QQ Plotとは",
    "text": "QQ Plotとは\nQuantile–Quantile Plot\n\n(簡単にいうと)2つの変数を小さい値から大きい値の順に並び替え、 散布図を作成し、45度線を引いたもの\nもし2つのデータが完全に同じ分布をしているのなら、全ての点は 45 度線上に位置\n長所\n\n分布の形まで比較可能\n平均値と分散のみに依存するため、サンプルサイズに鈍感\n\n短所\n\n離散変数においては使いにくい"
  },
  {
    "objectID": "slide/matching.html#同じ平均値分散相関異なる分布",
    "href": "slide/matching.html#同じ平均値分散相関異なる分布",
    "title": "方法論特殊講義III",
    "section": "同じ平均値・分散・相関、異なる分布",
    "text": "同じ平均値・分散・相関、異なる分布\nAlberto CairoのDatasaurus dataset\n\n以下の2つの散布図のXとYは平均値、分散、相関係数が同じ\n\n記述統計量だけでなく、可視化も必要\n\n\n.pull-left[] .pull-left[]"
  },
  {
    "objectID": "slide/matching.html#qq-plotの例-1",
    "href": "slide/matching.html#qq-plotの例-1",
    "title": "方法論特殊講義III",
    "section": "QQ Plotの例 (1)",
    "text": "QQ Plotの例 (1)\nもし2つのデータが完全に同じなら…\n\nX: -5, -4, -2, -1, 0, 1, 2, 3, 4, 5\nY: -5, -4, -2, -1, 0, 1, 2, 3, 4, 5"
  },
  {
    "objectID": "slide/matching.html#qq-plotの例-2",
    "href": "slide/matching.html#qq-plotの例-2",
    "title": "方法論特殊講義III",
    "section": "QQ Plotの例 (2)",
    "text": "QQ Plotの例 (2)\nもし2つのデータが同じ分布から生成されたなら… * \\(X \\sim \\text{Normal}(5, 2)\\) * \\(Y \\sim \\text{Normal}(5, 2)\\)"
  },
  {
    "objectID": "slide/matching.html#qq-plotの例-3",
    "href": "slide/matching.html#qq-plotの例-3",
    "title": "方法論特殊講義III",
    "section": "QQ Plotの例 (3)",
    "text": "QQ Plotの例 (3)\nもし2つのデータが分散が異なる分布から生成されたなら… * \\(X \\sim \\text{Normal}(5, 1)\\) * \\(Y \\sim \\text{Normal}(5, 2)\\)"
  },
  {
    "objectID": "slide/matching.html#qq-plotの例-4",
    "href": "slide/matching.html#qq-plotの例-4",
    "title": "方法論特殊講義III",
    "section": "QQ Plotの例 (4)",
    "text": "QQ Plotの例 (4)\nもし2つのデータが平均が異なる分布から生成されたなら * \\(X \\sim \\text{Normal}(2, 2)\\) * \\(Y \\sim \\text{Normal}(5, 2)\\)"
  },
  {
    "objectID": "slide/matching.html#qq-plotの例-5",
    "href": "slide/matching.html#qq-plotの例-5",
    "title": "方法論特殊講義III",
    "section": "QQ Plotの例 (5)",
    "text": "QQ Plotの例 (5)\nもし2つのデータが平均と分散が異なる分布から生成されたなら… * \\(X \\sim \\text{Normal}(2, 2)\\) * \\(Y \\sim \\text{Normal}(5, 5)\\)"
  },
  {
    "objectID": "slide/matching.html#jitter-plot",
    "href": "slide/matching.html#jitter-plot",
    "title": "方法論特殊講義III",
    "section": "Jitter Plot",
    "text": "Jitter Plot\n傾向スコアの分布を確認する\n\nplot(マッチング・オブジェクト名, type = \"jitter\")で出力"
  },
  {
    "objectID": "slide/matching.html#cobaltの利用-1",
    "href": "slide/matching.html#cobaltの利用-1",
    "title": "方法論特殊講義III",
    "section": "{cobalt}の利用 (1)",
    "text": "{cobalt}の利用 (1)\n傾向スコアのヒストグラム: cobalt::bal.plot() * ヒストグラムが上下対称ならバランス"
  },
  {
    "objectID": "slide/matching.html#cobaltの利用-2",
    "href": "slide/matching.html#cobaltの利用-2",
    "title": "方法論特殊講義III",
    "section": "{cobalt}の利用 (2)",
    "text": "{cobalt}の利用 (2)\nラブ・プロット: cobalt::love.plot()\n\n標準化差分によるバランス・チェック"
  },
  {
    "objectID": "slide/matching.html#真の因果効果は",
    "href": "slide/matching.html#真の因果効果は",
    "title": "方法論特殊講義III",
    "section": "真の因果効果は?",
    "text": "真の因果効果は?\n手法ごとに異なる処置効果 * .kenten[真の]因果効果が分からないため、どの推定値が正しいかは分からない * 最近傍マッチングはランダム要素があるため、分析の度に変化 * 様々な手法で推定値、または傾向が安定しているかを確認\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://www.jaysong.net/kobe-ci"
  },
  {
    "objectID": "slide/matching.html#att-処置群における平均処置効果",
    "href": "slide/matching.html#att-処置群における平均処置効果",
    "title": "方法論特殊講義III",
    "section": "ATT: 処置群における平均処置効果",
    "text": "ATT: 処置群における平均処置効果\n\n処置群の潜在的結果を統制群から割り当てる。\n処置群は \\(Y_i(T_i = 1)\\) が観察済みであり、潜在的結果は \\(Y_i(T_i = 0)\\)\nやる気のない学生の \\(Y_i(T_i = 0)\\) は193.5、ある学生は394.2\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Yarukii\n      Yi(Ti = 1)\n      Yi(Ti = 0)\n      ITEi\n    \n  \n  \n    1\n0\n659\n193.5\n465.5\n    2\n1\n587\n394.2\n192.8\n    3\n1\n628\n394.2\n233.8\n    4\n1\n563\n394.2\n168.8\n    5\n1\n531\n394.2\n136.8\n    7\n0\n356\n193.5\n162.5\n    ...\n...\n...\n...\n...\n    20\n1\n648\n394.2\n253.8\n    22\n1\n768\n394.2\n373.8\n    26\n1\n408\n394.2\n13.8\n    28\n1\n516\n394.2\n121.8\n    平均\n\n\n\n185.1"
  },
  {
    "objectID": "slide/matching.html#atc-統制群における平均処置効果",
    "href": "slide/matching.html#atc-統制群における平均処置効果",
    "title": "方法論特殊講義III",
    "section": "ATC: 統制群における平均処置効果",
    "text": "ATC: 統制群における平均処置効果\n\n統制群の潜在的結果を処置群から割り当てる。\n統制群は \\(Y_i(T_i = 0)\\) が観察済みであり、潜在的結果は \\(Y_i(T_i = 1)\\)\nやる気のない学生の \\(Y_i(T_i = 1)\\) は402.5、ある学生は570.5\n\n\n\n\n\n\n\n  \n  \n    \n      ID (i)\n      Yarukii\n      Yi(Ti = 1)\n      Yi(Ti = 0)\n      ITEi\n    \n  \n  \n    6\n0\n402.5\n79\n323.5\n    8\n0\n402.5\n176\n226.5\n    9\n0\n402.5\n339\n63.5\n    11\n0\n402.5\n239\n163.5\n    12\n1\n570.5\n276\n294.5\n    14\n0\n402.5\n254\n148.5\n    ...\n...\n...\n...\n...\n    25\n0\n402.5\n304\n98.5\n    27\n0\n402.5\n259\n143.5\n    29\n1\n570.5\n476\n94.5\n    30\n0\n402.5\n110\n292.5\n    平均\n\n\n\n185.1"
  },
  {
    "objectID": "slide/matching.html#ate-平均処置効果",
    "href": "slide/matching.html#ate-平均処置効果",
    "title": "方法論特殊講義III",
    "section": "ATE: 平均処置効果",
    "text": "ATE: 平均処置効果\n\nATTとATCの加重平均\n今回は処置群と統制群が15:15 \\(\\rightarrow\\) 単純平均でOK\n\n\\(\\frac{1}{2}(185.1 + 198.1) = 191.6\\)\n手計算マッチングとと同じ結果\n\n\n\\[\\text{ATE} = \\frac{N_{\\text{treated}}}{N_{\\text{all}}} \\text{ATT} + \\frac{N_{\\text{controlled}}}{N_{\\text{all}}} \\text{ATC}.\\]"
  },
  {
    "objectID": "slide/matching.html#マンハッタン距離manhattan-distance-city-block-distance",
    "href": "slide/matching.html#マンハッタン距離manhattan-distance-city-block-distance",
    "title": "方法論特殊講義III",
    "section": "マンハッタン距離（Manhattan Distance; City-block Distance）",
    "text": "マンハッタン距離（Manhattan Distance; City-block Distance）\n\\[d(i, j) = |X_i - X_j| + |Y_i - Y_j| \\text{ where } i \\neq j.\\]"
  },
  {
    "objectID": "slide/matching.html#標準化ユークリッド距離standardized-euclidean-distance",
    "href": "slide/matching.html#標準化ユークリッド距離standardized-euclidean-distance",
    "title": "方法論特殊講義III",
    "section": "標準化ユークリッド距離（Standardized Euclidean Distance）",
    "text": "標準化ユークリッド距離（Standardized Euclidean Distance）\n\\[d(i, j) = \\sqrt{\\Bigg(\\frac{X_i - X_j}{\\sigma_X}\\Bigg)^2 + \\Bigg(\\frac{Y_i - Y_j}{\\sigma_Y}\\Bigg)^2} \\text{ where } i \\neq j.\\]"
  },
  {
    "objectID": "slide/matching.html#マハラノビス距離mahalanobis-distance",
    "href": "slide/matching.html#マハラノビス距離mahalanobis-distance",
    "title": "方法論特殊講義III",
    "section": "マハラノビス距離（Mahalanobis Distance）",
    "text": "マハラノビス距離（Mahalanobis Distance）\n\n共変量間の相関が0（\\(\\rho = 0\\)）の場合、Standardized Euclidean Distanceと同じ\n\n\\[d(i, j) = \\sqrt{\\frac{1}{1 - \\rho^2_{X, Y}} \\Bigg[\\Bigg(\\frac{X_i - X_j}{\\sigma_X}\\Bigg)^2 + \\Bigg(\\frac{Y_i - Y_j}{\\sigma_Y}\\Bigg)^2 - 2\\rho_{X,Y}\\Bigg(\\frac{X_i - X_j}{\\sigma_X}\\Bigg) \\Bigg(\\frac{Y_i - Y_j}{\\sigma_Y}\\Bigg)\\Bigg]} \\text{ where } i \\neq j.\\]"
  },
  {
    "objectID": "slide/matching.html#cemの例",
    "href": "slide/matching.html#cemの例",
    "title": "方法論特殊講義III",
    "section": "CEMの例",
    "text": "CEMの例\nmatching_data2.csvの例\n\n年齢は10歳刻み、学歴は大卒以上・未満に層化\n\n\n\n\n\n\n\n  \n  \n    \n      ID\n      年齢\n      教育\n      処置\n      結果\n      　\n      ID\n      年齢\n      教育\n      処置\n      結果\n    \n  \n  \n    1\n29\n院\n0\n6\n\n13\n57\n高\n0\n4\n    2\n41\n大\n0\n3\n\n14\n25\n院\n1\n5\n    3\n31\n院\n1\n7\n\n15\n55\n中\n1\n9\n    4\n39\n院\n0\n5\n\n16\n48\n院\n0\n2\n    5\n53\n大\n0\n6\n\n17\n23\n専\n0\n2\n    6\n59\n大\n0\n1\n\n18\n34\n大\n1\n4\n    7\n37\n高\n1\n8\n\n19\n42\n大\n1\n9\n    8\n44\n中\n0\n4\n\n20\n23\n高\n0\n4\n    9\n51\n中\n0\n2\n\n21\n22\n高\n1\n8\n    10\n59\n小\n1\n8\n\n22\n49\n大\n0\n9\n    11\n21\n大\n1\n4\n\n23\n45\n高\n1\n6\n    12\n24\n中\n1\n6\n\n24\n33\n大\n0\n8"
  },
  {
    "objectID": "slide/matching.html#cemの例-1",
    "href": "slide/matching.html#cemの例-1",
    "title": "方法論特殊講義III",
    "section": "CEMの例",
    "text": "CEMの例\n年齢は10歳刻み、学歴は大卒以上・未満に層化\n\nカテゴリが少なくなり、マッチングしやすくなる\n\n\n\n\n\n\n\n  \n  \n    \n      ID\n      年齢\n      教育\n      処置\n      結果\n      　\n      ID\n      年齢\n      教育\n      処置\n      結果\n    \n  \n  \n    1\n20代\nH\n0\n6\n\n13\n50代\nL\n0\n4\n    2\n40代\nH\n0\n3\n\n14\n20代\nH\n1\n5\n    3\n30代\nH\n1\n7\n\n15\n50代\nL\n1\n9\n    4\n30代\nH\n0\n5\n\n16\n40代\nH\n0\n2\n    5\n50代\nH\n0\n6\n\n17\n20代\nL\n0\n2\n    6\n50代\nH\n0\n1\n\n18\n30代\nH\n1\n4\n    7\n30代\nL\n1\n8\n\n19\n40代\nH\n1\n9\n    8\n40代\nL\n0\n4\n\n20\n20代\nL\n0\n4\n    9\n50代\nL\n0\n2\n\n21\n20代\nL\n1\n8\n    10\n50代\nL\n1\n8\n\n22\n40代\nH\n0\n9\n    11\n20代\nH\n1\n4\n\n23\n40代\nL\n1\n6\n    12\n20代\nL\n1\n6\n\n24\n30代\nH\n0\n8"
  },
  {
    "objectID": "slide/matching.html#cemの例-2",
    "href": "slide/matching.html#cemの例-2",
    "title": "方法論特殊講義III",
    "section": "CEMの例",
    "text": "CEMの例\n\n\n層ごとにケースをマッチング\n\nペアが組めない層が存在\n\n30代 & L\n50代 & H\n\n\n\n\n\n\n\n\n\n  \n  \n    \n      年齢\n      教育\n      \n        処置群\n      \n      \n        統制群\n      \n    \n    \n      ID\n      処置\n      結果\n      ID\n      処置\n      結果\n    \n  \n  \n    20代\nH\n11\n1\n5\n1\n0\n6\n    20代\nH\n14\n1\n5\n\n\n\n    20代\nL\n12\n1\n6\n17\n0\n2\n    20代\nL\n21\n1\n8\n20\n0\n4\n    30代\nH\n3\n1\n7\n4\n0\n5\n    30代\nH\n18\n1\n4\n24\n0\n8\n    30代\nL\n7\n1\n8\n\n\n\n    40代\nH\n19\n1\n9\n2\n0\n3\n    40代\nH\n\n\n\n16\n0\n2\n    40代\nH\n\n\n\n22\n0\n9\n    40代\nL\n23\n1\n6\n8\n0\n4\n    50代\nH\n\n\n\n5\n0\n6\n    50代\nH\n\n\n\n6\n0\n1\n    50代\nL\n10\n1\n8\n9\n0\n2\n    50代\nL\n15\n1\n9\n13\n0\n4"
  },
  {
    "objectID": "slide/matching.html#cemの例-3",
    "href": "slide/matching.html#cemの例-3",
    "title": "方法論特殊講義III",
    "section": "CEMの例",
    "text": "CEMの例\nペアが組めない層を除外（30代Lと50代H）\n\n\n\n\n\n\n  \n  \n    \n      年齢\n      教育\n      \n        処置群\n      \n      \n        統制群\n      \n    \n    \n      ID\n      処置\n      結果\n      ID\n      処置\n      結果\n    \n  \n  \n    20代\nH\n11\n1\n5\n1\n0\n6\n    20代\nH\n14\n1\n5\n\n\n\n    20代\nL\n12\n1\n6\n17\n0\n2\n    20代\nL\n21\n1\n8\n20\n0\n4\n    30代\nH\n3\n1\n7\n4\n0\n5\n    30代\nH\n18\n1\n4\n24\n0\n8\n    40代\nH\n19\n1\n9\n2\n0\n3\n    40代\nH\n\n\n\n16\n0\n2\n    40代\nH\n\n\n\n22\n0\n9\n    40代\nL\n23\n1\n6\n8\n0\n4\n    50代\nL\n10\n1\n8\n9\n0\n2\n    50代\nL\n15\n1\n9\n13\n0\n4"
  },
  {
    "objectID": "slide/matching.html#cemの例-4",
    "href": "slide/matching.html#cemの例-4",
    "title": "方法論特殊講義III",
    "section": "CEMの例",
    "text": "CEMの例\n各ユニットの重みを計算\n\n\n\\[w_i = \\begin{cases} 1 & \\text{ if } T_i = 1, \\\\ \\frac{m_C}{m_T} \\cdot \\frac{m^s_T}{m^s_C} & \\text{ if } T_i = 0.\\end{cases}\\]\n\n\\(m_{C,T}\\): 統制・処置ケースの数\n\nペアを組めなかったケースはカウントしない\n\n\\(m^s_{C,T}\\): 層 \\(s\\) 内の統制・処置ケースの数\n\n\n\n\n\n\n\n\n  \n  \n    \n      年齢\n      教育\n      \n        処置群\n      \n      \n        統制群\n      \n    \n    \n      ID\n      処置\n      結果\n      重み\n      ID\n      処置\n      結果\n      重み\n    \n  \n  \n    20代\nH\n11\n1\n5\n1\n1\n0\n6\n2.2\n    20代\nH\n14\n1\n5\n1\n\n\n\n\n    20代\nL\n12\n1\n6\n1\n17\n0\n2\n1.1\n    20代\nL\n21\n1\n8\n1\n20\n0\n4\n1.1\n    30代\nH\n3\n1\n7\n1\n4\n0\n5\n1.1\n    30代\nH\n18\n1\n4\n1\n24\n0\n8\n1.1\n    40代\nH\n19\n1\n9\n1\n2\n0\n3\n0.367\n    40代\nH\n\n\n\n\n16\n0\n2\n0.367\n    40代\nH\n\n\n\n\n22\n0\n9\n0.367\n    40代\nL\n23\n1\n6\n1\n8\n0\n4\n1.1\n    50代\nL\n10\n1\n8\n1\n9\n0\n2\n1.1\n    50代\nL\n15\n1\n9\n1\n13\n0\n4\n1.1"
  },
  {
    "objectID": "slide/matching.html#cemの例-5",
    "href": "slide/matching.html#cemの例-5",
    "title": "方法論特殊講義III",
    "section": "CEMの例",
    "text": "CEMの例\n重み付け回帰分析\n\n\\(W = \\{2.200, 0.367, 1.000, 1.100, 0.000, 0.000, ..., 1.100\\}^{\\top}\\)\n\nマッチングされないケースの重みは0にするか、分析から除外\n\\(\\beta = (X^{\\top}WX)^{-1}X^{\\top}WY\\)\n\nRの場合、lm(formula, data, weight = ...)で推定可能\n\n{cem} or {MatchIt}パッケージならもっと簡単\n\n\\(\\widehat{\\text{Outcome}} = 4.567 + 2.033 \\cdot \\text{Treat}\\)\n\n処置群における因果効果 (ATT) = 2.033"
  },
  {
    "objectID": "slide/matching.html#実習内容",
    "href": "slide/matching.html#実習内容",
    "title": "方法論特殊講義III",
    "section": "実習内容",
    "text": "実習内容\n\nマッチングの実装: {MatchIt}、{WeightIt}パッケージ\n\n線形回帰分析\n最近傍マッチング（Mahalanobis Matching）\nCEM\n傾向スコアマッチング\nIPW\n\nバランスチェック: {cobalt}パッケージ\n\n標準化差分\nヒストグラム\nQQ Plotなど\n\nマッチング手法間の比較"
  },
  {
    "objectID": "slide/matching.html#実習用データ",
    "href": "slide/matching.html#実習用データ",
    "title": "方法論特殊講義III",
    "section": "実習用データ",
    "text": "実習用データ\n定番のlalondeデータセット: 職業訓練の有無と所得\n\n\n\ndata(\"lalonde\", package = \"cobalt\")で読み込み\n\nその前に{cobalt}パッケージを読み込む\nlalondeデータは様々なパッケージがサンプルデータとして提供しているが、本講義では{cobalt}パッケージのlalondeデータセットを利用する。\n\n\n\n\n    treat age educ   race married nodegree        re74        re75        re78\n1       1  37   11  black       1        1     0.00000     0.00000  9930.04600\n2       1  22    9 hispan       0        1     0.00000     0.00000  3595.89400\n3       1  30   12  black       0        0     0.00000     0.00000 24909.45000\n4       1  27   11  black       0        1     0.00000     0.00000  7506.14600\n5       1  33    8  black       0        1     0.00000     0.00000   289.78990\n6       1  22    9  black       0        1     0.00000     0.00000  4056.49400\n7       1  23   12  black       0        0     0.00000     0.00000     0.00000\n8       1  32   11  black       0        1     0.00000     0.00000  8472.15800\n9       1  22   16  black       0        0     0.00000     0.00000  2164.02200\n10      1  33   12  white       1        0     0.00000     0.00000 12418.07000\n11      1  19    9  black       0        1     0.00000     0.00000  8173.90800\n12      1  21   13  black       0        0     0.00000     0.00000 17094.64000\n13      1  18    8  black       0        1     0.00000     0.00000     0.00000\n14      1  27   10  black       1        1     0.00000     0.00000 18739.93000\n15      1  17    7  black       0        1     0.00000     0.00000  3023.87900\n16      1  19   10  black       0        1     0.00000     0.00000  3228.50300\n17      1  27   13  black       0        0     0.00000     0.00000 14581.86000\n18      1  23   10  black       0        1     0.00000     0.00000  7693.40000\n19      1  40   12  black       0        0     0.00000     0.00000 10804.32000\n20      1  26   12  black       0        0     0.00000     0.00000 10747.35000\n21      1  23   11  black       0        1     0.00000     0.00000     0.00000\n22      1  41   14  white       0        0     0.00000     0.00000  5149.50100\n23      1  38    9  white       0        1     0.00000     0.00000  6408.95000\n24      1  24   11  black       0        1     0.00000     0.00000  1991.40000\n25      1  18   10  black       0        1     0.00000     0.00000 11163.17000\n26      1  29   11  black       1        1     0.00000     0.00000  9642.99900\n27      1  25   11  black       0        1     0.00000     0.00000  9897.04900\n28      1  27   10 hispan       0        1     0.00000     0.00000 11142.87000\n29      1  17   10  black       0        1     0.00000     0.00000 16218.04000\n30      1  24   11  black       0        1     0.00000     0.00000   995.70020\n31      1  17   10  black       0        1     0.00000     0.00000     0.00000\n32      1  48    4  black       0        1     0.00000     0.00000  6551.59200\n33      1  25   11  black       1        1     0.00000     0.00000  1574.42400\n34      1  20   12  black       0        0     0.00000     0.00000     0.00000\n35      1  25   12  black       0        0     0.00000     0.00000  3191.75300\n36      1  42   14  black       0        0     0.00000     0.00000 20505.93000\n37      1  25    5  black       0        1     0.00000     0.00000  6181.88000\n38      1  23   12  black       1        0     0.00000     0.00000  5911.55100\n39      1  46    8  black       1        1     0.00000     0.00000  3094.15600\n40      1  24   10  black       0        1     0.00000     0.00000     0.00000\n41      1  21   12  black       0        0     0.00000     0.00000  1254.58200\n42      1  19    9  white       0        1     0.00000     0.00000 13188.83000\n43      1  17    8  black       0        1     0.00000     0.00000  8061.48500\n44      1  18    8 hispan       1        1     0.00000     0.00000  2787.96000\n45      1  20   11  black       0        1     0.00000     0.00000  3972.54000\n46      1  25   11  black       1        1     0.00000     0.00000     0.00000\n47      1  17    8  black       0        1     0.00000     0.00000     0.00000\n48      1  17    9  black       0        1     0.00000     0.00000     0.00000\n49      1  25    5  black       0        1     0.00000     0.00000 12187.41000\n50      1  23   12  black       0        0     0.00000     0.00000  4843.17600\n51      1  28    8  black       0        1     0.00000     0.00000     0.00000\n52      1  31   11  black       1        1     0.00000     0.00000  8087.48700\n53      1  18   11  black       0        1     0.00000     0.00000     0.00000\n54      1  25   12  black       0        0     0.00000     0.00000  2348.97300\n55      1  30   11  black       1        1     0.00000     0.00000   590.78180\n56      1  17   10  black       0        1     0.00000     0.00000     0.00000\n57      1  37    9  black       0        1     0.00000     0.00000  1067.50600\n58      1  41    4  black       1        1     0.00000     0.00000  7284.98600\n59      1  42   14  black       1        0     0.00000     0.00000 13167.52000\n60      1  22   11  white       0        1     0.00000     0.00000  1048.43200\n61      1  17    8  black       0        1     0.00000     0.00000     0.00000\n62      1  29    8  black       0        1     0.00000     0.00000  1923.93800\n63      1  35   10  black       0        1     0.00000     0.00000  4666.23600\n64      1  27   11  black       0        1     0.00000     0.00000   549.29840\n65      1  29    4  black       0        1     0.00000     0.00000   762.91460\n66      1  28    9  black       0        1     0.00000     0.00000 10694.29000\n67      1  27   11  black       0        1     0.00000     0.00000     0.00000\n68      1  23    7  white       0        1     0.00000     0.00000     0.00000\n69      1  45    5  black       1        1     0.00000     0.00000  8546.71500\n70      1  29   13  black       0        0     0.00000     0.00000  7479.65600\n71      1  27    9  black       0        1     0.00000     0.00000     0.00000\n72      1  46   13  black       0        0     0.00000     0.00000   647.20460\n73      1  18    6  black       0        1     0.00000     0.00000     0.00000\n74      1  25   12  black       0        0     0.00000     0.00000 11965.81000\n75      1  28   15  black       0        0     0.00000     0.00000  9598.54100\n76      1  25   11  white       0        1     0.00000     0.00000 18783.35000\n77      1  22   12  black       0        0     0.00000     0.00000 18678.08000\n78      1  21    9  black       0        1     0.00000     0.00000     0.00000\n79      1  40   11  black       0        1     0.00000     0.00000 23005.60000\n80      1  22   11  black       0        1     0.00000     0.00000  6456.69700\n81      1  25   12  black       0        0     0.00000     0.00000     0.00000\n82      1  18   12  black       0        0     0.00000     0.00000  2321.10700\n83      1  38   12  white       0        0     0.00000     0.00000  4941.84900\n84      1  27   13  black       0        0     0.00000     0.00000     0.00000\n85      1  27    8  black       0        1     0.00000     0.00000     0.00000\n86      1  38   11  black       0        1     0.00000     0.00000     0.00000\n87      1  23    8 hispan       0        1     0.00000     0.00000  3881.28400\n88      1  26   11  black       0        1     0.00000     0.00000 17230.96000\n89      1  21   12  white       0        0     0.00000     0.00000  8048.60300\n90      1  25    8  black       0        1     0.00000     0.00000     0.00000\n91      1  31   11  black       1        1     0.00000     0.00000 14509.93000\n92      1  17   10  black       0        1     0.00000     0.00000     0.00000\n93      1  25   11  black       0        1     0.00000     0.00000     0.00000\n94      1  21   12  black       0        0     0.00000     0.00000  9983.78400\n95      1  44   11  black       0        1     0.00000     0.00000     0.00000\n96      1  25   12  white       0        0     0.00000     0.00000  5587.50300\n97      1  18    9  black       0        1     0.00000     0.00000  4482.84500\n98      1  42   12  black       0        0     0.00000     0.00000  2456.15300\n99      1  25   10  black       0        1     0.00000     0.00000     0.00000\n100     1  31    9 hispan       0        1     0.00000     0.00000 26817.60000\n101     1  24   10  black       0        1     0.00000     0.00000     0.00000\n102     1  26   10  black       0        1     0.00000     0.00000  9265.78800\n103     1  25   11  black       0        1     0.00000     0.00000   485.22980\n104     1  18   11  black       0        1     0.00000     0.00000  4814.62700\n105     1  19   11  black       0        1     0.00000     0.00000  7458.10500\n106     1  43    9  black       0        1     0.00000     0.00000     0.00000\n107     1  27   13  black       0        0     0.00000     0.00000 34099.28000\n108     1  17    9  black       0        1     0.00000     0.00000  1953.26800\n109     1  30   11  black       0        1     0.00000     0.00000     0.00000\n110     1  26   10  black       1        1  2027.99900     0.00000     0.00000\n111     1  20    9  black       0        1  6083.99400     0.00000  8881.66500\n112     1  17    9 hispan       0        1   445.17040    74.34345  6210.67000\n113     1  20   12  black       0        0   989.26780   165.20770     0.00000\n114     1  18   11  black       0        1   858.25430   214.56360   929.88390\n115     1  27   12  black       1        0  3670.87200   334.04930     0.00000\n116     1  21   12  white       0        0  3670.87200   334.04940 12558.02000\n117     1  27   12  black       0        0  2143.41300   357.94990 22163.25000\n118     1  20   12  black       0        0     0.00000   377.56860  1652.63700\n119     1  19   10  black       0        1     0.00000   385.27410  8124.71500\n120     1  23   12  black       0        0  5506.30800   501.07410   671.33180\n121     1  29   14  black       0        0     0.00000   679.67340 17814.98000\n122     1  18   10  black       0        1     0.00000   798.90790  9737.15400\n123     1  19    9  black       0        1     0.00000   798.90790 17685.18000\n124     1  27   13  white       1        0  9381.56600   853.72250     0.00000\n125     1  18   11  white       0        1  3678.23100   919.55790  4321.70500\n126     1  27    9  black       1        1     0.00000   934.44540  1773.42300\n127     1  22   12  black       0        0  5605.85200   936.17730     0.00000\n128     1  23   10  black       1        1     0.00000   936.43860 11233.26000\n129     1  23   12 hispan       0        0  9385.74000  1117.43900   559.44320\n130     1  20   11  black       0        1  3637.49800  1220.83600  1085.44000\n131     1  17    9  black       0        1  1716.50900  1253.43900  5445.20000\n132     1  28   11  black       0        1     0.00000  1284.07900 60307.93000\n133     1  26   11  black       1        1     0.00000  1392.85300  1460.36000\n134     1  20   11  black       0        1 16318.62000  1484.99400  6943.34200\n135     1  24   11  black       1        1   824.38860  1666.11300  4032.70800\n136     1  31    9  black       0        1     0.00000  1698.60700 10363.27000\n137     1  23    8  white       1        1     0.00000  1713.15000  4232.30900\n138     1  18   10  black       0        1  2143.41100  1784.27400 11141.39000\n139     1  29   12  black       0        0 10881.94000  1817.28400     0.00000\n140     1  26   11  white       0        1     0.00000  2226.26600 13385.86000\n141     1  24    9  black       0        1  9154.70000  2288.67500  4849.55900\n142     1  25   12  black       0        0 14426.79000  2409.27400     0.00000\n143     1  24   10  black       0        1  4250.40200  2421.94700  1660.50800\n144     1  46    8  black       0        1  3165.65800  2594.72300     0.00000\n145     1  31   12  white       0        0     0.00000  2611.21800  2484.54900\n146     1  19   11  black       0        1  2305.02600  2615.27600  4146.60300\n147     1  19    8  black       0        1     0.00000  2657.05700  9970.68100\n148     1  27   11  black       0        1  2206.94000  2666.27400     0.00000\n149     1  26   11  black       1        1     0.00000  2754.64600 26372.28000\n150     1  20   10  black       0        1  5005.73100  2777.35500  5615.18900\n151     1  28   10  black       0        1     0.00000  2836.50600  3196.57100\n152     1  24   12  black       0        0 13765.75000  2842.76400  6167.68100\n153     1  19    8  black       0        1  2636.35300  2937.26400  7535.94200\n154     1  23   12  black       0        0  6269.34100  3039.96000  8484.23900\n155     1  42    9  black       1        1     0.00000  3058.53100  1294.40900\n156     1  25   13  black       0        0 12362.93000  3090.73200     0.00000\n157     1  18    9  black       0        1     0.00000  3287.37500  5010.34200\n158     1  21   12  black       0        0  6473.68300  3332.40900  9371.03700\n159     1  27   10  black       0        1  1001.14600  3550.07500     0.00000\n160     1  21    8  black       0        1   989.26780  3695.89700  4279.61300\n161     1  22    9  black       0        1  2192.87700  3836.98600  3462.56400\n162     1  31    4  black       0        1  8517.58900  4023.21100  7382.54900\n163     1  24   10  black       1        1 11703.20000  4078.15200     0.00000\n164     1  29   10  black       0        1     0.00000  4398.95000     0.00000\n165     1  29   12  black       0        0  9748.38700  4878.93700 10976.51000\n166     1  19   10  white       0        1     0.00000  5324.10900 13829.62000\n167     1  19   11 hispan       1        1  5424.48500  5463.80300  6788.46300\n168     1  31    9  black       0        1 10717.03000  5517.84100  9558.50100\n169     1  22   10  black       1        1  1468.34800  5588.66400 13228.28000\n170     1  21    9  black       0        1  6416.47000  5749.33100   743.66660\n171     1  17   10  black       0        1  1291.46800  5793.85200  5522.78800\n172     1  26   12  black       1        0  8408.76200  5794.83100  1424.94400\n173     1  20    9 hispan       0        1 12260.78000  5875.04900  1358.64300\n174     1  19   10  black       0        1  4121.94900  6056.75400     0.00000\n175     1  26   10  black       0        1 25929.68000  6788.95800   672.87730\n176     1  28   11  black       0        1  1929.02900  6871.85600     0.00000\n177     1  22   12 hispan       1        0   492.23050  7055.70200 10092.83000\n178     1  33   11  black       0        1     0.00000  7867.91600  6281.43300\n179     1  22   12  white       0        0  6759.99400  8455.50400 12590.71000\n180     1  29   10 hispan       0        1     0.00000  8853.67400  5112.01400\n181     1  33   12  black       1        0 20279.95000 10941.35000 15952.60000\n182     1  25   14  black       1        0 35040.07000 11536.57000 36646.95000\n183     1  35    9  black       1        1 13602.43000 13830.64000 12803.97000\n184     1  35    8  black       1        1 13732.07000 17976.15000  3786.62800\n185     1  33   11  black       1        1 14660.71000 25142.24000  4181.94200\n186     0  30   12  white       1        0 20166.73000 18347.23000 25564.67000\n187     0  26   12  white       1        0 25862.32000 17806.55000 25564.67000\n188     0  25   16  white       1        0 25862.32000 15316.21000 25564.67000\n189     0  42   11  white       1        1 21787.05000 14265.29000 15491.01000\n190     0  25    9  black       1        1 14829.69000 13776.53000     0.00000\n191     0  37    9  black       1        1 13685.48000 12756.05000 17833.20000\n192     0  32   12  white       1        0 19067.58000 12625.35000 14146.28000\n193     0  20   12  black       0        0  7392.31400 12396.19000 17765.23000\n194     0  38    9 hispan       1        1 16826.18000 12029.18000     0.00000\n195     0  39   10  white       1        1 16767.41000 12022.02000  4433.18000\n196     0  41    5  white       1        1 10785.76000 11991.58000 19451.31000\n197     0  31   14  white       1        0 17831.29000 11563.69000 22094.97000\n198     0  34    8  white       1        1  8038.87200 11404.35000  5486.79900\n199     0  29   12  white       1        0 14768.95000 11146.55000  6420.72200\n200     0  22   14  black       1        0   748.43990 11105.37000 18208.55000\n201     0  42    0 hispan       1        1  2797.83300 10929.92000  9922.93400\n202     0  25    9 hispan       0        1  5460.47700 10589.76000  7539.36100\n203     0  28    9  white       1        1 11091.41000 10357.02000 15406.78000\n204     0  40   13  white       1        0  3577.62100 10301.52000 11911.95000\n205     0  35    9  white       1        1 11475.43000  9397.40300 11087.38000\n206     0  27   10 hispan       1        1 15711.36000  9098.41900 17023.41000\n207     0  27    6 hispan       1        1  7831.18900  9071.56500  5661.17100\n208     0  36   12  white       1        0 25535.12000  8695.59700 21905.82000\n209     0  47    8  black       1        1  9275.16900  8543.41900     0.00000\n210     0  40   11  white       1        1 20666.35000  8502.24200 25564.67000\n211     0  27    7  white       1        1  3064.29300  8461.06500 11149.45000\n212     0  36    9  black       1        1 13256.40000  8457.48400     0.00000\n213     0  39    6 hispan       1        1 13279.91000  8441.37100 25048.94000\n214     0  21    9  white       1        1 11156.07000  8441.37100  1213.21400\n215     0  29   12  white       1        0 11199.17000  8081.51600     0.00000\n216     0  22   13 hispan       0        0  6404.84300  7882.79000  9453.01700\n217     0  25   10  white       1        1 13634.54000  7793.27400 11688.82000\n218     0  27   12  white       1        0 12270.89000  7709.12900  7806.82900\n219     0  45    8  white       1        1 22415.97000  7635.72600 15931.37000\n220     0  26   12  white       1        0  2345.24200  7565.90300  2838.71300\n221     0  27   12  white       1        0  9788.49700  7496.08100 14038.40000\n222     0  33    8  white       1        1 12312.03000  7474.59700 25514.43000\n223     0  25   12  white       1        0 11381.38000  7467.43500  4162.75600\n224     0  49    8  white       1        1  6459.70300  7431.62900  7503.89600\n225     0  40    3 hispan       1        1  7576.48500  7426.25800 12104.06000\n226     0  22   12  black       1        0  9729.71900  7372.54800  2231.36700\n227     0  25    5  white       1        1  7891.92700  7293.77400 14617.67000\n228     0  25   12  white       1        0 11516.57000  7263.33900 19588.74000\n229     0  21   12  white       1        0 13601.23000  7202.46800 10746.03000\n230     0  33    9 hispan       1        1 11959.36000  7087.88700 25564.67000\n231     0  20   12  black       1        0  9555.34400  7055.66100     0.00000\n232     0  19   11  white       1        1  4306.46800  6978.67700   837.87100\n233     0  25   12  black       1        0   295.84930  6942.87100   461.05070\n234     0  29   12  white       1        0 15303.83000  6932.12900 24290.87000\n235     0  20   12  white       1        0  3558.02900  6797.85500  6680.80200\n236     0  29    6 hispan       1        1  8542.40300  6701.17700  7196.52800\n237     0  25   13  white       1        0 19259.59000  6652.83900 13015.82000\n238     0  41   15  white       1        0 25862.32000  6563.32300 24647.00000\n239     0  39   10  white       1        1 22745.13000  6493.50000 25564.67000\n240     0  33   12  white       1        0 10819.07000  6369.96800  2936.24300\n241     0  29    8  white       1        1  9169.36900  6352.06500 20575.86000\n242     0  21   11  white       1        1 10679.96000  6276.87100 10923.35000\n243     0  31   12  white       1        0 23652.27000  6228.53200 22403.81000\n244     0  36   12  black       1        0 11040.47000  6221.37100  7215.73900\n245     0  25    7  white       1        1  5597.62500  6099.62900   122.65130\n246     0  35    7  white       1        1 10715.23000  6087.09700 15177.73000\n247     0  22    9  white       1        1  5683.83300  6038.75800  4742.02500\n248     0  31    2 hispan       1        1  3262.17900  5965.35500  9732.30700\n249     0  40   15  white       1        0 10907.24000  5922.38700  6238.96200\n250     0  47    3  white       1        1  9047.89400  5911.64500  6145.86500\n251     0  26    8 hispan       0        1  3168.13400  5872.25800 11136.15000\n252     0  42    7  white       1        1 10971.89000  5806.01600  9241.70200\n253     0  53   12  white       0        0 17104.40000  5775.58100 19965.56000\n254     0  30   17  black       0        0 17827.37000  5546.41900 14421.13000\n255     0  28   10  white       1        1 10415.46000  5544.62900 10289.41000\n256     0  46   11  white       1        1 14753.28000  5299.35500     0.00000\n257     0  28   12  white       0        0  8256.35000  5279.66100 21602.88000\n258     0  27   12 hispan       1        0 17604.01000  5222.37100 25564.67000\n259     0  25   10  white       1        1  4335.85700  5181.19400 12418.81000\n260     0  38    8  white       1        1 11242.27000  5174.03200     0.00000\n261     0  26   12 hispan       0        0  7968.33800  5109.58100  4181.96600\n262     0  54   12  white       0        0  7165.03900  5012.90300     0.00000\n263     0  38    8 hispan       1        1 22606.02000  4978.88700  8720.06500\n264     0  23   17  white       0        0     0.00000  4876.83900 16747.08000\n265     0  23    8  white       1        1  3595.25500  4866.09700  2782.55900\n266     0  23   12  white       1        0 11690.95000  4764.04800 14065.00000\n267     0  25   12 hispan       1        0  8746.16700  4762.25800   379.77570\n268     0  25   15  white       1        0  7386.43600  4738.98400 12705.49000\n269     0  37   11 hispan       0        1   615.20980  4713.91900     0.00000\n270     0  40   12  white       1        0 18389.68000  4688.85500 21857.05000\n271     0  19   10  white       0        1  5777.87800  4672.74200   135.95080\n272     0  48    7  white       1        1 13326.93000  4636.93500     0.00000\n273     0  19   12  white       0        0  8530.64800  4620.82300     0.00000\n274     0  16    9  white       0        1  2539.21000  4579.64500     0.00000\n275     0  29   10  white       1        1   713.17310  4542.04800  7781.70800\n276     0  30   16  white       0        0  3093.68200  4468.64500 15538.29000\n277     0  22   11  white       1        1  8761.84100  4463.27400 10642.59000\n278     0  22   10  white       0        1 17268.98000  4400.61300  2453.02600\n279     0  47   10  black       1        1 13311.26000  4397.03200 19330.14000\n280     0  25   12 hispan       1        0  2266.87200  4361.22600  3020.47300\n281     0  47   10  black       0        1 21918.32000  4323.62900 19438.02000\n282     0  24   12  black       1        0  8573.75200  4293.19400     0.00000\n283     0  20   12  black       1        0  2648.92900  4273.50000     0.00000\n284     0  28   12  black       0        0 16722.34000  4253.80600  7314.74700\n285     0  47   11  white       0        1  8060.42400  4232.32300  3358.87300\n286     0  50    0  white       1        1 10162.72000  4218.00000   220.18130\n287     0  18   12  white       0        0  2217.89000  4191.14500  8957.97800\n288     0  21   12  white       0        0  9665.06300  4110.58100  1687.56400\n289     0  47   11  white       1        1 23924.61000  4096.25800 17358.85000\n290     0  21   12  white       0        0  2827.22200  4056.87100  5937.50500\n291     0  34   11  white       1        1     0.00000  4010.32300 18133.18000\n292     0  19   12  white       1        0  5817.06300  3919.01600  1066.91900\n293     0  44   13  white       1        0  8032.99400  3881.41900  3104.70400\n294     0  21   15  white       1        0  6951.47900  3879.62900     0.00000\n295     0  20   12  black       0        0  5099.97100  3842.03200 12718.79000\n296     0  51   11  white       0        1    48.98167  3813.38700  1525.01400\n297     0  28   13  white       0        0  5260.63100  3790.11300  9253.52400\n298     0  24   15  white       0        0 12746.99000  3743.56500     0.00000\n299     0  28    8 hispan       1        1  8305.33200  3718.50000     0.00000\n300     0  20   11  white       1        1  5822.94100  3532.30600 11075.56000\n301     0  29   12  white       1        0 14288.93000  3503.66100  8133.40700\n302     0  23   12  white       1        0 14347.71000  3482.17700  3818.44500\n303     0  20   11  black       0        1     0.00000  3480.38700  5495.66500\n304     0  42    7  white       1        1  4324.10200  3457.11300  9856.43600\n305     0  43   12  white       1        0 14328.12000  3453.53200 18781.90000\n306     0  27   13  white       0        0 16406.90000  3426.67700  5344.93700\n307     0  27    4 hispan       1        1   626.96540  3410.56500  3367.73900\n308     0  25   12  white       1        0 21469.65000  3405.19400  7981.20100\n309     0  18   12  white       0        0  4729.67000  3328.21000 12602.05000\n310     0  31   16  white       1        0 25862.32000  3254.80600 25564.67000\n311     0  27   12  white       1        0  4043.92700  3231.53200  7240.86000\n312     0  18   11  white       0        1     0.00000  3226.16100 15814.63000\n313     0  24    7  white       1        1  7860.57800  3213.62900     0.00000\n314     0  23   12  white       1        0  7856.66000  3213.62900  5535.56400\n315     0  50   12  white       1        0 19929.66000  3190.35500 18597.19000\n316     0  19   12  white       0        0    99.92261  3172.45200 15436.33000\n317     0  23   10  white       1        1 15811.28000  3145.59700  6398.55600\n318     0  51   12  white       1        0 21001.38000  3140.22600 16015.60000\n319     0  19   11  black       0        1  5607.42200  3054.29000    94.57450\n320     0  20   10  white       1        1  3099.56000  2970.14500 21141.83000\n321     0  20   11 hispan       0        1  2868.36700  2968.35500  7403.41000\n322     0  21   12  white       0        0  8128.99800  2939.71000     0.00000\n323     0  39   10  white       1        1     0.00000  2886.00000 18761.22000\n324     0  36    5  white       0        1  3814.69200  2873.46800  2751.52700\n325     0  19    9  black       0        1  1079.55600  2873.46800 14344.29000\n326     0  42    6 hispan       1        1  2425.57200  2832.29000  1907.74500\n327     0  20    7  white       0        1  1902.44800  2792.90300  6098.57800\n328     0  23   12  white       1        0  4954.98600  2771.41900     0.00000\n329     0  35   12  white       1        0  1469.45000  2719.50000     0.00000\n330     0  18   12  white       0        0   881.67010  2696.22600 12120.31000\n331     0  43    8  white       1        1 18338.74000  2674.74200  6395.60100\n332     0  37   14  white       1        0 18501.36000  2638.93500 13429.58000\n333     0  24   10  white       1        1  4719.87400  2565.53200  2173.73600\n334     0  51   12  white       0        0 20742.76000  2538.67700  1019.63100\n335     0  22   11 hispan       0        1  7341.37300  2535.09700 14187.65000\n336     0  19   12  white       0        0   336.99390  2518.98400  7118.20900\n337     0  52    0 hispan       1        1   773.91040  2506.45200     0.00000\n338     0  21   12  white       0        0  2903.63300  2456.32300  4787.83400\n339     0  24   12  white       0        0  9784.57800  2413.35500     0.00000\n340     0  35    8  white       1        1  2241.40100  2399.03200  9460.40600\n341     0  20   13  white       0        0     0.00000  2352.48400     0.00000\n342     0  17    7  black       0        1  1054.08600  2286.24200  1613.67700\n343     0  18   10  black       0        1   311.52340  2284.45200  8154.09500\n344     0  28   12  black       0        0  6285.32800  2255.80600  7310.31300\n345     0  25   14 hispan       1        0  1622.27300  2239.69400  1892.96800\n346     0  40   12 hispan       0        0 13616.90000  2228.95200   876.29190\n347     0  50    3  white       1        1  3136.78600  2203.88700 13976.34000\n348     0  48    8  white       1        1 16050.31000  2116.16100 11600.15000\n349     0  17    7 hispan       0        1     0.00000  2082.14500  6460.62100\n350     0  30   12  white       1        0  7347.25100  2080.35500 14475.81000\n351     0  30    7  white       1        1   574.06520  2010.53200   366.47620\n352     0  22   11  white       1        1  3030.98600  1976.51600     0.00000\n353     0  27   12  white       1        0 11493.06000  1906.69400 13419.24000\n354     0  25    9  white       1        1 23377.97000  1901.32300  1898.87900\n355     0  21   14  white       0        0    80.32994  1890.58100  6389.69000\n356     0  17   10  white       0        1     0.00000  1888.79000 19993.64000\n357     0  39    7  white       0        1  7786.12600  1844.03200  9206.23700\n358     0  18    9  black       0        1  1183.39700  1822.54800   803.88330\n359     0  25   12  white       1        0  2721.42200  1754.51600  1037.36400\n360     0  20    8  white       1        1  2360.91600  1741.98400     0.00000\n361     0  19   13  white       0        0  2366.79400  1709.75800     0.00000\n362     0  19   11  white       0        1     0.00000  1693.64500  9853.48100\n363     0  22   12  white       0        0 10137.25000  1679.32300 25564.67000\n364     0  18   11  black       0        1  2068.98600  1623.82300 20243.38000\n365     0  21   10  white       0        1  1767.25900  1555.79000  7675.31200\n366     0  24   12  white       1        0  7643.10000  1546.83900  3262.82000\n367     0  18   11  white       0        1  1273.52300  1532.51600 12489.75000\n368     0  17   10  white       0        1   568.18740  1525.35500  6231.57300\n369     0  17   10  white       0        1     0.00000  1503.87100  7843.77300\n370     0  18   10  white       0        1     0.00000  1491.33900   237.91400\n371     0  53   10 hispan       0        1  7878.21200  1489.54800 13170.98000\n372     0  18   11  black       0        1  1191.23400  1478.80600  3683.97200\n373     0  17   10 hispan       0        1     0.00000  1453.74200  6918.71600\n374     0  26   12  black       0        0     0.00000  1448.37100     0.00000\n375     0  39    5  white       1        1 13082.02000  1434.04800 18323.81000\n376     0  18   12  black       0        0  1579.16900  1408.98400  3057.41600\n377     0  23   13  white       0        0   601.49490  1394.66100  4975.50500\n378     0  18    8  white       0        1  5023.56000  1391.08100  6756.16600\n379     0  28   10  white       1        1  7578.44400  1383.91900  2404.26100\n380     0  32    4  white       1        1     0.00000  1378.54800     0.00000\n381     0  18   11  black       0        1     0.00000  1367.80600    33.98771\n382     0  40   10  white       1        1  1543.90200  1342.74200     0.00000\n383     0  21   14  white       0        0  8456.19600  1330.21000 16967.26000\n384     0  29   10 hispan       0        1  3732.40300  1323.04800  6694.10100\n385     0  31    6  white       0        1  2666.56200  1321.25800     0.00000\n386     0  46    7  white       1        1 19171.43000  1317.67700     0.00000\n387     0  20    9 hispan       1        1     0.00000  1283.66100     0.00000\n388     0  36   18  white       1        0  3273.93500  1269.33900 18227.76000\n389     0  45   12  white       1        0 16559.72000  1265.75800  7987.11200\n390     0  16   10  white       0        1  1026.65600  1224.58100  6847.78500\n391     0  18   12  white       0        0   818.97350  1208.46800  2232.84500\n392     0  40   12 hispan       0        0 11867.28000  1195.93500  3873.12100\n393     0  16    9  white       0        1     0.00000  1188.77400  2451.54800\n394     0  16   10  white       0        1   574.06520  1181.61300  5578.41800\n395     0  28    5 hispan       1        1 10967.98000  1178.03200   239.39170\n396     0  20   12  white       0        0     0.00000  1147.59700 15554.55000\n397     0  19    8  white       1        1    39.18534  1136.85500  5327.20400\n398     0  16    8  white       0        1     0.00000  1113.58100   542.32570\n399     0  20   11  white       1        1  2547.04700  1099.25800     0.00000\n400     0  35   10  white       1        1  4964.78200  1086.72600  1745.19500\n401     0  32    6 hispan       1        1   979.63340  1036.59700     0.00000\n402     0  32   16  black       0        0 17135.75000  1031.22600     0.00000\n403     0  17    9  black       0        1     0.00000   981.09680  8900.34700\n404     0  16    7  white       0        1     0.00000   975.72580  4728.72500\n405     0  32   15  white       0        0   489.81670   968.56450  7684.17800\n406     0  19   12  white       0        0   815.05500   964.98390 12059.73000\n407     0  40   12  white       1        0 16851.65000   961.40320 17717.94000\n408     0  50    7  white       1        1 11473.47000   956.03230     0.00000\n409     0  39   11  white       0        1     0.00000   930.96770     0.00000\n410     0  18    8 hispan       0        1     0.00000   902.32260  1306.31000\n411     0  39   10  black       0        1   844.44400   889.79030   701.92010\n412     0  17   11 hispan       0        1     0.00000   873.67740  7759.54200\n413     0  17    5  black       0        1    96.00407   868.30650     0.00000\n414     0  19   12  white       0        0  2425.57200   861.14520  2587.49900\n415     0  27   15  white       0        0     0.00000   857.56450  3392.86000\n416     0  18   11  black       0        1   587.78000   841.45160  7933.91400\n417     0  20   14  white       1        0     0.00000   805.64520  1454.08300\n418     0  20   12  white       1        0 12145.49000   791.32260 13683.75000\n419     0  19   13  black       0        0  1714.35800   785.95160  9067.33000\n420     0  24    8  white       1        1   213.56010   760.88710  2340.71900\n421     0  27   12  white       1        0  4222.22000   751.93550     0.00000\n422     0  19    9  white       0        1   773.91040   676.74190  5647.87100\n423     0  52    8  black       1        1  5454.59900   666.00000     0.00000\n424     0  18   11 hispan       0        1     0.00000   630.19350     0.00000\n425     0  16   10 hispan       0        1     0.00000   630.19350  3892.33200\n426     0  18   12 hispan       0        0     0.00000   630.19350  4843.98800\n427     0  45   12  white       0        0  4473.00600   608.70970     0.00000\n428     0  21   14  white       0        0  9708.16700   594.38710  2256.48800\n429     0  36    8  white       1        1  2715.54400   585.43550     0.00000\n430     0  21   13  white       0        0   513.32790   578.27420     0.00000\n431     0  41    7  white       1        1 19573.08000   565.74190     0.00000\n432     0  18    7  white       0        1   491.77600   558.58060   642.81110\n433     0  39    9  white       0        1 11230.52000   537.09680  5752.79000\n434     0  19    3  white       1        1     0.00000   537.09680     0.00000\n435     0  32   13  white       1        0 12553.02000   524.56450 15353.58000\n436     0  16    9  white       0        1     0.00000   485.17740  4112.51300\n437     0  16    7  white       0        1   658.31360   479.80650  6210.88500\n438     0  21    9  black       0        1  1030.57400   470.85480  1223.55800\n439     0  22   12  white       1        0 12096.51000   469.06450 14289.62000\n440     0  23   11 hispan       1        1  8946.01200   469.06450  4776.01200\n441     0  17    8  black       0        1     0.00000   451.16130     0.00000\n442     0  21    8  white       1        1  5699.50700   388.50000  8844.19400\n443     0  18   10  white       0        1     0.00000   386.70970     0.00000\n444     0  24   12  white       1        0  9051.81300   327.62900  8547.17100\n445     0  24   12  black       1        0  4232.01600   320.46770  1273.80000\n446     0  16    9  white       0        1     0.00000   320.46770  3707.61600\n447     0  20    8  white       1        1   621.08760   306.14520  5551.81900\n448     0  42    8  white       0        1 17925.33000   300.77420 14116.72000\n449     0  17    8 hispan       0        1   391.85340   300.77420 18891.26000\n450     0  19    8 hispan       0        1   368.34220   300.77420 18510.00000\n451     0  17    9  black       0        1     0.00000   297.19350    54.67588\n452     0  21   14  white       0        0   107.75970   293.61290  7698.95500\n453     0  16    9  black       0        1     0.00000   277.50000  3983.95100\n454     0  23   13  black       0        0   172.41550   272.12900   582.22430\n455     0  16    9  white       0        1   411.44600   254.22580  1725.98500\n456     0  17   11 hispan       0        1   803.29940   248.85480  5173.52100\n457     0  46    7  white       0        1  1081.51500   245.27420     0.00000\n458     0  32   10  white       1        1  4145.80900   238.11290  8245.71400\n459     0  18   11  white       0        1   131.27090   218.41940  7503.89600\n460     0  23   12 hispan       1        0     0.00000   216.62900     0.00000\n461     0  18   10  white       1        1     0.00000   211.25810 14053.18000\n462     0  19   10  black       0        1  1056.04500   205.88710     0.00000\n463     0  16    7  black       0        1   133.23010   205.88710  6145.86500\n464     0  26    7  white       1        1  1538.02400   189.77420   650.19970\n465     0  16   10  white       0        1     0.00000   189.77420  2136.79300\n466     0  17   10  white       0        1     0.00000   182.61290  6423.67700\n467     0  17   10  white       0        1     0.00000   171.87100  1483.63700\n468     0  23    8  white       1        1    33.30754   166.50000     0.00000\n469     0  29   12  white       1        0 14641.60000   162.91940  9473.70500\n470     0  17   10  white       0        1     0.00000   152.17740 10301.23000\n471     0  49    8  white       1        1 14684.70000   136.06450 14963.46000\n472     0  20   10  white       1        1  6563.54400   134.27420 15363.92000\n473     0  40   16  white       1        0     0.00000   114.58060     0.00000\n474     0  19   10  white       0        1  1933.79600   112.79030   675.32100\n475     0  18   11  white       0        1  1481.20600    57.29032  1421.57300\n476     0  16    6  black       0        1     0.00000    44.75806     0.00000\n477     0  22    8  white       1        1   105.80040    42.96774   209.83720\n478     0  31   12  black       1        0     0.00000    42.96774 11023.84000\n479     0  20   11  white       1        1  4478.88400    39.38710  6280.33800\n480     0  17   11 hispan       0        1   601.49490    10.74194  1913.65600\n481     0  50   12  white       1        0 25862.32000     0.00000 25564.67000\n482     0  49   14  white       1        0 25862.32000     0.00000 25564.67000\n483     0  47    9  white       1        1 25862.32000     0.00000 25564.67000\n484     0  34   11 hispan       1        1 22198.49000     0.00000     0.00000\n485     0  22    8  black       1        1 16961.37000     0.00000   959.04450\n486     0  27   12  white       1        0 15509.56000     0.00000 12593.19000\n487     0  30   10  white       1        1 14913.94000     0.00000 11563.21000\n488     0  52   12  white       1        0 14780.71000     0.00000 25564.67000\n489     0  43   12  white       1        0 13321.05000     0.00000 16860.86000\n490     0  27    9 hispan       1        1 12829.28000     0.00000     0.00000\n491     0  35   13  white       0        0  9537.71100     0.00000 11269.14000\n492     0  45   12  white       1        0  9277.12800     0.00000 12108.49000\n493     0  22   11  black       1        1  9049.85300     0.00000  9088.01800\n494     0  22   12  white       1        0  9022.42400     0.00000  3342.61800\n495     0  23   11  white       1        1  8910.74500     0.00000  4183.44400\n496     0  55    7  white       1        1  8832.37500     0.00000     0.00000\n497     0  26   14  white       0        0  8411.13200     0.00000     0.00000\n498     0  34   12  white       0        0  8125.07900     0.00000  6032.08000\n499     0  22   11  white       0        1  8013.40100     0.00000  5748.35600\n500     0  31   12  white       0        0  6156.01600     0.00000  4094.78000\n501     0  19   12  white       0        0  5797.47000     0.00000  2160.43600\n502     0  24   10  white       1        1  5523.17300     0.00000  5040.52500\n503     0  36   12  white       1        0  5374.26900     0.00000     0.00000\n504     0  20    9  white       1        1  5229.28300     0.00000 15892.95000\n505     0  23    8  white       1        1  4610.15500     0.00000     0.00000\n506     0  35   11  white       1        1  3975.35200     0.00000 21963.45000\n507     0  23   12  white       0        0  3893.06300     0.00000 16324.45000\n508     0  29   10  white       0        1  3751.99600     0.00000   251.21350\n509     0  24    9  white       1        1  3438.51300     0.00000   818.66050\n510     0  18   10  white       0        1  3360.14300     0.00000     0.00000\n511     0  45    8  black       0        1  3299.40500     0.00000    31.03226\n512     0  21   13 hispan       0        0  3015.31200     0.00000 17627.80000\n513     0  29   13  white       1        0  2780.20000     0.00000 14339.86000\n514     0  21   15  white       0        0  2629.33600     0.00000  1717.11800\n515     0  22   16  black       0        0  2564.68000     0.00000   116.74040\n516     0  24   12  black       1        0  2355.03900     0.00000  2448.59300\n517     0  20   14  white       0        0  2210.05300     0.00000  2813.59100\n518     0  19    6  black       0        1  1955.34800     0.00000 14998.92000\n519     0  19    9 hispan       0        1  1822.11800     0.00000  3372.17200\n520     0  19   12  black       0        0  1681.05100     0.00000     0.00000\n521     0  20   13  white       0        0  1657.54000     0.00000   913.23500\n522     0  19   12  black       0        0  1655.58000     0.00000     0.00000\n523     0  26    5  white       1        1  1573.29100     0.00000  3700.22700\n524     0  26    9 hispan       0        1  1563.49500     0.00000  2862.35600\n525     0  23   12  white       0        0  1504.71700     0.00000     0.00000\n526     0  20    9 hispan       0        1  1500.79800     0.00000 12618.31000\n527     0  20   10  white       0        1  1412.63100     0.00000  6290.68200\n528     0  36   11  white       1        1  1404.79400     0.00000     0.00000\n529     0  39   12  white       1        0  1289.19800     0.00000  1202.86900\n530     0  17    9  black       0        1  1222.58200     0.00000   422.62980\n531     0  55    3  white       0        1  1208.86800     0.00000     0.00000\n532     0  28    8  white       1        1  1202.99000     0.00000 19516.33000\n533     0  19   12 hispan       0        0  1058.00400     0.00000  8923.99100\n534     0  37    7  white       1        1   963.95930     0.00000     0.00000\n535     0  16    9  white       1        1   920.85540     0.00000 15997.87000\n536     0  17   10  white       0        1   646.55800     0.00000  9438.24000\n537     0  24   12  black       0        0   566.22810     0.00000  2284.56500\n538     0  19   11  white       0        1   540.75760     0.00000  3406.16000\n539     0  50    5  black       1        1   411.44600     0.00000  9166.33800\n540     0  19    9  black       0        1   384.01630     0.00000     0.00000\n541     0  36    1  black       0        1   348.74950     0.00000     0.00000\n542     0  18   11  white       0        1   321.31980     0.00000  7722.59900\n543     0  16    7 hispan       0        1   289.97150     0.00000  7515.71700\n544     0  21   11  white       1        1   246.86760     0.00000  6708.87900\n545     0  55    6  white       1        1   111.67820     0.00000     0.00000\n546     0  37   12  white       0        0    48.98167     0.00000   877.76960\n547     0  26   12 hispan       1        0    47.02240     0.00000     0.00000\n548     0  54   12  white       1        0     0.00000     0.00000     0.00000\n549     0  50   12  white       1        0     0.00000     0.00000     0.00000\n550     0  16    8  white       0        1     0.00000     0.00000  2559.42200\n551     0  16    9 hispan       0        1     0.00000     0.00000     0.00000\n552     0  18   10  black       0        1     0.00000     0.00000  2281.61000\n553     0  40   11  black       1        1     0.00000     0.00000     0.00000\n554     0  16    8  white       0        1     0.00000     0.00000     0.00000\n555     0  16    9  black       0        1     0.00000     0.00000  2158.95900\n556     0  26   14  white       0        0     0.00000     0.00000  6717.74500\n557     0  20    9  black       0        1     0.00000     0.00000  6083.80000\n558     0  20   12  black       0        0     0.00000     0.00000     0.00000\n559     0  18   11  black       0        1     0.00000     0.00000     0.00000\n560     0  46   11  black       1        1     0.00000     0.00000  2820.98000\n561     0  17    8  black       0        1     0.00000     0.00000 12760.17000\n562     0  16    9  white       0        1     0.00000     0.00000  4974.02800\n563     0  30   10  white       1        1     0.00000     0.00000  3151.99100\n564     0  33   12 hispan       1        0     0.00000     0.00000  5841.45300\n565     0  34   12  black       1        0     0.00000     0.00000 18716.88000\n566     0  21   13  black       0        0     0.00000     0.00000 17941.08000\n567     0  29   11  white       1        1     0.00000     0.00000     0.00000\n568     0  19   12  white       0        0     0.00000     0.00000     0.00000\n569     0  31    4 hispan       0        1     0.00000     0.00000  1161.49300\n570     0  19   12 hispan       0        0     0.00000     0.00000 18573.55000\n571     0  20   12  black       0        0     0.00000     0.00000 11594.24000\n572     0  55    4  black       0        1     0.00000     0.00000     0.00000\n573     0  19   11  black       0        1     0.00000     0.00000 16485.52000\n574     0  18   11  black       0        1     0.00000     0.00000  7146.28600\n575     0  48   13  white       1        0     0.00000     0.00000     0.00000\n576     0  16    9 hispan       1        1     0.00000     0.00000  6821.18600\n577     0  17   10  black       0        1     0.00000     0.00000     0.00000\n578     0  38   12  white       1        0     0.00000     0.00000 18756.78000\n579     0  34    8  white       1        1     0.00000     0.00000  2664.34100\n580     0  53   12  white       0        0     0.00000     0.00000     0.00000\n581     0  48   14  white       1        0     0.00000     0.00000  7236.42700\n582     0  16    9  white       0        1     0.00000     0.00000  6494.60800\n583     0  17    8  black       0        1     0.00000     0.00000  4520.36600\n584     0  27   14  black       0        0     0.00000     0.00000 10122.43000\n585     0  37    8  black       0        1     0.00000     0.00000   648.72200\n586     0  17   10  black       0        1     0.00000     0.00000  1053.61900\n587     0  16    8  white       0        1     0.00000     0.00000     0.00000\n588     0  48   12  white       1        0     0.00000     0.00000  1491.02600\n589     0  55    7  white       0        1     0.00000     0.00000     0.00000\n590     0  21   15  white       0        0     0.00000     0.00000     0.00000\n591     0  16   10  black       0        1     0.00000     0.00000  1730.41800\n592     0  23   12  white       0        0     0.00000     0.00000  3902.67600\n593     0  46   11  black       1        1     0.00000     0.00000     0.00000\n594     0  17   10  white       0        1     0.00000     0.00000 14942.77000\n595     0  42   16  white       0        0     0.00000     0.00000 23764.80000\n596     0  18   10  black       0        1     0.00000     0.00000  5306.51600\n597     0  53   12  black       0        0     0.00000     0.00000     0.00000\n598     0  17   10  white       1        1     0.00000     0.00000  3859.82200\n599     0  17    6  white       0        1     0.00000     0.00000     0.00000\n600     0  43    6  white       1        1     0.00000     0.00000     0.00000\n601     0  34   12  black       0        0     0.00000     0.00000     0.00000\n602     0  16    8 hispan       0        1     0.00000     0.00000 12242.96000\n603     0  27   12  white       1        0     0.00000     0.00000  1533.88000\n604     0  51    4  black       0        1     0.00000     0.00000     0.00000\n605     0  39    2  black       1        1     0.00000     0.00000   964.95550\n606     0  55    8  white       1        1     0.00000     0.00000     0.00000\n607     0  16    9  white       0        1     0.00000     0.00000  5551.81900\n608     0  27   10  black       0        1     0.00000     0.00000  7543.79400\n609     0  25   14  white       0        0     0.00000     0.00000     0.00000\n610     0  18   11  white       0        1     0.00000     0.00000 10150.50000\n611     0  24    1 hispan       1        1     0.00000     0.00000 19464.61000\n612     0  21   18  white       0        0     0.00000     0.00000     0.00000\n613     0  32    5  black       1        1     0.00000     0.00000   187.67130\n614     0  16    9  white       0        1     0.00000     0.00000  1495.45900\n\n\n\n\n\n\n\n変数名\n説明\n\n\n\n\n1\ntreat\n職業訓練の履修有無 (Treatment)\n\n\n2\nage\n年齢\n\n\n3\neduc\n教育年数\n\n\n4\nrace\n人種(白人、黒人、ヒスパニック)\n\n\n5\nmarried\n結婚有無\n\n\n6\nnodegree\n学位有無\n\n\n7\nre74\n1974 年の所得\n\n\n8\nre75\n1975 年の所得\n\n\n9\nre78\n1978 年の所得 (Outcome)\n\n\n\n\n\n\n\n\nhttps://www.jaysong.net/kobe-ci"
  },
  {
    "objectID": "slide/matching.html#実習用データ-職業訓練の有無と所得",
    "href": "slide/matching.html#実習用データ-職業訓練の有無と所得",
    "title": "方法論特殊講義III",
    "section": "実習用データ: 職業訓練の有無と所得",
    "text": "実習用データ: 職業訓練の有無と所得\nlalondeデータセットは様々なパッケージがサンプルデータとして提供しているが、本講義では{cobalt}パッケージのlalondeデータセットを利用する。\n\n\n\ndata(\"lalonde\", package = \"cobalt\")で読み込み\n\nその前に{cobalt}パッケージを読み込む\n\n\n\ndata(\"lalonde\", package = \"cobalt\")\nlalonde\n\n\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   <int> <int> <int> <fct>    <int>    <int> <dbl> <dbl>  <dbl>\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# … with 604 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\n\n\n\n変数名\n説明\n\n\n\n\n1\ntreat\n職業訓練の履修有無 (処置)\n\n\n2\nage\n年齢\n\n\n3\neduc\n教育年数\n\n\n4\nrace\n人種\n\n\n5\nmarried\n結婚有無\n\n\n6\nnodegree\n学位有無\n\n\n7\nre74\n1974年の所得\n\n\n8\nre75\n1975年の所得\n\n\n9\nre78\n1978年の所得 (結果)\n\n\n\n\n\n\n\n\nhttps://www.jaysong.net/kobe-ci"
  },
  {
    "objectID": "slide/matching.html#実習用データ-lalonde職業訓練の有無と所得",
    "href": "slide/matching.html#実習用データ-lalonde職業訓練の有無と所得",
    "title": "方法論特殊講義III",
    "section": "実習用データ: lalonde（職業訓練の有無と所得）",
    "text": "実習用データ: lalonde（職業訓練の有無と所得）\nlalondeは様々なパッケージがサンプルデータとして提供しているが、本講義では{cobalt}パッケージのlalondeデータセットを利用\n\n\n\ndata(\"lalonde\", package = \"cobalt\")で読み込み\n\nその前に{cobalt}パッケージを読み込む\n\n\n\ndata(\"lalonde\", package = \"cobalt\")\nlalonde\n\n\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   <int> <int> <int> <fct>    <int>    <int> <dbl> <dbl>  <dbl>\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# … with 604 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\n\n\n\n変数名\n説明\n\n\n\n\n1\ntreat\n職業訓練の履修有無 (処置)\n\n\n2\nage\n年齢\n\n\n3\neduc\n教育年数\n\n\n4\nrace\n人種\n\n\n5\nmarried\n結婚有無\n\n\n6\nnodegree\n学位有無\n\n\n7\nre74\n1974年の所得\n\n\n8\nre75\n1975年の所得\n\n\n9\nre78\n1978年の所得 (結果)\n\n\n\n\n\n\n\n\nhttps://www.jaysong.net/kobe-ci"
  },
  {
    "objectID": "slide/matching.html#実習用データ-lalonde職業訓練と所得",
    "href": "slide/matching.html#実習用データ-lalonde職業訓練と所得",
    "title": "方法論特殊講義III",
    "section": "実習用データ: lalonde（職業訓練と所得）",
    "text": "実習用データ: lalonde（職業訓練と所得）\nlalondeは様々なパッケージがサンプルデータとして提供しているが、本講義では{cobalt}のlalondeデータセットを利用\n\n\n\ndata(\"lalonde\", package = \"cobalt\")で読み込み\n\nその前に{cobalt}パッケージを読み込む\n\n\n\ndata(\"lalonde\", package = \"cobalt\")\nla_df <- as_tibble(lalonde)\nla_df\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   <int> <int> <int> <fct>    <int>    <int> <dbl> <dbl>  <dbl>\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# … with 604 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\n\n\n\n\n変数名\n説明\n\n\n\n\n1\ntreat\n職業訓練の履修有無\n\n\n2\nage\n年齢\n\n\n3\neduc\n教育年数\n\n\n4\nrace\n人種\n\n\n5\nmarried\n結婚有無\n\n\n6\nnodegree\n学位有無\n\n\n7\nre74\n1974年の所得\n\n\n8\nre75\n1975年の所得\n\n\n9\nre78\n1978年の所得\n\n\n\n\n\n\n\n\nhttps://www.jaysong.net/kobe-ci"
  },
  {
    "objectID": "material/matching.html#パッケージとデータの読み込み",
    "href": "material/matching.html#パッケージとデータの読み込み",
    "title": "マッチング",
    "section": "パッケージとデータの読み込み",
    "text": "パッケージとデータの読み込み\n　本日の実習で使用するパッケージを読み込む。\n\npacman::p_load(tidyverse, \n               MatchIt, \n               WeightIt, \n               cobalt, \n               summarytools,\n               modelsummary,\n               kableExtra,\n               fastDummies)\npacman::p_load_gh(\"JaehyunSong/BalanceR\")\n\n　マッチングにおける古典的なデータセット、lalondeを読み込む。data(lalonde, package = \"cobalt\")を入力するだけで、{cobalt}パッケージ内のlaondeという名前のデータフレームが作業環境内にlalondeという名で格納される1。このデータをla_dfという名のオブジェクトとして改めて保存しておこう。ただし、lalondeデータセットの形式はdata.frameである。このままでも全く問題ないが、data.frameの拡張版であるtibble形式の方がより読みやすいので、格納する前にlalondeのデータ構造をdata.frameからtibbleへ変更しておこう（as_tibble()関数を使う）。\n\n# cobaltパッケージが提供するデータセットの読み込み\ndata(\"lalonde\", package = \"cobalt\")\n\nla_df <- as_tibble(lalonde)\n\n　それでは、データの中身を確認してみよう。\n\nla_df\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   <int> <int> <int> <fct>    <int>    <int> <dbl> <dbl>  <dbl>\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# … with 604 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　分析に入る前に、名目変数である人種（race）をダミー変数に変換する。raceは3種類の値で構成されているため、生成するダミー変数も3つとなる。ダミー化には{fastDummies}パッケージのdummy_cols()関数を使用する。\n\nla_df <- la_df |>\n  dummy_cols(select_columns = \"race\")\n\nla_df\n\n# A tibble: 614 × 12\n   treat   age  educ race   married nodegree  re74  re75   re78 race_b…¹ race_…²\n   <int> <int> <int> <fct>    <int>    <int> <dbl> <dbl>  <dbl>    <int>   <int>\n 1     1    37    11 black        1        1     0     0  9930.        1       0\n 2     1    22     9 hispan       0        1     0     0  3596.        0       1\n 3     1    30    12 black        0        0     0     0 24909.        1       0\n 4     1    27    11 black        0        1     0     0  7506.        1       0\n 5     1    33     8 black        0        1     0     0   290.        1       0\n 6     1    22     9 black        0        1     0     0  4056.        1       0\n 7     1    23    12 black        0        0     0     0     0         1       0\n 8     1    32    11 black        0        1     0     0  8472.        1       0\n 9     1    22    16 black        0        0     0     0  2164.        1       0\n10     1    33    12 white        1        0     0     0 12418.        0       0\n# … with 604 more rows, 1 more variable: race_white <int>, and abbreviated\n#   variable names ¹​race_black, ²​race_hispan\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n　このまま記述統計を見たり、分析に入っても良いが、もう少しデータを加工してみよう。まずrace_で始まる3つのダミー変数の位置をraceの前へ変更する。また、race変数は不要なので、race変数を除外する。最後に、race_で始まるダミー変数の名前を変更してみよう。変数の位置変更はrelocate()関数を使用する。\n\nla_df <- la_df |>\n  relocate(starts_with(\"race_\"), .before = race) |>\n  select(-race) |>\n  rename(\"black\"    = \"race_black\",\n         \"hispanic\" = \"race_hispan\",\n         \"white\"    = \"race_white\")\n\nla_df\n\n# A tibble: 614 × 11\n   treat   age  educ black hispanic white married nodegree  re74  re75   re78\n   <int> <int> <int> <int>    <int> <int>   <int>    <int> <dbl> <dbl>  <dbl>\n 1     1    37    11     1        0     0       1        1     0     0  9930.\n 2     1    22     9     0        1     0       0        1     0     0  3596.\n 3     1    30    12     1        0     0       0        0     0     0 24909.\n 4     1    27    11     1        0     0       0        1     0     0  7506.\n 5     1    33     8     1        0     0       0        1     0     0   290.\n 6     1    22     9     1        0     0       0        1     0     0  4056.\n 7     1    23    12     1        0     0       0        0     0     0     0 \n 8     1    32    11     1        0     0       0        1     0     0  8472.\n 9     1    22    16     1        0     0       0        0     0     0  2164.\n10     1    33    12     0        0     1       1        0     0     0 12418.\n# … with 604 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　それでは記述統計量を確認してみよう。\n\ndescr(la_df,\n      stats = c(\"mean\", \"sd\", \"min\", \"max\"),\n      transpose = TRUE,\n      order = \"p\")\n\n\n\n\n\n \nMean\nStd.Dev\nMin\nMax\n\n\n\n\ntreat\n0.30\n0.46\n0.00\n1.00\n\n\nage\n27.36\n9.88\n16.00\n55.00\n\n\neduc\n10.27\n2.63\n0.00\n18.00\n\n\nblack\n0.40\n0.49\n0.00\n1.00\n\n\nhispanic\n0.12\n0.32\n0.00\n1.00\n\n\nwhite\n0.49\n0.50\n0.00\n1.00\n\n\nmarried\n0.42\n0.49\n0.00\n1.00\n\n\nnodegree\n0.63\n0.48\n0.00\n1.00\n\n\nre74\n4557.55\n6477.96\n0.00\n35040.07\n\n\nre75\n2184.94\n3295.68\n0.00\n25142.24\n\n\nre78\n6792.83\n7470.73\n0.00\n60307.93"
  },
  {
    "objectID": "material/matching.html#まずは単純差分から",
    "href": "material/matching.html#まずは単純差分から",
    "title": "マッチング",
    "section": "まずは単純差分から",
    "text": "まずは単純差分から\ntreat変数の値ごとに結果変数であるre78の平均値を計算してみましょう。\n\nDiff_Mean_df <- la_df |> \n    group_by(treat) |>\n    summarise(Outcome = mean(re78),\n              .groups = \"drop\")\n\nDiff_Mean_df\n\n# A tibble: 2 × 2\n  treat Outcome\n  <int>   <dbl>\n1     0   6984.\n2     1   6349.\n\n\nこの結果を可視化するコードは以下の通りです。\n\nDiff_Mean_df |>\n  ggplot() +\n  geom_bar(aes(x = treat, y = Outcome), \n           stat = \"identity\", width = 0.5) +\n  geom_label(aes(x = treat, y = Outcome,\n                 label = round(Outcome, 3))) +\n  labs(x = \"Treatment\",\n       y = \"Outcome (US Dollars)\") +\n  # scale_x_continuous()を使って0/1をControl/Treatmentに置換する\n  # 目盛りはX軸上の0と1、各目盛りのラベルはControlとTreatmentに\n  scale_x_continuous(breaks = c(0, 1), labels = c(\"Control\", \"Treatment\")) +\n  coord_cartesian(xlim = c(-0.5, 1.5))\n\n\n\n\n\n\n\n\ntreat == 0の回答者、つまり職業訓練を受けていない回答者の平均所得は約6984ドル、treat == 1の回答者、つまり職業訓練を受けた回答者の平均所得は約6394ドルです。その差は約-650ドルですが、職業訓練を受けた回答者の方が低所得になっています。これは直感的に納得できる結果ではないでしょう。むろん、実際、職業訓練が所得を減らす可能性もありますが、今回の結果はより詳しく分析してみる価値はあるでしょう。\nちなみに、以上の結果は単回帰分析からも確認可能です (ただし、統計的に有意ではない)。\n\nRaw_Fit1 <- lm(re78 ~ treat, data = la_df)\n# modelsummaryを使った推定結果の要約\n# デフォルトでは点推定値と標準誤差が出力されるが、\n# ここでは点推定値と95%信頼区間を出力する。\nmodelsummary(Raw_Fit1,\n             statistic = \"[{conf.low}, {conf.high}]\",\n             conf_level = 0.95)\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    6984.170 \n  \n  \n     \n    [6275.791, 7692.549] \n  \n  \n    treat \n    −635.026 \n  \n  \n     \n    [−1925.544, 655.492] \n  \n  \n    Num.Obs. \n    614 \n  \n  \n    R2 \n    0.002 \n  \n  \n    R2 Adj. \n    0.000 \n  \n  \n    AIC \n    12698.7 \n  \n  \n    BIC \n    12712.0 \n  \n  \n    F \n    0.934 \n  \n  \n    RMSE \n    7458.96 \n  \n\n\n\n\n\nこの直感的でない結果は、もしかしたらセレクションバイアスが原因かも知れません。職業訓練の対象が元々非常に所得が低い被験者になっている可能性があります。たとえば、下の図のように職業訓練の有無が教育水準や人種、これまでの所得などと関係しているとしましょう。これらの要因は回答者の現在所得にも関係していると考えられます。この場合、処置有無と所得の間には内生性が存在することになります。\n\n\n\n\n\n\n\n\n\n本当にそうなのか、共変量のバランスチェックをしてみましょう。もし、処置有無によって回答者の社会経済的要因に大きな差があれば、内生性が存在する証拠になります。ここでは誰かが作成しました{BalanceR}パッケージを使います。\n\nBlc_Chk <- la_df |>\n  BalanceR(group = treat, cov = c(age, educ, white:re75))\n\n{BalanceR}パッケージで共変量を指定する際、:演算子が使用可能です。white:re75は、データセットのwhiteからre75変数までをすべて指定することを意味します。Rコンソール上で変数名のみを出力するにはnames()関数を使用します。\n\nnames(la_df)\n\n [1] \"treat\"    \"age\"      \"educ\"     \"black\"    \"hispanic\" \"white\"   \n [7] \"married\"  \"nodegree\" \"re74\"     \"re75\"     \"re78\"    \n\n\nwhiteからre75までblack、hispanic、marriedなどの変数がありますが、これらを一々指定せず、:を使えば一気に指定することができます。それではバランスチェックの結果を確認してみましょう。\n\nBlc_Chk\n\n\n\n  Covariate   Mean:0     SD:0   Mean:1     SD:1  SB:0-1\n1       age   28.030   10.787   25.816    7.155  24.190\n2      educ   10.235    2.855   10.346    2.011  -4.476\n3     white    0.655    0.476    0.097    0.297 140.799\n4   married    0.513    0.500    0.189    0.393  72.076\n5  nodegree    0.597    0.491    0.708    0.456 -23.549\n6      re74 5619.237 6788.751 2095.574 4886.620  59.575\n7      re75 2466.484 3291.996 1532.055 3219.251  28.700\n\n\n\n\n \n  \n    Covariate \n    Mean:0 \n    SD:0 \n    Mean:1 \n    SD:1 \n    SB:0-1 \n  \n \n\n  \n    age \n    28.030 \n    10.787 \n    25.816 \n    7.155 \n    24.190 \n  \n  \n    educ \n    10.235 \n    2.855 \n    10.346 \n    2.011 \n    -4.476 \n  \n  \n    white \n    0.655 \n    0.476 \n    0.097 \n    0.297 \n    140.799 \n  \n  \n    married \n    0.513 \n    0.500 \n    0.189 \n    0.393 \n    72.076 \n  \n  \n    nodegree \n    0.597 \n    0.491 \n    0.708 \n    0.456 \n    -23.549 \n  \n  \n    re74 \n    5619.237 \n    6788.751 \n    2095.574 \n    4886.620 \n    59.575 \n  \n  \n    re75 \n    2466.484 \n    3291.996 \n    1532.055 \n    3219.251 \n    28.700 \n  \n\n\n\n\n\nいくつか怪しい箇所があります。たとえば、treat == 0の回答者において黒人の割合は約20%ですが、treat == 1のそれは約85%です。つまり、黒人ほどより職業訓練を受ける傾向があることを意味します。実際、標準化バイアスは-167という、非常に大きい数値を示しています。この結果を表としてまとめてみましょう。\n\n# 標準化バイアスの可視化\n# 絶対値変換。SB = 25に破線\nplot(Blc_Chk, abs = TRUE, vline = 25)\n\n\n\n\n\n\n\n\nかなり緩めの基準である25を採用しても、人種（黒人）、結婚有無、74年の所得のバランスが非常によくありません。内生性があると判断して良いでしょう。以下では様々な方法を使い、この内生性に対処していきたいと思います。"
  },
  {
    "objectID": "material/matching.html#重回帰分析",
    "href": "material/matching.html#重回帰分析",
    "title": "マッチング",
    "section": "重回帰分析",
    "text": "重回帰分析\n　まずは、重回帰分析からだ。用いる共変量は年齢、教育水準、黒人ダミー、ヒスパニックダミー2、既婚ダミー、学位なしダミー、74・75年の所得だ。lm()関数で78年の所得をこちらの変数に回帰させてみよう。\n\\[\n\\begin{align}\n\\widehat{\\mbox{re78}} = & \\beta_0 + \\beta_1 \\mbox{treat} + \\beta_2 \\mbox{age} + \\beta_3 \\mbox{educ} + \\\\\n& \\beta_4 \\mbox{black} + \\beta_5 \\mbox{hispanic} + \\beta_6 \\mbox{married} + \\beta_7 \\mbox{nodegree} + \\beta_8 \\mbox{re74} + \\beta_9 \\mbox{re75}.\n\\end{align}\n\\]\n\nmlm_fit <- lm(re78 ~ treat + age + educ + black + hispanic + married + \n                   nodegree + re74 + re75, data = la_df)\n\nmodelsummary(list(\"単回帰分析\" = DiM_fit, \"重回帰分析\" = mlm_fit), \n             estimate  = \"{estimate} ({std.error})\",\n             statistic = NULL,\n             gof_map   = c(\"nobs\", \"r.squared\", \"adj.r.squared\"))\n\n\n\n \n  \n      \n    単回帰分析 \n    重回帰分析 \n  \n \n\n  \n    (Intercept) \n    6984.170 (360.710) \n    66.515 (2436.746) \n  \n  \n    treat \n    −635.026 (657.137) \n    1548.244 (781.279) \n  \n  \n    age \n     \n    12.978 (32.489) \n  \n  \n    educ \n     \n    403.941 (158.906) \n  \n  \n    black \n     \n    −1240.644 (768.764) \n  \n  \n    hispanic \n     \n    498.897 (941.943) \n  \n  \n    married \n     \n    406.621 (695.472) \n  \n  \n    nodegree \n     \n    259.817 (847.442) \n  \n  \n    re74 \n     \n    0.296 (0.058) \n  \n  \n    re75 \n     \n    0.232 (0.105) \n  \n  \n    Num.Obs. \n    614 \n    614 \n  \n  \n    R2 \n    0.002 \n    0.148 \n  \n  \n    R2 Adj. \n    0.000 \n    0.135 \n  \n\n\n\n\n\n　共変量を統制したら処置変数の係数は約1548.244ドルだ。単回帰分析の結果とは違って、統計的に有意な正の効果が確認されている。ますます分からなくなってしまう。"
  },
  {
    "objectID": "material/matching.html#マハラノビス最近傍マッチング",
    "href": "material/matching.html#マハラノビス最近傍マッチング",
    "title": "マッチング",
    "section": "マハラノビス最近傍マッチング",
    "text": "マハラノビス最近傍マッチング\n重回帰分析は非常にシンプルで便利な分析方法ですが、いくつかの欠点があります。まず、重回帰分析は変数間の関係（線形結合）および誤差項の分布（平均0の正規分布）などを仮定したパラメトリック分析になります。この場合、同じ共変量を持たないケースであっても、勝手に予測を行います。重回帰分析における処置変数の解釈は「他の共変量がすべて同じ」場合の処置効果になります。これは、共変量がすべて同じ場合における（最初に見た）単純差分のようなものです。しかし、「他の共変量がすべて同じ」ケースが存在しない可能性があります。特に、共変量が多く、連続変数の場合、共変量がすべて同じことは実質あり得ないか、非常に少ないケースに限定されます。一方、マッチングを行うと、「他の共変量がすべて同じ」、または「非常に似ている」ケース間で比較を行います。\nここでは以下の4つの手法を紹介します。\n\nマッチング\n\nマハラノビス距離最近傍マッチング\nCoarsened Exact Matching (CEM)\n傾向スコアマッチング\n\n逆確率重み付け\n\nまずは、マハラノビス最近傍マッチングから始めましょう。だいたいのマッチング手法は{MatchIt}パッケージで解決できます。マッチングデータセットを作成する関数はmatchit()関数であり、使い方は\n\nmatchit(処置変数 ~ 共変量1 + ... + 共変量k, \n            data = データフレーム名, estimand = \"ATT\",\n            method = \"nearest\", distance = \"mahalanobis\")\n\nのように入力します。method = \"nearest\"は最近傍マッチングを意味し、distance = \"mahalanobis\"はマハラノビス距離を意味します。estimand = \"ATT\"はATTを推定することを意味します。{MatchIt}の最近傍マッチングの場合、\"ATT\"、または\"ATC\"のみ指定可能です。早速やってみましょう。\n\nMH_Matching <- matchit(treat ~ age + educ + black + hispanic + married + \n                           nodegree + re74 + re75, data = la_df,\n                       method = \"nearest\", distance = \"mahalanobis\",\n                       estimand = \"ATT\")\n\nバランスチェックは数値的に確認する方法と視覚的にする方法があります。まずは、数値的に確認してみましょう。\n\n# バランスチェック (Numerical)\nsummary(MH_Matching)\n\n\nCall:\nmatchit(formula = treat ~ age + educ + black + hispanic + married + \n    nodegree + re74 + re75, data = la_df, method = \"nearest\", \n    distance = \"mahalanobis\", estimand = \"ATT\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\nage            25.8162       28.0303         -0.3094     0.4400    0.0813\neduc           10.3459       10.2354          0.0550     0.4959    0.0347\nblack           0.8432        0.2028          1.7615          .    0.6404\nhispanic        0.0595        0.1422         -0.3498          .    0.0827\nmarried         0.1892        0.5128         -0.8263          .    0.3236\nnodegree        0.7081        0.5967          0.2450          .    0.1114\nre74         2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75         1532.0553     2466.4844         -0.2903     0.9563    0.1342\n         eCDF Max\nage        0.1577\neduc       0.1114\nblack      0.6404\nhispanic   0.0827\nmarried    0.3236\nnodegree   0.1114\nre74       0.4470\nre75       0.2876\n\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\nage            25.8162       24.9189          0.1254     0.5333    0.0692\neduc           10.3459       10.4378         -0.0457     0.8306    0.0111\nblack           0.8432        0.4649          1.0407          .    0.3784\nhispanic        0.0595        0.0595          0.0000          .    0.0000\nmarried         0.1892        0.2486         -0.1518          .    0.0595\nnodegree        0.7081        0.6595          0.1070          .    0.0486\nre74         2095.5737     3308.2167         -0.2482     0.8710    0.1016\nre75         1532.0553     1960.6355         -0.1331     1.0081    0.0662\n         eCDF Max Std. Pair Dist.\nage        0.2324          0.6210\neduc       0.0486          0.2930\nblack      0.3784          1.0407\nhispanic   0.0000          0.0000\nmarried    0.0595          0.1794\nnodegree   0.0486          0.1070\nre74       0.3405          0.4552\nre75       0.2216          0.4123\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\n表が3つでますが、上から(1)マッチング前の各共変量の情報、(2)マッチング後の各共変量の情報、(3)バランス改善の指標となります。(3)から見る場合、hispanic変数は改善率が100%となっています。実際、マッチング前は統制群と処置群におけるhispanic変数の平均値の差分は-0.0827でしたが、マッチング後は0となり、完全にマッチングされたことが分かります。他にも多くの共変量においてバランスの改善が見られます。eudcはむしろ悪化しましたが、他の変数はかなり改善されているので、このままいきましょう。\nつづいて視覚的な方法です。\n\n# バランスチェック (Graphical): QQプロットによるバランスチェック\nplot(MH_Matching)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n各ポイントが45度直線上に並ぶのが理想的なバランシングです。バランスが改善されていることが確認できます。\nもっと直感的なバランスチェックの方法として{BalanceR}パッケージと同様、標準化差分を使う方法です。マッチング前後のバランスを同時に確認するには{cobalt}パッケージのlove.plotが便利です。\n\nlove.plot(MH_Matching, thresholds = 0.25, abs = TRUE)\n\n\n\n\n\n\n\n\nthresholds引数は垂直線（破線）の位置、absは標準化差分を絶対値で示すことを意味します。マッチング後の標準化差分（Adjusted; 赤い点）が0.25より左側に位置している場合、バランスしていると判断できます2。他にもマッチング後の標準化差分がマッチング前（Unadjusted; 青い点）より改善されるいるか否かも判断できます。今回の例だと、大幅にバランスが改善されました。blackはまだバランスが取れておりませんが、それでも大幅に改善されていることが分かります。\nそれではATTを推定してみましょう。推定方法としてはノンパラメトリックな方法とパラメトリック方法がありますが、結果は変わりません。ノンパラメトリックな方法はペアごとの差分を計算し、その平均値を求める方法ですが、マッチング済みのデータに対し、処置変数を結果変数を回帰させることも、結果的には同じことを行うことになります。したがって、もっと簡単なパラメトリック方法でATTを推定します。ノンパラメトリック方法によるATTの推定は本資料の付録を参照してください。\n回帰分析を行うためにはデータが必要ですね。つまり、マッチングされないケースをデータから除去する必要があります。したがって、マッチングされた後のデータセットを抽出する必要があります。使う関数はmatch.data()関数です。\n\n# マッチング後データを抽出\nMH_Data <- match.data(MH_Matching)\n\nマッチングデータが取れたら、その中身を確認してみましょう。\n\ndim(MH_Data)\n\n[1] 370  13\n\n\n\nMH_Data\n\n\n\n\n\n\n\n\nデータのサイズは370行14列です。この370行には意味があります。それは処置群の大きさの2倍という点です。多くの場合、マッチングから計算される処置効果はATEではなく、ATTです。したがって、処置群のデータを100%活用し、統制群から共変量が最も近いケースを抽出&マッチングすることになります。だから、マッチング後のサンプルサイズは処置群のサイズの2倍になります。\nそれでは職業訓練のATTを推定します。方法は簡単です。このマッチング後のデータを用い、単回帰分析を行うのみです。\n\n# 処置効果の確認 (Model-Based)\nMH_Fit <- lm(re78 ~ treat, data = MH_Data)\n\nmodelsummary(MH_Fit)\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    5832.507 \n  \n  \n     \n    (527.987) \n  \n  \n    treat \n    516.637 \n  \n  \n     \n    (746.686) \n  \n  \n    Num.Obs. \n    370 \n  \n  \n    R2 \n    0.001 \n  \n  \n    R2 Adj. \n    −0.001 \n  \n  \n    AIC \n    7624.7 \n  \n  \n    BIC \n    7636.4 \n  \n  \n    F \n    0.479 \n  \n  \n    RMSE \n    7161.96 \n  \n\n\n\n\n\n処置効果は約516.637ドルであり、付録にあるノンパラメトリック推定値と一致していることが分かります。今回の結果は重回帰分析よりも推定値が低めであり、統計的に有意に職業訓練の効果があったとは言えないという結果が得られましたね。\nちなみに、{MatchIt}パッケージを使った最近傍マッチングのの結果は行う度に変化することがあります。なぜなら、{MatchIt}パッケージを使った最近傍マッチングの場合、処置群 (統制群)から一つのケースを選択し、最も近い統制群 (処置群)とマッチングさせます。マッチングされたケースは次のステップからはマッチング対象から除外されます3。また、1:1マッチングの場合4、同距離に複数のマッチング対象があると、ランダムに1つのみを選択します。最近傍マッチングを用いる際は、複数推定を行い、推定が安定するかを確認し、不安定な場合は他の手法を使うか、k-最近傍マッチングなどを使ってみましょう。"
  },
  {
    "objectID": "material/matching.html#傾向スコアマッチング",
    "href": "material/matching.html#傾向スコアマッチング",
    "title": "マッチング",
    "section": "傾向スコアマッチング",
    "text": "傾向スコアマッチング\n傾向スコアマッチングも、これまでのコマンドとほぼ同じです。マハラノビス最近傍マッチングのコマンドからdistance = ...引数を抜けば、傾向スコアマッチングができます7。また、replace引数にTRUEを指定します。これは一度マッチングしたケースを捨てずに、次のマッチングにも使うことを意味します。デフォルトはFALSEとなっており、この場合、一度マッチングされたケースは二度とマッチングされません。したがって、replace = TRUEを指定した方が共変量バランスがより改善されます。ただし、有効サンプルサイズ（Effective Sample Size; ESS）が小さくなり、精度が悪くなる点、場合によっては特殊な標準誤差8を使う必要があるといった欠点があります。これは最近傍マッチングに共通する問題ですが、ここではこれまでの結果と比較するために既定値のままにしましょう。\n\nPS_Matching <- matchit(treat ~ age + educ + black + hispanic + married + \n                           nodegree + re74 + re75, data = la_df,\n                       method = \"nearest\", estimand = \"ATT\")\n\nsummary(PS_Matching)\n\n\nCall:\nmatchit(formula = treat ~ age + educ + black + hispanic + married + \n    nodegree + re74 + re75, data = la_df, method = \"nearest\", \n    estimand = \"ATT\")\n\nSummary of Balance for All Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.1822          1.7941     0.9211    0.3774\nage            25.8162       28.0303         -0.3094     0.4400    0.0813\neduc           10.3459       10.2354          0.0550     0.4959    0.0347\nblack           0.8432        0.2028          1.7615          .    0.6404\nhispanic        0.0595        0.1422         -0.3498          .    0.0827\nmarried         0.1892        0.5128         -0.8263          .    0.3236\nnodegree        0.7081        0.5967          0.2450          .    0.1114\nre74         2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75         1532.0553     2466.4844         -0.2903     0.9563    0.1342\n         eCDF Max\ndistance   0.6444\nage        0.1577\neduc       0.1114\nblack      0.6404\nhispanic   0.0827\nmarried    0.3236\nnodegree   0.1114\nre74       0.4470\nre75       0.2876\n\n\nSummary of Balance for Matched Data:\n         Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance        0.5774        0.3629          0.9739     0.7566    0.1321\nage            25.8162       25.3027          0.0718     0.4568    0.0847\neduc           10.3459       10.6054         -0.1290     0.5721    0.0239\nblack           0.8432        0.4703          1.0259          .    0.3730\nhispanic        0.0595        0.2162         -0.6629          .    0.1568\nmarried         0.1892        0.2108         -0.0552          .    0.0216\nnodegree        0.7081        0.6378          0.1546          .    0.0703\nre74         2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75         1532.0553     1614.7451         -0.0257     1.4956    0.0452\n         eCDF Max Std. Pair Dist.\ndistance   0.4216          0.9740\nage        0.2541          1.3938\neduc       0.0757          1.2474\nblack      0.3730          1.0259\nhispanic   0.1568          1.0743\nmarried    0.0216          0.8281\nnodegree   0.0703          1.0106\nre74       0.2757          0.7965\nre75       0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\n処置効果の推定の前に、バランスが取れているかどうかを確認してみましょう。これまではQQプロットのみでバランスチェックをしましたが、今回はヒストグラムとJitterプロットで確認します。\n\n# 1. QQプロットによるバランスチェック\nplot(PS_Matching)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQQプロットを見ると、バランスが多少改善されていることが分かります。\n\n# 2. ヒストグラムによるバランスチェック\nplot(PS_Matching, type = \"hist\")\n\n\n\n\n\n\n\n\nヒストグラムからもバランスが改善されたことが確認できますね。出来れば、ヒストグラムを重ねて比較したいんですが、残念です。あとでオーバーラップされたヒストグラムの作成方法について解説します。\n\n# 3. 散布図によるバランスチェック\npar(mfrow = c(1, 1))\nplot(PS_Matching, type = \"jitter\")\n\n\n\n\n\n\n\n\n[1] \"To identify the units, use first mouse button; to stop, use second.\"\n\n\ninteger(0)\n\n\n散布図をみると、傾向スコア (\\(e\\))が小さいケースがマッチングから除外されたことが分かります。実際、処置群には傾向スコアが小さいケースが非常に少ないため、これは妥当な結果でしょう。\nむろん、おなじみのcobalt::love.plot()でもバランスチェックができます。\n\nlove.plot(PS_Matching, thresholds = 0.25, abs = TRUE)\n\n\n\n\n\n\n\n\nそれでは、ATT推定のためにマッチング後のデータを抽出します。\n\n# 傾向スコアマッチング後のデータセットを抽出\nPS_Data <- match.data(PS_Matching)\n\n傾向スコアを用いたATTをの推定もこれまでと同様、回帰分析を用います。普通の回帰分析ですが、用いるデータは傾向スコアでマッチングされたデータのみになります9。\n\n# 処置効果の推定\nPS_Fit <- lm(re78 ~ treat, data = PS_Data)\n\nmodelsummary(PS_Fit)\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    5454.776 \n  \n  \n     \n    (516.396) \n  \n  \n    treat \n    894.367 \n  \n  \n     \n    (730.295) \n  \n  \n    Num.Obs. \n    370 \n  \n  \n    R2 \n    0.004 \n  \n  \n    R2 Adj. \n    0.001 \n  \n  \n    AIC \n    7608.2 \n  \n  \n    BIC \n    7620.0 \n  \n  \n    F \n    1.500 \n  \n  \n    RMSE \n    7004.74 \n  \n\n\n\n\n\n処置効果は約894.367ドルでした。\nちなみに、講義スライド47〜48ページ目の例を{MatchIt}パッケージを使うと以下のようなコードになります。講義スライドでは一度マッチングされたケースを再利用したため、replace = TRUEを指定し、ATT/ATCを推定する際にweights =引数を指定する必要があります。\n\n# 講義スライドのデータ\nPS_df2 <- read_csv(\"data/matching_data2.csv\")\n\np47_mat <- matchit(Treat ~ Age + Edu, estimand = \"ATT\",\n                   data = PS_df2, method = \"nearest\", replace = TRUE)\np48_mat <- matchit(Treat ~ Age + Edu, estimand = \"ATC\",\n                   data = PS_df2, method = \"nearest\", replace = TRUE)\n\np47_data <- match.data(p47_mat)\np48_data <- match.data(p48_mat)\n\np47_fit <- lm(Outcome ~ Treat, data = p47_data, weights = weights)\np48_fit <- lm(Outcome ~ Treat, data = p48_data, weights = weights)\n\nmodelsummary(list(\"ATT\" = p47_fit, \"ATC\" = p48_fit))\n\n\n\n \n  \n      \n    ATT \n    ATC \n  \n \n\n  \n    (Intercept) \n    4.909 \n    4.308 \n  \n  \n     \n    (0.830) \n    (0.599) \n  \n  \n    Treat \n    1.818 \n    3.923 \n  \n  \n     \n    (1.031) \n    (1.012) \n  \n  \n    Num.Obs. \n    17 \n    20 \n  \n  \n    R2 \n    0.172 \n    0.455 \n  \n  \n    R2 Adj. \n    0.116 \n    0.425 \n  \n  \n    AIC \n    76.7 \n    93.8 \n  \n  \n    BIC \n    79.2 \n    96.8 \n  \n  \n    F \n    3.108 \n    15.031 \n  \n  \n    RMSE \n    1.81 \n    2.14 \n  \n\n\n\n\n\n手計算から得られた結果と一致することが分かります。"
  },
  {
    "objectID": "material/matching.html#ipw推定量",
    "href": "material/matching.html#ipw推定量",
    "title": "マッチング",
    "section": "IPW推定量",
    "text": "IPW推定量\n最後にATTでなく、ATEが推定の対象となるIPW推定量を計算してみましょう。{WeightIt}パッケージを使用すれば便利ですが、ここではあえて使わずにIPW推定量を計算してみましょう。こうすることでIPWが具体的にどのような仕組みで推定されるかが理解できるはずです。一通りの推定が終わりましたら{WeightIt}パッケージの使い方も紹介します。\n\nパッケージを使わない方法\nまずは、傾向スコアの算出ですが、ここではロジスティック回帰分析で傾向スコアを計算します10。\n\n# Logitで傾向スコアを推定\nPS <- glm(treat ~ age + educ + black + hispanic + married + nodegree +\n              re74 + re75, data = la_df,\n          family = binomial(\"logit\"))\n\n続いて、既存のデータフレームに傾向スコアが格納された列を追加します。まず、既存のlalondeデータセットをMatching.dfという名前で複製します。\n新しいデータフレームにPSという列を作成し、ここに傾向スコアを格納します。傾向スコアはロジスティック回帰分析のオブジェクトに$fitted.valuesを加えるだけで抽出できます。\n\n# lalondeデータフレームを複製\nipw_df <- la_df\n\n# 傾向スコア推定値をデータフレーム内に格納\nipw_df$PS <- PS$fitted.values\n\n次は、各ケースの重みを計算し、Wという新しい列として追加します。傾向スコアは、もしケースが処置群 (treat == 1)なら、\\(\\frac{1}{e}\\)を、統制群 (treat == 0)なら\\(\\frac{1}{(1-e)}\\)となります。\n\n# 重み変数を作成\n## ケースが処置群なら、重み = 傾向スコアの逆数\n## ケースが統制群あら、重み = (1 - 傾向スコア)の逆数\nipw_df <- ipw_df |>\n    mutate(W = ifelse(treat == 1, (1 / PS), (1 / (1 - PS))))\n\n以上のコードは以下のコードでも同じく作動します。\n\nipw_df <- ipw_df |>\n    mutate(W = treat * (1 / PS) + (1 - treat) * (1 / (1 - PS)))\n\nむろん、dplyrのmutate()を使わずに、以下のように入力しても重み変数は作成可能である。\n\nipw_df$W <- ifelse(ipw_df$treat == 1,\n                   1 / ipw_df$PS,\n                   1 / (1 - ipw_df$PS))\n\n最後に処置効果の推定です。CEMの時と同じやり方です。weight = ...引数を追加するだけです。\n\n# 回帰分析にweight = ...を指定するだけ\nIPW_fit <- lm(re78 ~ treat, data = ipw_df, weights = W)\n\nmodelsummary(IPW_fit)\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    6422.839 \n  \n  \n     \n    (397.427) \n  \n  \n    treat \n    224.676 \n  \n  \n     \n    (577.657) \n  \n  \n    Num.Obs. \n    614 \n  \n  \n    R2 \n    0.000 \n  \n  \n    R2 Adj. \n    −0.001 \n  \n  \n    AIC \n    12796.0 \n  \n  \n    BIC \n    12809.3 \n  \n  \n    F \n    0.151 \n  \n  \n    RMSE \n    7475.49 \n  \n\n\n\n\n\n推定された処置効果は約224.676ドルです。\n\n\n{WeightIt}パッケージを使用した場合\nそれでは{WeightIt}パッケージを使ってみましょう。これまで使ってきました{MatchIt}パッケージや傾向スコア算出のためのglm()関数と使い方が非常に似ています。まず、第一引数として処置変数を結果変数、処置有無に影響を与えると考えられる共変量を説明変数とした数式オブジェクト（formulaクラス）を指定します。続いて、データ（data）、IPW算出の方法（method）、推定の対象（estimand）を指定します。データはdata = la_dfとし、傾向スコアからIPWを算出するためmethod = \"ps\"を指定、最後にATT推定のためにestimand = \"ATT\"を指定します。ATEあるいはATCを計算する場合は\"ATT\"を適宜変更してください。また、IPW算出のために今回は傾向スコアを使いましたが、「処置を受ける確率」が計算できるなら何でも良いです。たとえば、Imai and Ratkovic (2014)11が推奨しているCovariate Balancing Propensity Score (CBPS) を使用する場合は\"ps\"の代わりに\"cbps\"を、複数の推定を組み合わせるスーパーラーニングをする場合は\"super\"12などが使えます。他にもエントロピーバランシングなど様々なオプションが提供されています。ここでは、先程の手法との比較のために、デフォルトの\"ps\"を使います。\n\nWeighted_Data <- weightit(treat ~ age + educ + black + hispanic + married +\n                            nodegree + re74 + re75, data = la_df,\n                          method = \"ps\", estimand = \"ATT\")\n\n{WeightIt}パッケージの便利なところはcobalt::love.plot()によるバランスチェックが簡単ということです。実際にやってみましょう。\n\nlove.plot(Weighted_Data, thresholds = 0.25, abs = TRUE)\n\n\n\n\n\n\n\n\n最後にIPW推定量を計算してみましょう。ここは先ほどの面倒くさい方法と同じですが、重み付け変数はweightit()関数から得られたオブジェクト（Weighted_Data）のweights列を使用します。推定ができたら結果を出力します。\n\nIPW_Result <- lm(re78 ~ treat, data = la_df,\n                 weights = Weighted_Data$weights)\n\nmodelsummary(IPW_Result)\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    5135.072 \n  \n  \n     \n    (401.194) \n  \n  \n    treat \n    1214.071 \n  \n  \n     \n    (568.904) \n  \n  \n    Num.Obs. \n    614 \n  \n  \n    R2 \n    0.007 \n  \n  \n    R2 Adj. \n    0.006 \n  \n  \n    AIC \n    13241.9 \n  \n  \n    BIC \n    13255.1 \n  \n  \n    F \n    4.554 \n  \n  \n    RMSE \n    7617.41 \n  \n\n\n\n\n\nという結果が得られましたが、これは{WeightIt}パッケージを使わなかった結果と完全に一致しますね。"
  },
  {
    "objectID": "material/matching.html#ノンパラメトリック推定",
    "href": "material/matching.html#ノンパラメトリック推定",
    "title": "マッチング",
    "section": "ノンパラメトリック推定",
    "text": "ノンパラメトリック推定\nここではマハラノビス最近傍マッチングの例を使って、ATTを推定してみましょう。まずは、処置群の潜在的結果を統制群から割り当てて差分を算出し、その平均値を求るというのがノンパラメトリックな推定方法です。以下のコードが理解できるようになれば、他の最近傍マッチング（傾向スコアマッチングなど）も外部パッケージを使わずに計算できるようになります。\n各処置群がどのケースとマッチングされたかはマッチングのオブジェクト名の後に$match.matrixを付けることで抽出できます。ちょっとやってみましょう。\n\n# 全部表示させると長くなんるので、最初の6行まで確認\nhead(MH_Matching$match.matrix) \n\n  [,1] \n1 \"553\"\n2 \"526\"\n3 \"601\"\n4 \"608\"\n5 \"585\"\n6 \"557\"\n\n\n左が処置群、右が統制群ですね。それぞれケースのIDを抽出しましょう。\n\n# MH.Matching$match.matrixは1列の行列です。\n# 処置群IDは行の名前として格納され、統制群IDは1列目に入っています。\nMH_Treat   <- as.vector(row.names(MH_Matching$match.matrix))\nMH_Control <- as.vector(MH_Matching$match.matrix[, 1])\n\n# 中身を確認\nprint(MH_Treat)\n\n  [1] \"1\"   \"2\"   \"3\"   \"4\"   \"5\"   \"6\"   \"7\"   \"8\"   \"9\"   \"10\"  \"11\"  \"12\" \n [13] \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\"  \"19\"  \"20\"  \"21\"  \"22\"  \"23\"  \"24\" \n [25] \"25\"  \"26\"  \"27\"  \"28\"  \"29\"  \"30\"  \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\" \n [37] \"37\"  \"38\"  \"39\"  \"40\"  \"41\"  \"42\"  \"43\"  \"44\"  \"45\"  \"46\"  \"47\"  \"48\" \n [49] \"49\"  \"50\"  \"51\"  \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"60\" \n [61] \"61\"  \"62\"  \"63\"  \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"70\"  \"71\"  \"72\" \n [73] \"73\"  \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"80\"  \"81\"  \"82\"  \"83\"  \"84\" \n [85] \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"90\"  \"91\"  \"92\"  \"93\"  \"94\"  \"95\"  \"96\" \n [97] \"97\"  \"98\"  \"99\"  \"100\" \"101\" \"102\" \"103\" \"104\" \"105\" \"106\" \"107\" \"108\"\n[109] \"109\" \"110\" \"111\" \"112\" \"113\" \"114\" \"115\" \"116\" \"117\" \"118\" \"119\" \"120\"\n[121] \"121\" \"122\" \"123\" \"124\" \"125\" \"126\" \"127\" \"128\" \"129\" \"130\" \"131\" \"132\"\n[133] \"133\" \"134\" \"135\" \"136\" \"137\" \"138\" \"139\" \"140\" \"141\" \"142\" \"143\" \"144\"\n[145] \"145\" \"146\" \"147\" \"148\" \"149\" \"150\" \"151\" \"152\" \"153\" \"154\" \"155\" \"156\"\n[157] \"157\" \"158\" \"159\" \"160\" \"161\" \"162\" \"163\" \"164\" \"165\" \"166\" \"167\" \"168\"\n[169] \"169\" \"170\" \"171\" \"172\" \"173\" \"174\" \"175\" \"176\" \"177\" \"178\" \"179\" \"180\"\n[181] \"181\" \"182\" \"183\" \"184\" \"185\"\n\nprint(MH_Control)\n\n  [1] \"553\" \"526\" \"601\" \"608\" \"585\" \"557\" \"537\" \"411\" \"515\" \"578\" \"540\" \"566\"\n [13] \"561\" \"493\" \"463\" \"552\" \"454\" \"462\" \"597\" \"374\" \"573\" \"405\" \"409\" \"559\"\n [25] \"596\" \"560\" \"574\" \"384\" \"577\" \"416\" \"586\" \"604\" \"593\" \"558\" \"571\" \"584\"\n [37] \"518\" \"516\" \"423\" \"438\" \"522\" \"422\" \"583\" \"576\" \"372\" \"381\" \"441\" \"451\"\n [49] \"413\" \"520\" \"403\" \"565\" \"364\" \"419\" \"478\" \"591\" \"511\" \"613\" \"581\" \"538\"\n [61] \"555\" \"358\" \"303\" \"343\" \"476\" \"530\" \"319\" \"432\" \"539\" \"344\" \"453\" \"580\"\n [73] \"342\" \"376\" \"295\" \"610\" \"445\" \"325\" \"279\" \"542\" \"282\" \"283\" \"546\" \"244\"\n [85] \"508\" \"285\" \"450\" \"459\" \"592\" \"562\" \"567\" \"536\" \"475\" \"525\" \"296\" \"430\"\n [97] \"455\" \"427\" \"527\" \"524\" \"365\" \"474\" \"367\" \"594\" \"443\" \"572\" \"556\" \"582\"\n[109] \"362\" \"458\" \"284\" \"551\" \"414\" \"470\" \"231\" \"507\" \"500\" \"226\" \"467\" \"501\"\n[121] \"415\" \"466\" \"436\" \"353\" \"510\" \"563\" \"339\" \"544\" \"512\" \"499\" \"446\" \"212\"\n[133] \"352\" \"402\" \"399\" \"209\" \"420\" \"390\" \"363\" \"312\" \"378\" \"306\" \"271\" \"457\"\n[145] \"297\" \"368\" \"398\" \"370\" \"275\" \"394\" \"356\" \"288\" \"327\" \"322\" \"323\" \"254\"\n[157] \"274\" \"273\" \"369\" \"393\" \"465\" \"541\" \"485\" \"191\" \"257\" \"607\" \"387\" \"281\"\n[169] \"233\" \"190\" \"193\" \"228\" \"335\" \"203\" \"278\" \"205\" \"280\" \"276\" \"290\" \"202\"\n[181] \"192\" \"208\" \"195\" \"186\" \"200\"\n\n\n次は、元のlalondeデータから処置群と統制群の結果変数 (re78)を抽出し、MH.Matched.dfというデータフレームとしてまとめます。続いて、各ケースのITEを計算します。\n\nMH_Matched_df <- tibble(\n    Treat_ID   = MH_Treat,\n    Control_ID = MH_Control,\n    Treat_Y    = lalonde[MH_Treat,]$re78,\n    Control_Y  = lalonde[MH_Control,]$re78\n)\n\nMH_Matched_df <- MH_Matched_df |>\n    mutate(ITE = Treat_Y - Control_Y)\n\n\nMH_Matched_df\n\n# A tibble: 185 × 5\n   Treat_ID Control_ID Treat_Y Control_Y     ITE\n   <chr>    <chr>        <dbl>     <dbl>   <dbl>\n 1 1        553          9930.        0   9930. \n 2 2        526          3596.    12618. -9022. \n 3 3        601         24909.        0  24909. \n 4 4        608          7506.     7544.   -37.6\n 5 5        585           290.      649.  -359. \n 6 6        557          4056.     6084. -2027. \n 7 7        537             0      2285. -2285. \n 8 8        411          8472.      702.  7770. \n 9 9        515          2164.      117.  2047. \n10 10       578         12418.    18757. -6339. \n# … with 175 more rows\n\n\n1行目を見ると1は553とマッチングされ、それぞれの結果変数は9930.046ドル、0ドルです。その差分がITEであり、9930.046ドルですね。\nこれらITEの平均値がATTとなります。早速計算してみましょう。\n\nMH_ATT <- mean(MH_Matched_df$ITE)\n\nMH_ATT\n\n[1] 516.6367\n\n\nATTは約516.637ドルでした。回帰分析でやった結果と一致しますね。大人しく回帰分析でやりましょう。"
  },
  {
    "objectID": "material/matching.html#複数の手法を用いた場合推定値の比較",
    "href": "material/matching.html#複数の手法を用いた場合推定値の比較",
    "title": "マッチング",
    "section": "複数の手法を用いた場合、推定値の比較",
    "text": "複数の手法を用いた場合、推定値の比較\n\natt_df <- bind_rows(list(\"単回帰_最近傍（マハラノビス）\" = tidy(mh_fit3, conf.int = TRUE),\n                         \"重回帰_最近傍（マハラノビス）\" = tidy(mh_fit4, conf.int = TRUE),\n                         \"単回帰_最近傍（傾向スコア）\"   = tidy(ps_fit1, conf.int = TRUE),\n                         \"重回帰_最近傍（傾向スコア）\"   = tidy(ps_fit2, conf.int = TRUE),\n                         \"単回帰_CEM\" = tidy(cem_fit1, conf.int = TRUE),\n                         \"重回帰_CEM\" = tidy(cem_fit2, conf.int = TRUE),\n                         \"単回帰_IPW\" = tidy(ipw_fit1, conf.int = TRUE),\n                         \"重回帰_IPW\" = tidy(ipw_fit2, conf.int = TRUE)),\n                    .id = \"Model\")\n\natt_df\n\n# A tibble: 48 × 8\n   Model                term   estim…¹ std.e…² statis…³  p.value conf.…⁴ conf.…⁵\n   <chr>                <chr>    <dbl>   <dbl>    <dbl>    <dbl>   <dbl>   <dbl>\n 1 単回帰_最近傍（マハ… (Inte…  5.74e3   855.   6.72    1.15e-10   4062.   7427.\n 2 単回帰_最近傍（マハ… treat   6.05e2  1013.   0.597   5.51e- 1  -1390.   2600.\n 3 重回帰_最近傍（マハ… (Inte…  1.37e3  4809.   0.285   7.76e- 1  -8099.  10843.\n 4 重回帰_最近傍（マハ… treat   5.24e2  1011.   0.518   6.05e- 1  -1468.   2516.\n 5 重回帰_最近傍（マハ… age     8.36e0    64.2  0.130   8.96e- 1   -118.    135.\n 6 重回帰_最近傍（マハ… educ    4.77e2   319.   1.49    1.37e- 1   -152.   1106.\n 7 重回帰_最近傍（マハ… black  -1.56e3  1569.  -0.996   3.20e- 1  -4652.   1527.\n 8 重回帰_最近傍（マハ… hispa… -1.76e1  2424.  -0.00726 9.94e- 1  -4791.   4756.\n 9 重回帰_最近傍（マハ… marri…  3.31e2  1285.   0.257   7.97e- 1  -2200.   2862.\n10 重回帰_最近傍（マハ… nodeg…  1.07e2  1421.   0.0751  9.40e- 1  -2692.   2906.\n# … with 38 more rows, and abbreviated variable names ¹​estimate, ²​std.error,\n#   ³​statistic, ⁴​conf.low, ⁵​conf.high\n# ℹ Use `print(n = ...)` to see more rows\n\n\n\natt_df <- att_df %>%\n  filter(term == \"treat\")\n\natt_df\n\n# A tibble: 8 × 8\n  Model                    term  estim…¹ std.e…² stati…³ p.value conf.…⁴ conf.…⁵\n  <chr>                    <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 単回帰_最近傍（マハラノ… treat    605.   1013.   0.597  0.551  -1390.    2600.\n2 重回帰_最近傍（マハラノ… treat    524.   1011.   0.518  0.605  -1468.    2516.\n3 単回帰_最近傍（傾向スコ… treat   1968.    971.   2.03   0.0438    55.3   3881.\n4 重回帰_最近傍（傾向スコ… treat   1903.    975.   1.95   0.0520   -17.0   3824.\n5 単回帰_CEM               treat   1071.   1248.   0.858  0.392  -1397.    3539.\n6 重回帰_CEM               treat   1207.   1246.   0.969  0.334  -1258.    3672.\n7 単回帰_IPW               treat   1214.    569.   2.13   0.0332    96.8   2331.\n8 重回帰_IPW               treat   1237.    555.   2.23   0.0261   148.    2327.\n# … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic,\n#   ⁴​conf.low, ⁵​conf.high\n\n\n\natt_df <- att_df %>%\n  separate(col  = Model,\n           into = c(\"Regression\", \"Method\"),\n           sep  = \"_\")\n\natt_df\n\n# A tibble: 8 × 9\n  Regression Method        term  estim…¹ std.e…² stati…³ p.value conf.…⁴ conf.…⁵\n  <chr>      <chr>         <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 単回帰     最近傍（マハ… treat    605.   1013.   0.597  0.551  -1390.    2600.\n2 重回帰     最近傍（マハ… treat    524.   1011.   0.518  0.605  -1468.    2516.\n3 単回帰     最近傍（傾向… treat   1968.    971.   2.03   0.0438    55.3   3881.\n4 重回帰     最近傍（傾向… treat   1903.    975.   1.95   0.0520   -17.0   3824.\n5 単回帰     CEM           treat   1071.   1248.   0.858  0.392  -1397.    3539.\n6 重回帰     CEM           treat   1207.   1246.   0.969  0.334  -1258.    3672.\n7 単回帰     IPW           treat   1214.    569.   2.13   0.0332    96.8   2331.\n8 重回帰     IPW           treat   1237.    555.   2.23   0.0261   148.    2327.\n# … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic,\n#   ⁴​conf.low, ⁵​conf.high\n\n\n\natt_df %>%\n  mutate(Regression = fct_inorder(Regression),\n         Method     = fct_inorder(Method)) %>%\n  ggplot() +\n  geom_pointrange(aes(x = estimate, y = Method,\n                      xmin = conf.low, xmax = conf.high,\n                      color = Regression),\n                  position = position_dodge2(1/2)) +\n  labs(x = \"処置群における処置効果（ATT）\", y = \"\", color = \"モデル\",\n       caption = \"注: マッチングの場合、復元マッチングを行った。\") +\n  theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\n\natt_df %>%\n  mutate(Regression = fct_inorder(Regression),\n         Method     = fct_inorder(Method),\n         Method     = fct_rev(Method)) %>%\n  ggplot() +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(x = estimate, y = Method,\n                      xmin = conf.low, xmax = conf.high,\n                      color = Regression),\n                  position = position_dodge2(1/2)) +\n  labs(x = \"処置群における処置効果（ATT）\", y = \"\", color = \"モデル\",\n       caption = \"注: マッチングの場合、復元マッチングを行った。\") +\n  theme_bw(base_size = 12)"
  },
  {
    "objectID": "material/matching.html#dim推定量",
    "href": "material/matching.html#dim推定量",
    "title": "マッチング",
    "section": "DiM推定量",
    "text": "DiM推定量\n　処置効果を確認するために、まずはグループごとの応答変数の差分（Difference-in-Means; DiM）を計算してみよう。処置変数はtreatであり、職業訓練を受けた回答者は1、受けなかった回答者は0となる。応答変数re78は1978年における回答者の収入である。\n\nDiff_Mean_df <- la_df |> \n    group_by(treat) |>\n    summarise(Outcome = mean(re78),\n              .groups = \"drop\")\n\nDiff_Mean_df\n\n# A tibble: 2 × 2\n  treat Outcome\n  <int>   <dbl>\n1     0   6984.\n2     1   6349.\n\n\n　この結果を可視化する必要はあまり無いかも知れないが、以下のようなコードで可視化することもできる。\n\nDiff_Mean_df |>\n  ggplot() +\n  geom_bar(aes(x = treat, y = Outcome), \n           stat = \"identity\", width = 0.5) +\n  geom_label(aes(x = treat, y = Outcome,\n                 label = round(Outcome, 3))) +\n  labs(x = \"Treatment\",\n       y = \"Outcome (US Dollars)\") +\n  # scale_x_continuous()を使って0/1をControl/Treatmentに置換する\n  # 目盛りはX軸上の0と1、各目盛りのラベルはControlとTreatmentに\n  scale_x_continuous(breaks = c(0, 1), labels = c(\"Control\", \"Treatment\")) +\n  coord_cartesian(xlim = c(-0.5, 1.5))\n\n\n\n\n\n\n\n\n　treat == 0の回答者、つまり職業訓練を受けていない回答者の平均所得は約6984ドル、treat == 1の回答者、つまり職業訓練を受けた回答者の平均所得は約6394ドルだ。その差は約-650ドルだが、職業訓練を受けた回答者の方が低所得になっている。これは直感的に納得できる結果ではないだろう。むろん、実際、職業訓練が所得を減らす可能性もあるが、今回の結果はより詳しく分析してみる価値があろう。\n　ちなみに、以上の結果は単回帰分析からも確認できる (ただし、統計的に有意ではない)。\n\nDiM_fit <- lm(re78 ~ treat, data = la_df)\nmodelsummary(DiM_fit,\n             # 係数の点推定値と95%信頼区間を示す場合\n             estimate   = \"{estimate} [{conf.low}, {conf.high}]\",\n             statistic  = NULL,\n             conf_level = 0.95,\n             # ケース数、決定係数、調整済み決定係数を出力\n             gof_map    = c(\"nobs\", \"r.squared\", \"adj.r.squared\"))\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    6984.170 [6275.791, 7692.549] \n  \n  \n    treat \n    −635.026 [−1925.544, 655.492] \n  \n  \n    Num.Obs. \n    614 \n  \n  \n    R2 \n    0.002 \n  \n  \n    R2 Adj. \n    0.000 \n  \n\n\n\n\n\n　この直感的でない結果は、もしかしたらセレクションバイアスが原因かも知れない。職業訓練の対象が元々非常に所得が低い回答者になっている可能性がある。たとえば、下の図のように職業訓練の有無が教育水準や人種、これまでの所得などと関係しているとしよう。これらの要因は回答者の現在所得にも関係していると考えられる。この場合、処置有無と所得の間には内生性が存在することになる。\n\n\n\n\n\n\n\n\n\n　本当にそうなのかを、共変量のバランスチェックをしてみよう。もし、処置有無によって回答者の社会経済的要因に大きな差があれば、内生性が存在する証拠になろう。ここでは誰かが作成しました{BalanceR}パッケージを使ってみよう。\n\nblc_chk <- la_df |>\n  BalanceR(group = treat, cov = age:re75)\n\n　{BalanceR}パッケージで共変量を指定する際、:演算子が使える。age:re75は、データセットのageからre75変数までをすべて指定することを意味する。names(la_df)で変数がどの順番で並んでいるかが分かる。\n\nnames(la_df)\n\n [1] \"treat\"    \"age\"      \"educ\"     \"black\"    \"hispanic\" \"white\"   \n [7] \"married\"  \"nodegree\" \"re74\"     \"re75\"     \"re78\"    \n\n\n　それではバランスチェックの結果を確認してみよう。\n\nblc_chk\n\n  Covariate   Mean:0     SD:0   Mean:1     SD:1   SB:0-1\n1       age   28.030   10.787   25.816    7.155   24.190\n2      educ   10.235    2.855   10.346    2.011   -4.476\n3     black    0.203    0.403    0.843    0.365 -167.083\n4  hispanic    0.142    0.350    0.059    0.237   27.740\n5     white    0.655    0.476    0.097    0.297  140.799\n6   married    0.513    0.500    0.189    0.393   72.076\n7  nodegree    0.597    0.491    0.708    0.456  -23.549\n8      re74 5619.237 6788.751 2095.574 4886.620   59.575\n9      re75 2466.484 3291.996 1532.055 3219.251   28.700\n\n\n　アンバランスと判定する標準化差分（標準化バイアス）の閾値には決まった値が無いが、最も緩い基準でも25程度である（計算時に100を掛けないのであれば0.25）。しかし、いくつか怪しい箇所がある。たとえば、treat == 0の回答者において黒人の割合は約20%だが、treat == 1のそれは約85%だ。つまり、黒人ほどより職業訓練を受ける傾向があることを意味する。また、人種は所得にも影響を与えると考えられる。これは処置と応答変数の間に交絡要因があることを意味する。実際、標準化バイアスは-167という、非常に大きい数値を示している。この結果を図としてまとめてみましょう。\n\n# 絶対値変換。SB = 25に破線\nplot(blc_chk, abs = TRUE, vline = 25) +\n  # 縦軸目盛りラベルの修正\n  scale_y_discrete(labels = c(\"age\"      = \"Age\",\n                              \"educ\"     = \"Education\",\n                              \"black\"    = \"Race (Black)\",\n                              \"hispanic\" = \"Race (Hispanic)\",\n                              \"white\"    = \"Race (White)\",\n                              \"married\"  = \"Married\",\n                              \"nodegree\" = \"No Degree\",\n                              \"re74\"     = \"Revenue (1974)\",\n                              \"re75\"     = \"Revenue (1975)\")) +\n  # 凡例の削除\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n　かなり緩めの基準である25を採用しても、人種、結婚有無、74・75年の所得のバランスが非常に悪く、内生性（=自己選択バイアス）があると判断して良いだろう。以下ではこの内生性に対処する様々な方法を紹介する。"
  },
  {
    "objectID": "material/matching.html#マッチング",
    "href": "material/matching.html#マッチング",
    "title": "マッチング",
    "section": "マッチング",
    "text": "マッチング\n\n最近傍マッチング\n　重回帰分析は非常にシンプルで便利な分析方法ですが、いくつかの欠点がある。まず、重回帰分析は変数間の関係（線形結合）および誤差項の分布（平均0の正規分布）などを仮定したパラメトリック分析ということだ。この場合、同じ共変量を持たないケースであっても、勝手に予測を行うこととなる。重回帰分析における処置変数の解釈は「他の共変量がすべて同じ」場合の処置効果である。これは、共変量がすべて同じ場合における（最初に見た）単純差分のようなものである。しかし、「他の共変量がすべて同じ」ケースが存在しない可能性があろう。特に、共変量が多く、連続変数の場合、共変量がすべて同じことは実質あり得ないか、非常に少ないケースに限定されることもある。一方、マッチングを行うと、「他の共変量がすべて同じ」、または「非常に似ている」ケース間で比較を行うことになる。\n　本資料では以下の3つのマッチング手法の実装方法について解説する。\n\n最近傍マッチング（マハラノビス距離）\n最近傍マッチング（傾向スコア）\nCoarsened Exact Matching (CEM)\n\n　まずは、マハラノビス距離を用いた最近傍マッチングから始めよう。だいたいのマッチング手法は{MatchIt}パッケージで解決できる。マッチングデータセットを作成する関数はmatchit()関数であり、使い方は以下の通りである。\n\nmatchit(処置変数 ~ 共変量1 + ... + 共変量k, \n            data = データフレーム名, estimand = \"ATT\",\n            method = \"nearest\", distance = \"mahalanobis\")\n\n　method = \"nearest\"は最近傍マッチングを、distance = \"mahalanobis\"はマハラノビス距離を意味する。estimand = \"ATT\"はATTを推定することを意味する。{MatchIt}の最近傍マッチングの場合、\"ATT\"、または\"ATC\"のみ指定可能である（後で紹介するCEMでは\"ATE\"も指定可能）。早速やってみよう。\n\nmh_mat1 <- matchit(treat ~ age + educ + black + hispanic + married + \n                     nodegree + re74 + re75, \n                   data = la_df, estimand = \"ATT\",\n                   method = \"nearest\", distance = \"mahalanobis\")\n\n　マッチング後のデータでバランスが取れているかを確認するためにはいくつかの方法があるが、ここでは{cobalt}パッケージを使って、標準化差分を確認してみよう。\n\nlove.plot(mh_mat1, thresholds = 0.25, abs = TRUE)\n\n\n\n\n\n\n\n\n　thresholds引数は垂直線（破線）の位置、absは標準化差分を絶対値で示すことを意味する。マッチング後の標準化差分（Adjusted; 赤い点）が0.25より左側に位置している場合、バランスしていると判断できる3。むろん、より厳格な基準として0.03、0.05、0.1を使うこともできる。他にもマッチング後の標準化差分がマッチング前（Unadjusted; 青い点）より改善されるいるか否かも判断できる。今回の例だと、大幅にバランスが改善されている。0.25を基準とした場合、blackはまだバランスが取れていないが、それでも大幅に改善されていることが分かる。\n　それではATTを推定してみよう。推定方法としてはノンパラメトリックな方法とパラメトリック方法があるが、結果は変わらない。ノンパラメトリックな方法はペアごとの差分を計算し、その平均値を求める方法だが、マッチング済みのデータに対し、処置変数を結果変数を回帰させることも、結果的には同じことを行うことになる。したがって、もっと簡単なパラメトリック方法、つまり単回帰分析でATTを推定しよう。\n　回帰分析を行うためにはデータが必要だ。つまり、マッチングされないケースをデータから除去する必要がある。ここではmatch.data()関数を使ったマッチングされたケースのみを抽出してみよう。抽出したデータはmh_data1と名付ける。\n\nmh_data1 <- match.data(mh_mat1)\n\nマッチングデータが取れたら、その中身を確認してみましょう。\n\nmh_data1\n\n# A tibble: 370 × 13\n   treat   age  educ black hispanic white married nodegree  re74  re75   re78\n   <int> <int> <int> <int>    <int> <int>   <int>    <int> <dbl> <dbl>  <dbl>\n 1     1    37    11     1        0     0       1        1     0     0  9930.\n 2     1    22     9     0        1     0       0        1     0     0  3596.\n 3     1    30    12     1        0     0       0        0     0     0 24909.\n 4     1    27    11     1        0     0       0        1     0     0  7506.\n 5     1    33     8     1        0     0       0        1     0     0   290.\n 6     1    22     9     1        0     0       0        1     0     0  4056.\n 7     1    23    12     1        0     0       0        0     0     0     0 \n 8     1    32    11     1        0     0       0        1     0     0  8472.\n 9     1    22    16     1        0     0       0        0     0     0  2164.\n10     1    33    12     0        0     1       1        0     0     0 12418.\n# … with 360 more rows, and 2 more variables: weights <dbl>, subclass <fct>\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n　データのサイズは370行14列であり、この370行には意味がある。それは処置群の大きさの2倍という点だ。多くの場合、マッチングから計算される処置効果はATEではなく、ATTである。したがって、処置群のデータを100%活用し、共変量（のマハラノビス距離）が最も近いケースを統制群から抽出&マッチングすることになる。だから、マッチング後のサンプルサイズは処置群のサイズの2倍になる。\n　それでは職業訓練のATTを推定してみよう。方法は簡単だ。マッチング後のデータ（mh_data1）を用い、単回帰分析を行うだけである。\n\nmh_fit1 <- lm(re78 ~ treat, data = mh_data1)\n\nmodelsummary(mh_fit1)\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    5832.507 \n  \n  \n     \n    (527.987) \n  \n  \n    treat \n    516.637 \n  \n  \n     \n    (746.686) \n  \n  \n    Num.Obs. \n    370 \n  \n  \n    R2 \n    0.001 \n  \n  \n    R2 Adj. \n    −0.001 \n  \n  \n    AIC \n    7624.7 \n  \n  \n    BIC \n    7636.4 \n  \n  \n    Log.Lik. \n    −3809.327 \n  \n  \n    F \n    0.479 \n  \n  \n    RMSE \n    7161.96 \n  \n\n\n\n\nmodelsummary(list(\"単回帰・非復元\" = mh_fit1), \n             estimate  = \"{estimate} ({std.error})\",\n             statistic = NULL,\n             gof_map   = c(\"nobs\", \"r.squared\", \"adj.r.squared\"))\n\n\n\n \n  \n      \n    単回帰・非復元 \n  \n \n\n  \n    (Intercept) \n    5832.507 (527.987) \n  \n  \n    treat \n    516.637 (746.686) \n  \n  \n    Num.Obs. \n    370 \n  \n  \n    R2 \n    0.001 \n  \n  \n    R2 Adj. \n    −0.001 \n  \n\n\n\n\n\n　処置効果は約516.637ドルである。今回の結果は重回帰分析よりも推定値が低めであり、統計的に有意に職業訓練の効果があったとは言えないという結果が得られましたね。また、マッチング後のデータを使って重回帰分析を行うこともできる。マッチング後のデータを見ると、黒人ダミーのバランスは大幅に改善されたが、それでもまだアンバランスしていると言える。他にも、74・75年の所得や年齢もそれなりに標準化差分が大きい。このような場合、もう一度共変量を投入して分析を行うこともできる。\n\nmh_fit2 <- lm(re78 ~ treat + age + educ + black + hispanic + married + \n                   nodegree + re74 + re75, data = mh_data1)\n\nmodelsummary(list(\"単回帰・非復元\" = mh_fit1,\n                  \"重回帰・非復元\" = mh_fit2), \n             gof_map   = c(\"nobs\", \"r.squared\", \"adj.r.squared\"))\n\n\n\n \n  \n      \n    単回帰・非復元 \n    重回帰・非復元 \n  \n \n\n  \n    (Intercept) \n    5832.507 \n    −453.365 \n  \n  \n     \n    (527.987) \n    (3546.028) \n  \n  \n    treat \n    516.637 \n    1239.696 \n  \n  \n     \n    (746.686) \n    (812.039) \n  \n  \n    age \n     \n    15.112 \n  \n  \n     \n     \n    (45.720) \n  \n  \n    educ \n     \n    533.042 \n  \n  \n     \n     \n    (243.621) \n  \n  \n    black \n     \n    −1158.148 \n  \n  \n     \n     \n    (906.948) \n  \n  \n    hispanic \n     \n    1120.562 \n  \n  \n     \n     \n    (1676.953) \n  \n  \n    married \n     \n    653.748 \n  \n  \n     \n     \n    (984.773) \n  \n  \n    nodegree \n     \n    −170.705 \n  \n  \n     \n     \n    (1116.240) \n  \n  \n    re74 \n     \n    0.071 \n  \n  \n     \n     \n    (0.098) \n  \n  \n    re75 \n     \n    0.272 \n  \n  \n     \n     \n    (0.155) \n  \n  \n    Num.Obs. \n    370 \n    370 \n  \n  \n    R2 \n    0.001 \n    0.076 \n  \n  \n    R2 Adj. \n    −0.001 \n    0.053 \n  \n\n\n\n\n\n　ちなみに、{MatchIt}パッケージを使った最近傍マッチングのの結果は行う度に変化することがある。{MatchIt}パッケージを使った最近傍マッチングの場合、処置群 (統制群)から一つのケースを選択し、最も近い統制群 (処置群)とマッチングする。マッチングされたケースは次のステップからはマッチング対象から除外されることになる4。また、1:1マッチングの場合5、同距離に複数のマッチング対象があると、ランダムに1つのみを選択する。最近傍マッチングを用いる際は、複数推定を行い、推定が安定するかを確認し、不安定な場合は他の手法を使うか、k-最近傍マッチングなどを使ってみよう。\n　ここでは復元マッチングの例を紹介しよう。やり方はmatchit()内にreplace = TRUEを追加するだけだ。\n\nmh_mat2 <- matchit(treat ~ age + educ + black + hispanic + married + \n                     nodegree + re74 + re75, \n                   data = la_df, estimand = \"ATT\", replace = TRUE,\n                   method = \"nearest\", distance = \"mahalanobis\")\nmh_data2 <- match.data(mh_mat2)\n\n　マッチング後のデータを確認する前に、バランスチェックをしてみよう。\n\nlove.plot(mh_mat2, thresholds = 0.25, abs = TRUE)\n\n\n\n\n\n\n\n\n　復元マッチングのメリットは非復元マッチングに比べ、バランス改善の程度が大きいという点だ。非復元マッチングの場合、マッチングに使われた統制群は二度と使われないため、場合によっては近いマッチングケースがあるにも関わらず、マッチングできないからだ。ただし、復元マッチングにもデメリットはある。たとえば、有効サンプルサイズ（Effective Sample Size; ESS）が小さくなり、精度が悪くなる点、場合によっては特殊な標準誤差6を使う必要があるといった欠点もある。\n　それではマッチング後のデータを確認してみよう。\n\nmh_data2\n\n# A tibble: 260 × 12\n   treat   age  educ black hispanic white married nodegree  re74  re75   re78\n   <int> <int> <int> <int>    <int> <int>   <int>    <int> <dbl> <dbl>  <dbl>\n 1     1    37    11     1        0     0       1        1     0     0  9930.\n 2     1    22     9     0        1     0       0        1     0     0  3596.\n 3     1    30    12     1        0     0       0        0     0     0 24909.\n 4     1    27    11     1        0     0       0        1     0     0  7506.\n 5     1    33     8     1        0     0       0        1     0     0   290.\n 6     1    22     9     1        0     0       0        1     0     0  4056.\n 7     1    23    12     1        0     0       0        0     0     0     0 \n 8     1    32    11     1        0     0       0        1     0     0  8472.\n 9     1    22    16     1        0     0       0        0     0     0  2164.\n10     1    33    12     0        0     1       1        0     0     0 12418.\n# … with 250 more rows, and 1 more variable: weights <dbl>\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n　今回は370行ではないことが分かる。なぜなら統制群のケースが複数マッチングされることもあるからだ。処置群は100%使われるので、マッチングに使われた統制群のケースは260-185=75ケースである。この特徴により推定の際は一点、注意が必要である。推定のやり方自体はほぼ同じである。しかし、非復元マッチングの場合、統制群からマッチングされたケースは1回のみ使われるため、一つ一つのケースの重みは同じである。match.data()から得られーたデータにはweights列が含まれており、mh_data1のweights列を見ると全ての重みが1だということが分かる。\n\nmh_data1$weights\n\n  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n 81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n181 182 183 184 185 186 190 191 192 193 195 200 202 203 205 208 209 212 226 228 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n231 233 244 254 257 271 273 274 275 276 278 279 280 281 282 283 284 285 288 290 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n295 296 297 303 306 312 319 322 323 325 327 335 339 342 343 344 352 353 356 358 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n362 363 364 365 367 368 369 370 372 374 376 378 381 384 387 390 393 394 398 399 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n402 403 405 409 411 413 414 415 416 419 420 422 423 427 430 432 436 438 441 443 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n445 446 450 451 453 454 455 457 458 459 462 463 465 466 467 470 474 475 476 478 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n485 493 499 500 501 507 508 510 511 512 515 516 518 520 522 524 525 526 527 530 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n536 537 538 539 540 541 542 544 546 551 552 553 555 556 557 558 559 560 561 562 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n563 565 566 567 571 572 573 574 576 577 578 580 581 582 583 584 585 586 591 592 \n  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1 \n593 594 596 597 601 604 607 608 610 613 \n  1   1   1   1   1   1   1   1   1   1 \n\n\n　一方、復元マッチングの場合、一つのケースが複数回マッチングされる場合もある。たとえば、191番目のケースは統制群であるが、重みが1.2162162だ。この意味は191番目のケースは計3回（\\(1.2162162\\times\\frac{185}{75}\\)）マッチングに使われたことを意味する。\n\nmh_data2$weights\n\n        1         2         3         4         5         6         7         8 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n        9        10        11        12        13        14        15        16 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       17        18        19        20        21        22        23        24 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       25        26        27        28        29        30        31        32 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       33        34        35        36        37        38        39        40 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       41        42        43        44        45        46        47        48 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       49        50        51        52        53        54        55        56 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       57        58        59        60        61        62        63        64 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       65        66        67        68        69        70        71        72 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       73        74        75        76        77        78        79        80 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       81        82        83        84        85        86        87        88 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       89        90        91        92        93        94        95        96 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n       97        98        99       100       101       102       103       104 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      105       106       107       108       109       110       111       112 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      113       114       115       116       117       118       119       120 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      121       122       123       124       125       126       127       128 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      129       130       131       132       133       134       135       136 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      137       138       139       140       141       142       143       144 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      145       146       147       148       149       150       151       152 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      153       154       155       156       157       158       159       160 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      161       162       163       164       165       166       167       168 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      169       170       171       172       173       174       175       176 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      177       178       179       180       181       182       183       184 \n1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 \n      185       191       202       244       257       280       281       282 \n1.0000000 1.2162162 0.4054054 0.4054054 0.4054054 0.4054054 0.4054054 0.4054054 \n      284       295       297       303       312       319       325       335 \n1.6216216 0.8108108 0.4054054 3.6486486 0.4054054 1.6216216 2.4324324 0.4054054 \n      343       344       353       362       364       384       387       403 \n0.4054054 0.8108108 0.4054054 0.4054054 0.8108108 0.4054054 0.4054054 0.8108108 \n      405       409       411       413       420       422       423       432 \n0.4054054 0.4054054 2.0270270 0.8108108 0.4054054 0.4054054 0.4054054 0.4054054 \n      438       450       451       454       463       475       476       493 \n1.6216216 0.4054054 0.8108108 1.2162162 0.4054054 0.4054054 0.4054054 1.2162162 \n      507       511       512       515       516       518       520       524 \n0.4054054 0.4054054 0.4054054 0.4054054 0.8108108 0.8108108 0.8108108 0.4054054 \n      526       530       537       538       539       540       546       551 \n0.4054054 0.4054054 3.2432432 0.8108108 0.4054054 0.8108108 0.4054054 0.4054054 \n      552       553       557       558       559       561       565       566 \n1.6216216 5.2702703 0.8108108 2.8378378 1.2162162 1.6216216 0.4054054 0.4054054 \n      573       576       577       578       584       585       592       597 \n2.4324324 0.4054054 1.6216216 0.4054054 1.2162162 1.2162162 0.8108108 0.8108108 \n      601       604       608       613 \n1.2162162 0.4054054 7.2972973 0.4054054 \n\n\n　したがって、復元マッチングの場合、lm()内にweights引数を必ず指定する必要がある。\n\nmh_fit3 <- lm(re78 ~ treat, \n              data = mh_data2, weights = weights)\nmh_fit4 <- lm(re78 ~ treat + age + educ + black + hispanic + married + \n                   nodegree + re74 + re75, \n              data = mh_data2, weights = weights)\n\nmodelsummary(list(\"単回帰・非復元\" = mh_fit1,\n                  \"重回帰・非復元\" = mh_fit2,\n                  \"単回帰・復元\"   = mh_fit3,\n                  \"重回帰・復元\"   = mh_fit4), \n             gof_map   = c(\"nobs\", \"r.squared\", \"adj.r.squared\"))\n\n\n\n \n  \n      \n    単回帰・非復元 \n    重回帰・非復元 \n    単回帰・復元 \n    重回帰・復元 \n  \n \n\n  \n    (Intercept) \n    5832.507 \n    −453.365 \n    5744.482 \n    1371.865 \n  \n  \n     \n    (527.987) \n    (3546.028) \n    (854.641) \n    (4808.980) \n  \n  \n    treat \n    516.637 \n    1239.696 \n    604.661 \n    524.037 \n  \n  \n     \n    (746.686) \n    (812.039) \n    (1013.175) \n    (1011.417) \n  \n  \n    age \n     \n    15.112 \n     \n    8.365 \n  \n  \n     \n     \n    (45.720) \n     \n    (64.228) \n  \n  \n    educ \n     \n    533.042 \n     \n    476.666 \n  \n  \n     \n     \n    (243.621) \n     \n    (319.397) \n  \n  \n    black \n     \n    −1158.148 \n     \n    −1562.414 \n  \n  \n     \n     \n    (906.948) \n     \n    (1568.526) \n  \n  \n    hispanic \n     \n    1120.562 \n     \n    −17.586 \n  \n  \n     \n     \n    (1676.953) \n     \n    (2423.801) \n  \n  \n    married \n     \n    653.748 \n     \n    330.704 \n  \n  \n     \n     \n    (984.773) \n     \n    (1285.183) \n  \n  \n    nodegree \n     \n    −170.705 \n     \n    106.692 \n  \n  \n     \n     \n    (1116.240) \n     \n    (1421.140) \n  \n  \n    re74 \n     \n    0.071 \n     \n    0.091 \n  \n  \n     \n     \n    (0.098) \n     \n    (0.134) \n  \n  \n    re75 \n     \n    0.272 \n     \n    0.193 \n  \n  \n     \n     \n    (0.155) \n     \n    (0.208) \n  \n  \n    Num.Obs. \n    370 \n    370 \n    260 \n    260 \n  \n  \n    R2 \n    0.001 \n    0.076 \n    0.001 \n    0.041 \n  \n  \n    R2 Adj. \n    −0.001 \n    0.053 \n    −0.002 \n    0.007 \n  \n\n\n\n\n\n　推定の結果は、いずれも正であり、職業訓練は所得に正の影響を与えるという結果が得られている。しかし、いずれも標準誤差が非常に大きく、統計的に有意な結果は得られていない。\n\n\n傾向スコア\n　傾向スコアマッチングも、これまでのコードとほぼ同じだ。マハラノビス最近傍マッチングのコマンドからdistance = ...引数を抜けば、傾向スコアマッチングができる7。ここでもreplace = TRUEを指定し、復元マッチングをやってみよう。\n\nps_mat <- matchit(treat ~ age + educ + black + hispanic + married + \n                    nodegree + re74 + re75, \n                  data = la_df, replace = TRUE,\n                  method = \"nearest\", estimand = \"ATT\")\n\n　続いて、バランスチェックをしよう。\n\nlove.plot(ps_mat, thresholds = 0.25, abs = TRUE)\n\n\n\n\n\n\n\n\n　バランスが大幅に改善されていることが分かる。ちなみに最上段のdistanceは傾向スコアを意味する。\n　それでは、ATT推定のためにマッチング後のデータを抽出しよう。\n\n# 傾向スコアマッチング後のデータセットを抽出\nps_data <- match.data(ps_mat)\n\n　傾向スコアを用いたATTをの推定もこれまでと同様、回帰分析を使用する。ここでも共変量なしの単回帰とありの重回帰を行い、マハラノビス距離最近傍マッチング（復元）と結果を比べてみよう。。\n\n# 処置効果の推定\nps_fit1 <- lm(re78 ~ treat, \n             data = ps_data, weights = weights)\nps_fit2 <- lm(re78 ~ treat + age + educ + black + hispanic + \n                married + nodegree + re74 + re75, \n             data = ps_data, weights = weights)\n\nmodelsummary(list(\"MH (単回帰)\"   = mh_fit3,\n                  \"MH (重回帰)\"   = mh_fit4,\n                  \"PS (単回帰)\"   = ps_fit1,\n                  \"PS (重回帰)\"   = ps_fit2), \n             gof_map   = c(\"nobs\", \"r.squared\", \"adj.r.squared\"))\n\n\n\n \n  \n      \n    MH (単回帰) \n    MH (重回帰) \n    PS (単回帰) \n    PS (重回帰) \n  \n \n\n  \n    (Intercept) \n    5744.482 \n    1371.865 \n    4381.204 \n    −985.292 \n  \n  \n     \n    (854.641) \n    (4808.980) \n    (811.618) \n    (4370.562) \n  \n  \n    treat \n    604.661 \n    524.037 \n    1967.940 \n    1903.412 \n  \n  \n     \n    (1013.175) \n    (1011.417) \n    (971.379) \n    (975.164) \n  \n  \n    age \n     \n    8.365 \n     \n    42.898 \n  \n  \n     \n     \n    (64.228) \n     \n    (60.174) \n  \n  \n    educ \n     \n    476.666 \n     \n    444.829 \n  \n  \n     \n     \n    (319.397) \n     \n    (278.130) \n  \n  \n    black \n     \n    −1562.414 \n     \n    −893.748 \n  \n  \n     \n     \n    (1568.526) \n     \n    (1538.965) \n  \n  \n    hispanic \n     \n    −17.586 \n     \n    −184.643 \n  \n  \n     \n     \n    (2423.801) \n     \n    (2343.260) \n  \n  \n    married \n     \n    330.704 \n     \n    299.425 \n  \n  \n     \n     \n    (1285.183) \n     \n    (1292.304) \n  \n  \n    nodegree \n     \n    106.692 \n     \n    92.911 \n  \n  \n     \n     \n    (1421.140) \n     \n    (1379.407) \n  \n  \n    re74 \n     \n    0.091 \n     \n    0.056 \n  \n  \n     \n     \n    (0.134) \n     \n    (0.122) \n  \n  \n    re75 \n     \n    0.193 \n     \n    0.160 \n  \n  \n     \n     \n    (0.208) \n     \n    (0.200) \n  \n  \n    Num.Obs. \n    260 \n    260 \n    265 \n    265 \n  \n  \n    R2 \n    0.001 \n    0.041 \n    0.015 \n    0.051 \n  \n  \n    R2 Adj. \n    −0.002 \n    0.007 \n    0.012 \n    0.018 \n  \n\n\n\n\n\n　傾向スコアマッチングでも正の処置効果（ATT）が確認され、今回は統計的に有意な結果が得られている。\n\n\nCEM\n　Coarsened Exact Matching（CEM）はマハラノビス最近傍マッチング同様、matchit()関数を使うが、事前に{cem}パッケージをインストールしておく必要がある（install.pacakges(\"cem\")）。\n　CEMのようなExact Matching類の手法は距離を図る必要がないので、distance引数は不要である。マッチング方法を指定するmethod引数はこれまで使ってきた\"nearest\"（最近傍）でなく、\"cem\"に替えよう。推定可能な処置効果はATE（最近傍マッチングでは指定できなかったもの）、ATT、ATCであるが、ここではATTを推定してみよう。\n　マッチングをしたらmatch.data()でマッチングされたデータを抽出する。\n\ncem_mat <- matchit(treat ~ age + educ + black + hispanic + married + \n                     nodegree + re74 + re75, data = la_df,\n                   method = \"cem\", estimand = \"ATT\")\n\n　つづいて、{cobalt}のlove.plot()を使用して、バランスチェックを行う。\n\nlove.plot(cem_mat, thresholds = 0.25, abs = TRUE)\n\n\n\n\n\n\n\n\n　CEMの場合、（非復元）最近傍マッチングよりもバランスが大きく改善されることが分かる。その理由は簡単だ。最近傍マッチングの場合、最も近いケースであれば、どれほど離れていてもマッチングされる。一方、CEMは正確マッチングの一種であるため、ある程度離れているケースを捨ててしまうため、結局は共変量が非常に近いケースのみを残すことになります。\n　それでは、match.data()関数を使ってマッチング後のデータを抽出してみよう。\n\ncem_data <- match.data(cem_mat)\n\ncem_data\n\n# A tibble: 140 × 13\n   treat   age  educ black hispanic white married nodegree  re74  re75   re78\n   <int> <int> <int> <int>    <int> <int>   <int>    <int> <dbl> <dbl>  <dbl>\n 1     1    22     9     0        1     0       0        1     0     0  3596.\n 2     1    27    11     1        0     0       0        1     0     0  7506.\n 3     1    22     9     1        0     0       0        1     0     0  4056.\n 4     1    23    12     1        0     0       0        0     0     0     0 \n 5     1    22    16     1        0     0       0        0     0     0  2164.\n 6     1    19     9     1        0     0       0        1     0     0  8174.\n 7     1    21    13     1        0     0       0        0     0     0 17095.\n 8     1    18     8     1        0     0       0        1     0     0     0 \n 9     1    17     7     1        0     0       0        1     0     0  3024.\n10     1    19    10     1        0     0       0        1     0     0  3229.\n# … with 130 more rows, and 2 more variables: weights <dbl>, subclass <fct>\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n　CEMの場合、マッチングされないブロックは捨てられるため、マハラノビス距離最近傍マッチングよりもサンプルサイズが小さくなりやすい。マッチング相手がなければ、たとえ処置群だとしても除外される。また、処置群と統制群のサンプルサイズも不均衡になる。マッチング結果を見ると、処置群からは65ケース、統制群からは75サンプルのみ残っている。\n\ncem_data |>\n  count(treat)\n\n# A tibble: 2 × 2\n  treat     n\n  <int> <int>\n1     0    75\n2     1    65\n\n\n　処置効果はこれまでの復元マッチングと同様、重み付き回帰分析で推定するｙ。ここでも共変量ありとなし、2パターンで推定してみよう。\n\ncem_fit1 <- lm(re78 ~ treat, \n               data = cem_data, weights = weights)\ncem_fit2 <- lm(re78 ~ treat + age + educ + black + hispanic +\n                 married + nodegree + re74 + re75, \n               data = cem_data, weights = weights)\n\nmodelsummary(list(\"MH (単回帰)\"   = mh_fit3,\n                  \"MH (重回帰)\"   = mh_fit4,\n                  \"PS (単回帰)\"   = ps_fit1,\n                  \"PS (重回帰)\"   = ps_fit2,\n                  \"CEM (単回帰)\"  = cem_fit1,\n                  \"CEM (重回帰)\"  = cem_fit2), \n             gof_map   = c(\"nobs\", \"r.squared\", \"adj.r.squared\"))\n\n\n\n \n  \n      \n    MH (単回帰) \n    MH (重回帰) \n    PS (単回帰) \n    PS (重回帰) \n    CEM (単回帰) \n    CEM (重回帰) \n  \n \n\n  \n    (Intercept) \n    5744.482 \n    1371.865 \n    4381.204 \n    −985.292 \n    5265.785 \n    −7253.717 \n  \n  \n     \n    (854.641) \n    (4808.980) \n    (811.618) \n    (4370.562) \n    (850.457) \n    (7031.576) \n  \n  \n    treat \n    604.661 \n    524.037 \n    1967.940 \n    1903.412 \n    1070.907 \n    1207.183 \n  \n  \n     \n    (1013.175) \n    (1011.417) \n    (971.379) \n    (975.164) \n    (1248.129) \n    (1245.959) \n  \n  \n    age \n     \n    8.365 \n     \n    42.898 \n     \n    147.970 \n  \n  \n     \n     \n    (64.228) \n     \n    (60.174) \n     \n    (107.716) \n  \n  \n    educ \n     \n    476.666 \n     \n    444.829 \n     \n    879.978 \n  \n  \n     \n     \n    (319.397) \n     \n    (278.130) \n     \n    (472.982) \n  \n  \n    black \n     \n    −1562.414 \n     \n    −893.748 \n     \n    −1941.465 \n  \n  \n     \n     \n    (1568.526) \n     \n    (1538.965) \n     \n    (2485.215) \n  \n  \n    hispanic \n     \n    −17.586 \n     \n    −184.643 \n     \n    437.208 \n  \n  \n     \n     \n    (2423.801) \n     \n    (2343.260) \n     \n    (4223.958) \n  \n  \n    married \n     \n    330.704 \n     \n    299.425 \n     \n    −3213.470 \n  \n  \n     \n     \n    (1285.183) \n     \n    (1292.304) \n     \n    (4445.151) \n  \n  \n    nodegree \n     \n    106.692 \n     \n    92.911 \n     \n    1779.900 \n  \n  \n     \n     \n    (1421.140) \n     \n    (1379.407) \n     \n    (2019.035) \n  \n  \n    re74 \n     \n    0.091 \n     \n    0.056 \n     \n    −0.262 \n  \n  \n     \n     \n    (0.134) \n     \n    (0.122) \n     \n    (0.601) \n  \n  \n    re75 \n     \n    0.193 \n     \n    0.160 \n     \n    2.034 \n  \n  \n     \n     \n    (0.208) \n     \n    (0.200) \n     \n    (0.883) \n  \n  \n    Num.Obs. \n    260 \n    260 \n    265 \n    265 \n    140 \n    140 \n  \n  \n    R2 \n    0.001 \n    0.041 \n    0.015 \n    0.051 \n    0.005 \n    0.079 \n  \n  \n    R2 Adj. \n    −0.002 \n    0.007 \n    0.012 \n    0.018 \n    −0.002 \n    0.015 \n  \n\n\n\n\n\n　推定の結果は、いずれも正であり、職業訓練は所得に正の影響を与えるという結果が得られている。しかし、いずれも標準誤差が非常に大きく、統計的に有意な結果は得られていない。"
  },
  {
    "objectID": "material/matching.html#ipw",
    "href": "material/matching.html#ipw",
    "title": "マッチング",
    "section": "IPW",
    "text": "IPW\n　最後に、{WeightIt}パッケージを使ってIPW推定を行ってみよう。このパッケージはこれまで使ってた{MatchIt}パッケージと非常に似ている。まず、第一引数として処置変数を結果変数、処置有無に影響を与えると考えられる共変量を説明変数とした回帰式を入れる。続いて、データ（data）、IPW算出の方法（method）、推定の対象（estimand）を指定する。データはdata = la_dfとし、傾向スコアからIPWを算出するためmethod = \"ps\"を指定、最後にATT推定のためにestimand = \"ATT\"を指定する。今回はIPW算出のために今回は傾向スコアを使うが、「処置を受ける確率」が計算できるなら何でも良い。たとえば、Imai and Ratkovic (2014)8が推奨しているCovariate Balancing Propensity Score (CBPS) を使用する場合は\"ps\"の代わりに\"cbps\"を、複数の推定を組み合わせるスーパーラーニングをする場合は\"super\"9などが使える。他にもエントロピーバランシングなど様々なオプションが提供されている。\n\nipw_data <- weightit(treat ~ age + educ + black + hispanic + \n                       married + nodegree + re74 + re75, \n                     data = la_df, method = \"ps\", estimand = \"ATT\")\n\n　matchit()とは違って、別途match.data()などの関数は不要である。weightit()パッケージを使うと、IPW推定のための重み変数を返してくれる。また、weightit()から得られたデータは{cobalt}でバランスチェックもできる。\n\nlove.plot(ipw_data, thresholds = 0.25, abs = TRUE)\n\n\n\n\n\n\n\n\n　読み方は最近傍マッチング（傾向スコア）と同じである。ここでもバランスが大幅に改善されていることが分かる。\n　それではIPW推定量を計算してみよう。ここで一つ注意が必要だ。それはdataをipw_dataでなく、元のデータであるla_dfを使うという点だ。また、重み変数はla_dfには含まれていないため、ipw_data$weightsを使う必要がある。\n\nipw_fit1 <- lm(re78 ~ treat, \n               data = la_df, weights = ipw_data$weights)\nipw_fit2 <- lm(re78 ~ treat + age + educ + black + hispanic + \n                       married + nodegree + re74 + re75, \n               data = la_df, weights = ipw_data$weights)\n\nmodelsummary(list(\"MH (単)\"   = mh_fit3,\n                  \"MH (重)\"   = mh_fit4,\n                  \"PS (単)\"   = ps_fit1,\n                  \"PS (重)\"   = ps_fit2,\n                  \"CEM (単)\"  = cem_fit1,\n                  \"CEM (重)\"  = cem_fit2,\n                  \"IPW (単)\"  = ipw_fit1,\n                  \"IPW (重)\"  = ipw_fit2), \n             gof_map   = c(\"nobs\", \"r.squared\", \"adj.r.squared\"))\n\n\n\n \n  \n      \n    MH (単) \n    MH (重) \n    PS (単) \n    PS (重) \n    CEM (単) \n    CEM (重) \n    IPW (単) \n    IPW (重) \n  \n \n\n  \n    (Intercept) \n    5744.482 \n    1371.865 \n    4381.204 \n    −985.292 \n    5265.785 \n    −7253.717 \n    5135.072 \n    853.666 \n  \n  \n     \n    (854.641) \n    (4808.980) \n    (811.618) \n    (4370.562) \n    (850.457) \n    (7031.576) \n    (401.194) \n    (2708.329) \n  \n  \n    treat \n    604.661 \n    524.037 \n    1967.940 \n    1903.412 \n    1070.907 \n    1207.183 \n    1214.071 \n    1237.405 \n  \n  \n     \n    (1013.175) \n    (1011.417) \n    (971.379) \n    (975.164) \n    (1248.129) \n    (1245.959) \n    (568.904) \n    (554.929) \n  \n  \n    age \n     \n    8.365 \n     \n    42.898 \n     \n    147.970 \n     \n    −17.562 \n  \n  \n     \n     \n    (64.228) \n     \n    (60.174) \n     \n    (107.716) \n     \n    (33.570) \n  \n  \n    educ \n     \n    476.666 \n     \n    444.829 \n     \n    879.978 \n     \n    489.248 \n  \n  \n     \n     \n    (319.397) \n     \n    (278.130) \n     \n    (472.982) \n     \n    (172.550) \n  \n  \n    black \n     \n    −1562.414 \n     \n    −893.748 \n     \n    −1941.465 \n     \n    −1149.368 \n  \n  \n     \n     \n    (1568.526) \n     \n    (1538.965) \n     \n    (2485.215) \n     \n    (950.708) \n  \n  \n    hispanic \n     \n    −17.586 \n     \n    −184.643 \n     \n    437.208 \n     \n    202.126 \n  \n  \n     \n     \n    (2423.801) \n     \n    (2343.260) \n     \n    (4223.958) \n     \n    (1463.378) \n  \n  \n    married \n     \n    330.704 \n     \n    299.425 \n     \n    −3213.470 \n     \n    424.461 \n  \n  \n     \n     \n    (1285.183) \n     \n    (1292.304) \n     \n    (4445.151) \n     \n    (807.171) \n  \n  \n    nodegree \n     \n    106.692 \n     \n    92.911 \n     \n    1779.900 \n     \n    −92.470 \n  \n  \n     \n     \n    (1421.140) \n     \n    (1379.407) \n     \n    (2019.035) \n     \n    (844.246) \n  \n  \n    re74 \n     \n    0.091 \n     \n    0.056 \n     \n    −0.262 \n     \n    0.050 \n  \n  \n     \n     \n    (0.134) \n     \n    (0.122) \n     \n    (0.601) \n     \n    (0.078) \n  \n  \n    re75 \n     \n    0.193 \n     \n    0.160 \n     \n    2.034 \n     \n    0.318 \n  \n  \n     \n     \n    (0.208) \n     \n    (0.200) \n     \n    (0.883) \n     \n    (0.121) \n  \n  \n    Num.Obs. \n    260 \n    260 \n    265 \n    265 \n    140 \n    140 \n    614 \n    614 \n  \n  \n    R2 \n    0.001 \n    0.041 \n    0.015 \n    0.051 \n    0.005 \n    0.079 \n    0.007 \n    0.071 \n  \n  \n    R2 Adj. \n    −0.002 \n    0.007 \n    0.012 \n    0.018 \n    −0.002 \n    0.015 \n    0.006 \n    0.057 \n  \n\n\n\n\n\n　IPWの場合、正の処置効果（ATT）が確認され、今回は統計的に有意な結果が得られた。"
  },
  {
    "objectID": "material/matching.html#付録",
    "href": "material/matching.html#付録",
    "title": "マッチング",
    "section": "付録",
    "text": "付録\n\nノンパラメトリック推定\nここではマハラノビス最近傍マッチングの例を使って、ATTを推定してみましょう。まずは、処置群の潜在的結果を統制群から割り当てて差分を算出し、その平均値を求るというのがノンパラメトリックな推定方法です。以下のコードが理解できるようになれば、他の最近傍マッチング（傾向スコアマッチングなど）も外部パッケージを使わずに計算できるようになります。\n各処置群がどのケースとマッチングされたかはマッチングのオブジェクト名の後に$match.matrixを付けることで抽出できます。ちょっとやってみましょう。\n\n# 全部表示させると長くなんるので、最初の6行まで確認\nhead(mh_mat1$match.matrix) \n\n  [,1] \n1 \"553\"\n2 \"526\"\n3 \"601\"\n4 \"608\"\n5 \"585\"\n6 \"557\"\n\n\n左が処置群、右が統制群ですね。それぞれケースのIDを抽出しましょう。\n\n# MH.Matching$match.matrixは1列の行列です。\n# 処置群IDは行の名前として格納され、統制群IDは1列目に入っています。\nMH_Treat   <- as.vector(row.names(mh_mat1$match.matrix))\nMH_Control <- as.vector(mh_mat1$match.matrix[, 1])\n\n# 中身を確認\nprint(MH_Treat)\n\n  [1] \"1\"   \"2\"   \"3\"   \"4\"   \"5\"   \"6\"   \"7\"   \"8\"   \"9\"   \"10\"  \"11\"  \"12\" \n [13] \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\"  \"19\"  \"20\"  \"21\"  \"22\"  \"23\"  \"24\" \n [25] \"25\"  \"26\"  \"27\"  \"28\"  \"29\"  \"30\"  \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\" \n [37] \"37\"  \"38\"  \"39\"  \"40\"  \"41\"  \"42\"  \"43\"  \"44\"  \"45\"  \"46\"  \"47\"  \"48\" \n [49] \"49\"  \"50\"  \"51\"  \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"60\" \n [61] \"61\"  \"62\"  \"63\"  \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"70\"  \"71\"  \"72\" \n [73] \"73\"  \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"80\"  \"81\"  \"82\"  \"83\"  \"84\" \n [85] \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"90\"  \"91\"  \"92\"  \"93\"  \"94\"  \"95\"  \"96\" \n [97] \"97\"  \"98\"  \"99\"  \"100\" \"101\" \"102\" \"103\" \"104\" \"105\" \"106\" \"107\" \"108\"\n[109] \"109\" \"110\" \"111\" \"112\" \"113\" \"114\" \"115\" \"116\" \"117\" \"118\" \"119\" \"120\"\n[121] \"121\" \"122\" \"123\" \"124\" \"125\" \"126\" \"127\" \"128\" \"129\" \"130\" \"131\" \"132\"\n[133] \"133\" \"134\" \"135\" \"136\" \"137\" \"138\" \"139\" \"140\" \"141\" \"142\" \"143\" \"144\"\n[145] \"145\" \"146\" \"147\" \"148\" \"149\" \"150\" \"151\" \"152\" \"153\" \"154\" \"155\" \"156\"\n[157] \"157\" \"158\" \"159\" \"160\" \"161\" \"162\" \"163\" \"164\" \"165\" \"166\" \"167\" \"168\"\n[169] \"169\" \"170\" \"171\" \"172\" \"173\" \"174\" \"175\" \"176\" \"177\" \"178\" \"179\" \"180\"\n[181] \"181\" \"182\" \"183\" \"184\" \"185\"\n\nprint(MH_Control)\n\n  [1] \"553\" \"526\" \"601\" \"608\" \"585\" \"557\" \"537\" \"411\" \"515\" \"578\" \"540\" \"566\"\n [13] \"561\" \"493\" \"463\" \"552\" \"454\" \"462\" \"597\" \"374\" \"573\" \"405\" \"409\" \"559\"\n [25] \"596\" \"560\" \"574\" \"384\" \"577\" \"416\" \"586\" \"604\" \"593\" \"558\" \"571\" \"584\"\n [37] \"518\" \"516\" \"423\" \"438\" \"522\" \"422\" \"583\" \"576\" \"372\" \"381\" \"441\" \"451\"\n [49] \"413\" \"520\" \"403\" \"565\" \"364\" \"419\" \"478\" \"591\" \"511\" \"613\" \"581\" \"538\"\n [61] \"555\" \"358\" \"303\" \"343\" \"476\" \"530\" \"319\" \"432\" \"539\" \"344\" \"453\" \"580\"\n [73] \"342\" \"376\" \"295\" \"610\" \"445\" \"325\" \"279\" \"542\" \"282\" \"283\" \"546\" \"244\"\n [85] \"508\" \"285\" \"450\" \"459\" \"592\" \"562\" \"567\" \"536\" \"475\" \"525\" \"296\" \"430\"\n [97] \"455\" \"427\" \"527\" \"524\" \"365\" \"474\" \"367\" \"594\" \"443\" \"572\" \"556\" \"582\"\n[109] \"362\" \"458\" \"284\" \"551\" \"414\" \"470\" \"231\" \"507\" \"500\" \"226\" \"467\" \"501\"\n[121] \"415\" \"466\" \"436\" \"353\" \"510\" \"563\" \"339\" \"544\" \"512\" \"499\" \"446\" \"212\"\n[133] \"352\" \"402\" \"399\" \"209\" \"420\" \"390\" \"363\" \"312\" \"378\" \"306\" \"271\" \"457\"\n[145] \"297\" \"368\" \"398\" \"370\" \"275\" \"394\" \"356\" \"288\" \"327\" \"322\" \"323\" \"254\"\n[157] \"274\" \"273\" \"369\" \"393\" \"465\" \"541\" \"485\" \"191\" \"257\" \"607\" \"387\" \"281\"\n[169] \"233\" \"190\" \"193\" \"228\" \"335\" \"203\" \"278\" \"205\" \"280\" \"276\" \"290\" \"202\"\n[181] \"192\" \"208\" \"195\" \"186\" \"200\"\n\n\n次は、元のlalondeデータから処置群と統制群の結果変数 (re78)を抽出し、MH.Matched.dfというデータフレームとしてまとめます。続いて、各ケースのITEを計算します。\n\nmh_mat1ched_df <- tibble(\n    Treat_ID   = MH_Treat,\n    Control_ID = MH_Control,\n    Treat_Y    = lalonde[MH_Treat,]$re78,\n    Control_Y  = lalonde[MH_Control,]$re78\n)\n\nmh_mat1ched_df <- mh_mat1ched_df |>\n    mutate(ITE = Treat_Y - Control_Y)\n\n\nmh_mat1ched_df\n\n# A tibble: 185 × 5\n   Treat_ID Control_ID Treat_Y Control_Y     ITE\n   <chr>    <chr>        <dbl>     <dbl>   <dbl>\n 1 1        553          9930.        0   9930. \n 2 2        526          3596.    12618. -9022. \n 3 3        601         24909.        0  24909. \n 4 4        608          7506.     7544.   -37.6\n 5 5        585           290.      649.  -359. \n 6 6        557          4056.     6084. -2027. \n 7 7        537             0      2285. -2285. \n 8 8        411          8472.      702.  7770. \n 9 9        515          2164.      117.  2047. \n10 10       578         12418.    18757. -6339. \n# … with 175 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n1行目を見ると1は553とマッチングされ、それぞれの結果変数は9930.046ドル、0ドルです。その差分がITEであり、9930.046ドルですね。\nこれらITEの平均値がATTとなります。早速計算してみましょう。\n\nMH_ATT <- mean(mh_mat1ched_df$ITE)\n\nMH_ATT\n\n[1] 516.6367\n\n\nATTは約516.637ドルでした。回帰分析でやった結果と一致しますね。大人しく回帰分析でやりましょう。"
  },
  {
    "objectID": "material/matching.html#セットアップ",
    "href": "material/matching.html#セットアップ",
    "title": "マッチング",
    "section": "セットアップ",
    "text": "セットアップ\n　本日の実習で使用するパッケージを読み込む。\n\npacman::p_load(tidyverse, \n               broom,\n               MatchIt, \n               WeightIt, \n               cobalt, \n               summarytools,\n               modelsummary,\n               fastDummies)\npacman::p_load_gh(\"JaehyunSong/BalanceR\")\n\n　マッチングにおける古典的なデータセット、lalondeを読み込む。data(lalonde, package = \"cobalt\")を入力するだけで、{cobalt}パッケージ内のlaondeという名前のデータフレームが作業環境内にlalondeという名で格納される1。このデータをla_dfという名のオブジェクトとして改めて保存しておこう。ただし、lalondeデータセットの形式はdata.frameである。このままでも全く問題ないが、data.frameの拡張版であるtibble形式の方がより読みやすいので、格納する前にlalondeのデータ構造をdata.frameからtibbleへ変更しておこう（as_tibble()関数を使う）。\n\n# cobaltパッケージが提供するデータセットの読み込み\ndata(\"lalonde\", package = \"cobalt\")\n\nla_df <- as_tibble(lalonde)\n\n　それでは、データの中身を確認してみよう。\n\nla_df\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   <int> <int> <int> <fct>    <int>    <int> <dbl> <dbl>  <dbl>\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# … with 604 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　分析に入る前に、名目変数である人種（race）をダミー変数に変換する。raceは3種類の値で構成されているため、生成するダミー変数も3つとなる。ダミー化には{fastDummies}パッケージのdummy_cols()関数を使用する。\n\nla_df <- la_df |>\n  dummy_cols(select_columns = \"race\")\n\nla_df\n\n# A tibble: 614 × 12\n   treat   age  educ race   married nodegree  re74  re75   re78 race_b…¹ race_…²\n   <int> <int> <int> <fct>    <int>    <int> <dbl> <dbl>  <dbl>    <int>   <int>\n 1     1    37    11 black        1        1     0     0  9930.        1       0\n 2     1    22     9 hispan       0        1     0     0  3596.        0       1\n 3     1    30    12 black        0        0     0     0 24909.        1       0\n 4     1    27    11 black        0        1     0     0  7506.        1       0\n 5     1    33     8 black        0        1     0     0   290.        1       0\n 6     1    22     9 black        0        1     0     0  4056.        1       0\n 7     1    23    12 black        0        0     0     0     0         1       0\n 8     1    32    11 black        0        1     0     0  8472.        1       0\n 9     1    22    16 black        0        0     0     0  2164.        1       0\n10     1    33    12 white        1        0     0     0 12418.        0       0\n# … with 604 more rows, 1 more variable: race_white <int>, and abbreviated\n#   variable names ¹​race_black, ²​race_hispan\n# ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names\n\n\n　このまま記述統計を見たり、分析に入っても良いが、もう少しデータを加工してみよう。まずrace_で始まる3つのダミー変数の位置をraceの前へ変更する。また、race変数は不要なので、race変数を除外する。最後に、race_で始まるダミー変数の名前を変更してみよう。変数の位置変更はrelocate()関数を使用する。\n\nla_df <- la_df |>\n  relocate(starts_with(\"race_\"), .before = race) |>\n  select(-race) |>\n  rename(\"black\"    = \"race_black\",\n         \"hispanic\" = \"race_hispan\",\n         \"white\"    = \"race_white\")\n\nla_df\n\n# A tibble: 614 × 11\n   treat   age  educ black hispanic white married nodegree  re74  re75   re78\n   <int> <int> <int> <int>    <int> <int>   <int>    <int> <dbl> <dbl>  <dbl>\n 1     1    37    11     1        0     0       1        1     0     0  9930.\n 2     1    22     9     0        1     0       0        1     0     0  3596.\n 3     1    30    12     1        0     0       0        0     0     0 24909.\n 4     1    27    11     1        0     0       0        1     0     0  7506.\n 5     1    33     8     1        0     0       0        1     0     0   290.\n 6     1    22     9     1        0     0       0        1     0     0  4056.\n 7     1    23    12     1        0     0       0        0     0     0     0 \n 8     1    32    11     1        0     0       0        1     0     0  8472.\n 9     1    22    16     1        0     0       0        0     0     0  2164.\n10     1    33    12     0        0     1       1        0     0     0 12418.\n# … with 604 more rows\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　それでは記述統計量を確認してみよう。\n\ndescr(la_df,\n      stats = c(\"mean\", \"sd\", \"min\", \"max\"),\n      transpose = TRUE,\n      order = \"p\")\n\n\n\n\n\n \nMean\nStd.Dev\nMin\nMax\n\n\n\n\ntreat\n0.30\n0.46\n0.00\n1.00\n\n\nage\n27.36\n9.88\n16.00\n55.00\n\n\neduc\n10.27\n2.63\n0.00\n18.00\n\n\nblack\n0.40\n0.49\n0.00\n1.00\n\n\nhispanic\n0.12\n0.32\n0.00\n1.00\n\n\nwhite\n0.49\n0.50\n0.00\n1.00\n\n\nmarried\n0.42\n0.49\n0.00\n1.00\n\n\nnodegree\n0.63\n0.48\n0.00\n1.00\n\n\nre74\n4557.55\n6477.96\n0.00\n35040.07\n\n\nre75\n2184.94\n3295.68\n0.00\n25142.24\n\n\nre78\n6792.83\n7470.73\n0.00\n60307.93"
  },
  {
    "objectID": "material/matching.html#回帰分析",
    "href": "material/matching.html#回帰分析",
    "title": "マッチング",
    "section": "回帰分析",
    "text": "回帰分析\n\nDiM推定量\n　処置効果を確認するために、まずはグループごとの応答変数の差分（Difference-in-Means; DiM）を計算してみよう。処置変数はtreatであり、職業訓練を受けた回答者は1、受けなかった回答者は0となる。応答変数re78は1978年における回答者の収入である。\n\nDiff_Mean_df <- la_df |> \n    group_by(treat) |>\n    summarise(Outcome = mean(re78),\n              .groups = \"drop\")\n\nDiff_Mean_df\n\n# A tibble: 2 × 2\n  treat Outcome\n  <int>   <dbl>\n1     0   6984.\n2     1   6349.\n\n\n　この結果を可視化する必要はあまり無いかも知れないが、以下のようなコードで可視化することもできる。\n\nDiff_Mean_df |>\n  ggplot() +\n  geom_bar(aes(x = treat, y = Outcome), \n           stat = \"identity\", width = 0.5) +\n  geom_label(aes(x = treat, y = Outcome,\n                 label = round(Outcome, 3))) +\n  labs(x = \"Treatment\",\n       y = \"Outcome (US Dollars)\") +\n  # scale_x_continuous()を使って0/1をControl/Treatmentに置換する\n  # 目盛りはX軸上の0と1、各目盛りのラベルはControlとTreatmentに\n  scale_x_continuous(breaks = c(0, 1), labels = c(\"Control\", \"Treatment\")) +\n  coord_cartesian(xlim = c(-0.5, 1.5))\n\n\n\n\n\n\n\n\n　treat == 0の回答者、つまり職業訓練を受けていない回答者の平均所得は約6984ドル、treat == 1の回答者、つまり職業訓練を受けた回答者の平均所得は約6394ドルだ。その差は約-650ドルだが、職業訓練を受けた回答者の方が低所得になっている。これは直感的に納得できる結果ではないだろう。むろん、実際、職業訓練が所得を減らす可能性もあるが、今回の結果はより詳しく分析してみる価値があろう。\n　ちなみに、以上の結果は単回帰分析からも確認できる (ただし、統計的に有意ではない)。\n\nDiM_fit <- lm(re78 ~ treat, data = la_df)\nmodelsummary(DiM_fit,\n             # 係数の点推定値と95%信頼区間を示す場合\n             estimate   = \"{estimate} [{conf.low}, {conf.high}]\",\n             statistic  = NULL,\n             conf_level = 0.95,\n             # ケース数、決定係数、調整済み決定係数を出力\n             gof_map    = c(\"nobs\", \"r.squared\", \"adj.r.squared\"))\n\n\n\n \n  \n      \n    Model 1 \n  \n \n\n  \n    (Intercept) \n    6984.170 [6275.791, 7692.549] \n  \n  \n    treat \n    −635.026 [−1925.544, 655.492] \n  \n  \n    Num.Obs. \n    614 \n  \n  \n    R2 \n    0.002 \n  \n  \n    R2 Adj. \n    0.000 \n  \n\n\n\n\n\n　この直感的でない結果は、もしかしたらセレクションバイアスが原因かも知れない。職業訓練の対象が元々非常に所得が低い回答者になっている可能性がある。たとえば、下の図のように職業訓練の有無が教育水準や人種、これまでの所得などと関係しているとしよう。これらの要因は回答者の現在所得にも関係していると考えられる。この場合、処置有無と所得の間には内生性が存在することになる。\n\n\n\n\n\n\n\n\n\n　本当にそうなのかを、共変量のバランスチェックをしてみよう。もし、処置有無によって回答者の社会経済的要因に大きな差があれば、内生性が存在する証拠になろう。ここでは誰かが作成しました{BalanceR}パッケージを使ってみよう。\n\nblc_chk <- la_df |>\n  BalanceR(group = treat, cov = age:re75)\n\n　{BalanceR}パッケージで共変量を指定する際、:演算子が使える。age:re75は、データセットのageからre75変数までをすべて指定することを意味する。names(la_df)で変数がどの順番で並んでいるかが分かる。\n\nnames(la_df)\n\n [1] \"treat\"    \"age\"      \"educ\"     \"black\"    \"hispanic\" \"white\"   \n [7] \"married\"  \"nodegree\" \"re74\"     \"re75\"     \"re78\"    \n\n\n　それではバランスチェックの結果を確認してみよう。\n\nblc_chk\n\n  Covariate   Mean:0     SD:0   Mean:1     SD:1   SB:0-1\n1       age   28.030   10.787   25.816    7.155   24.190\n2      educ   10.235    2.855   10.346    2.011   -4.476\n3     black    0.203    0.403    0.843    0.365 -167.083\n4  hispanic    0.142    0.350    0.059    0.237   27.740\n5     white    0.655    0.476    0.097    0.297  140.799\n6   married    0.513    0.500    0.189    0.393   72.076\n7  nodegree    0.597    0.491    0.708    0.456  -23.549\n8      re74 5619.237 6788.751 2095.574 4886.620   59.575\n9      re75 2466.484 3291.996 1532.055 3219.251   28.700\n\n\n　アンバランスと判定する標準化差分（標準化バイアス）の閾値には決まった値が無いが、最も緩い基準でも25程度である（計算時に100を掛けないのであれば0.25）。しかし、いくつか怪しい箇所がある。たとえば、treat == 0の回答者において黒人の割合は約20%だが、treat == 1のそれは約85%だ。つまり、黒人ほどより職業訓練を受ける傾向があることを意味する。また、人種は所得にも影響を与えると考えられる。これは処置と応答変数の間に交絡要因があることを意味する。実際、標準化バイアスは-167という、非常に大きい数値を示している。この結果を図としてまとめてみましょう。\n\n# 絶対値変換。SB = 25に破線\nplot(blc_chk, abs = TRUE, vline = 25) +\n  # 縦軸目盛りラベルの修正\n  scale_y_discrete(labels = c(\"age\"      = \"Age\",\n                              \"educ\"     = \"Education\",\n                              \"black\"    = \"Race (Black)\",\n                              \"hispanic\" = \"Race (Hispanic)\",\n                              \"white\"    = \"Race (White)\",\n                              \"married\"  = \"Married\",\n                              \"nodegree\" = \"No Degree\",\n                              \"re74\"     = \"Revenue (1974)\",\n                              \"re75\"     = \"Revenue (1975)\")) +\n  # 凡例の削除\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n　かなり緩めの基準である25を採用しても、人種、結婚有無、74・75年の所得のバランスが非常に悪く、内生性（=自己選択バイアス）があると判断して良いだろう。以下ではこの内生性に対処する様々な方法を紹介する。\n\n\n重回帰分析\n　まずは、重回帰分析からだ。用いる共変量は年齢、教育水準、黒人ダミー、ヒスパニックダミー2、既婚ダミー、学位なしダミー、74・75年の所得だ。lm()関数で78年の所得をこちらの変数に回帰させてみよう。\n\\[\n\\begin{align}\n\\widehat{\\mbox{re78}} = & \\beta_0 + \\beta_1 \\mbox{treat} + \\beta_2 \\mbox{age} + \\beta_3 \\mbox{educ} + \\\\\n& \\beta_4 \\mbox{black} + \\beta_5 \\mbox{hispanic} + \\beta_6 \\mbox{married} + \\beta_7 \\mbox{nodegree} + \\beta_8 \\mbox{re74} + \\beta_9 \\mbox{re75}.\n\\end{align}\n\\]\n\nmlm_fit <- lm(re78 ~ treat + age + educ + black + hispanic + married + \n                   nodegree + re74 + re75, data = la_df)\n\nmodelsummary(list(\"単回帰分析\" = DiM_fit, \"重回帰分析\" = mlm_fit), \n             estimate  = \"{estimate} ({std.error})\",\n             statistic = NULL,\n             gof_map   = c(\"nobs\", \"r.squared\", \"adj.r.squared\"))\n\n\n\n \n  \n      \n    単回帰分析 \n    重回帰分析 \n  \n \n\n  \n    (Intercept) \n    6984.170 (360.710) \n    66.515 (2436.746) \n  \n  \n    treat \n    −635.026 (657.137) \n    1548.244 (781.279) \n  \n  \n    age \n     \n    12.978 (32.489) \n  \n  \n    educ \n     \n    403.941 (158.906) \n  \n  \n    black \n     \n    −1240.644 (768.764) \n  \n  \n    hispanic \n     \n    498.897 (941.943) \n  \n  \n    married \n     \n    406.621 (695.472) \n  \n  \n    nodegree \n     \n    259.817 (847.442) \n  \n  \n    re74 \n     \n    0.296 (0.058) \n  \n  \n    re75 \n     \n    0.232 (0.105) \n  \n  \n    Num.Obs. \n    614 \n    614 \n  \n  \n    R2 \n    0.002 \n    0.148 \n  \n  \n    R2 Adj. \n    0.000 \n    0.135 \n  \n\n\n\n\n\n　共変量を統制したら処置変数の係数は約1548.244ドルだ。単回帰分析の結果とは違って、統計的に有意な正の効果が確認されている。ますます分からなくなってしまう。"
  },
  {
    "objectID": "material/matching.html#推定値の比較",
    "href": "material/matching.html#推定値の比較",
    "title": "マッチング",
    "section": "推定値の比較",
    "text": "推定値の比較\n　これまで見てきたように、同じく「マッチング」とは言っても手法によって結果のばらつきが大きいことが分かる。また、同じ手法であっても復元か、非復元か、1:1マッチングか、1:nマッチングか、CEMならレイヤーをどれほど細かくするかなどによっても結果は大きく変わる。\n　ここまで得られた多くの結果から自分にとって都合の良い結果のみを報告するのは、あるいみ研究不正に近い。なぜなら、これを逆にいうと自分にとって都合の悪い結果を隠蔽しているものだからだ。したがって、実際の論文にはそれぞれの結果を報告・比較し、その結果を慎重に解釈する必要がある。ここではこれまで推定してきたマッチングの結果を一つの図としてまとめてみよう。\n　まずは、{broom}パッケージのtidy()関数で回帰分析の結果を表でまとめ、bind_rows()を使って一つに統合する。tidy()内にconf.int = TRUEを入れておくと、95%信頼区間も出してくれるので、今の段階で入れておこう。\n\natt_df <- bind_rows(list(\"単回帰_最近傍（マハラノビス）\" = tidy(mh_fit3, conf.int = TRUE),\n                         \"重回帰_最近傍（マハラノビス）\" = tidy(mh_fit4, conf.int = TRUE),\n                         \"単回帰_最近傍（傾向スコア）\"   = tidy(ps_fit1, conf.int = TRUE),\n                         \"重回帰_最近傍（傾向スコア）\"   = tidy(ps_fit2, conf.int = TRUE),\n                         \"単回帰_CEM\" = tidy(cem_fit1, conf.int = TRUE),\n                         \"重回帰_CEM\" = tidy(cem_fit2, conf.int = TRUE),\n                         \"単回帰_IPW\" = tidy(ipw_fit1, conf.int = TRUE),\n                         \"重回帰_IPW\" = tidy(ipw_fit2, conf.int = TRUE)),\n                    .id = \"Model\")\n\natt_df\n\n# A tibble: 48 × 8\n   Model                term   estim…¹ std.e…² statis…³  p.value conf.…⁴ conf.…⁵\n   <chr>                <chr>    <dbl>   <dbl>    <dbl>    <dbl>   <dbl>   <dbl>\n 1 単回帰_最近傍（マハ… (Inte…  5.74e3   855.   6.72    1.15e-10   4062.   7427.\n 2 単回帰_最近傍（マハ… treat   6.05e2  1013.   0.597   5.51e- 1  -1390.   2600.\n 3 重回帰_最近傍（マハ… (Inte…  1.37e3  4809.   0.285   7.76e- 1  -8099.  10843.\n 4 重回帰_最近傍（マハ… treat   5.24e2  1011.   0.518   6.05e- 1  -1468.   2516.\n 5 重回帰_最近傍（マハ… age     8.36e0    64.2  0.130   8.96e- 1   -118.    135.\n 6 重回帰_最近傍（マハ… educ    4.77e2   319.   1.49    1.37e- 1   -152.   1106.\n 7 重回帰_最近傍（マハ… black  -1.56e3  1569.  -0.996   3.20e- 1  -4652.   1527.\n 8 重回帰_最近傍（マハ… hispa… -1.76e1  2424.  -0.00726 9.94e- 1  -4791.   4756.\n 9 重回帰_最近傍（マハ… marri…  3.31e2  1285.   0.257   7.97e- 1  -2200.   2862.\n10 重回帰_最近傍（マハ… nodeg…  1.07e2  1421.   0.0751  9.40e- 1  -2692.   2906.\n# … with 38 more rows, and abbreviated variable names ¹​estimate, ²​std.error,\n#   ³​statistic, ⁴​conf.low, ⁵​conf.high\n# ℹ Use `print(n = ...)` to see more rows\n\n\n　続いて、処置効果（ATT）を意味する行のみを残す。termの値が\"treat\"と一致する行が処置効果である。\n\natt_df <- att_df |>\n  filter(term == \"treat\")\n\natt_df\n\n# A tibble: 8 × 8\n  Model                    term  estim…¹ std.e…² stati…³ p.value conf.…⁴ conf.…⁵\n  <chr>                    <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 単回帰_最近傍（マハラノ… treat    605.   1013.   0.597  0.551  -1390.    2600.\n2 重回帰_最近傍（マハラノ… treat    524.   1011.   0.518  0.605  -1468.    2516.\n3 単回帰_最近傍（傾向スコ… treat   1968.    971.   2.03   0.0438    55.3   3881.\n4 重回帰_最近傍（傾向スコ… treat   1903.    975.   1.95   0.0520   -17.0   3824.\n5 単回帰_CEM               treat   1071.   1248.   0.858  0.392  -1397.    3539.\n6 重回帰_CEM               treat   1207.   1246.   0.969  0.334  -1258.    3672.\n7 単回帰_IPW               treat   1214.    569.   2.13   0.0332    96.8   2331.\n8 重回帰_IPW               treat   1237.    555.   2.23   0.0261   148.    2327.\n# … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic,\n#   ⁴​conf.low, ⁵​conf.high\n\n\n　ここでは新しく登場した関数を使用する。separate()関数は文字列で構成されている列を、特定の文字を基準に列分割する関数である。Model行のそれぞれの値は回帰モデル_マッチングモデルで構成され、これを_文字を基準にRegressionとMethod列に分割する。分割する列名はcol、分割後の列名はinto、分割の基準となる文字はsepに指定する。\n\natt_df <- att_df |>\n  separate(col  = Model,\n           into = c(\"Regression\", \"Method\"),\n           sep  = \"_\")\n\natt_df\n\n# A tibble: 8 × 9\n  Regression Method        term  estim…¹ std.e…² stati…³ p.value conf.…⁴ conf.…⁵\n  <chr>      <chr>         <chr>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 単回帰     最近傍（マハ… treat    605.   1013.   0.597  0.551  -1390.    2600.\n2 重回帰     最近傍（マハ… treat    524.   1011.   0.518  0.605  -1468.    2516.\n3 単回帰     最近傍（傾向… treat   1968.    971.   2.03   0.0438    55.3   3881.\n4 重回帰     最近傍（傾向… treat   1903.    975.   1.95   0.0520   -17.0   3824.\n5 単回帰     CEM           treat   1071.   1248.   0.858  0.392  -1397.    3539.\n6 重回帰     CEM           treat   1207.   1246.   0.969  0.334  -1258.    3672.\n7 単回帰     IPW           treat   1214.    569.   2.13   0.0332    96.8   2331.\n8 重回帰     IPW           treat   1237.    555.   2.23   0.0261   148.    2327.\n# … with abbreviated variable names ¹​estimate, ²​std.error, ³​statistic,\n#   ⁴​conf.low, ⁵​conf.high\n\n\n　後はこのデータを使って作図するだけである。データをggplot()に渡す前にRegressionとMethod列をfactor化しておこう。\n\natt_df |>\n  mutate(Regression = fct_inorder(Regression),\n         Method     = fct_inorder(Method)) |>\n  ggplot() +\n  geom_pointrange(aes(x = estimate, y = Method,\n                      xmin = conf.low, xmax = conf.high,\n                      color = Regression),\n                  position = position_dodge2(1/2)) +\n  labs(x = \"処置群における処置効果（ATT）\", y = \"\", color = \"モデル\",\n       caption = \"注: マッチングの場合、復元マッチングを行った。\") +\n  theme_bw(base_size = 12)\n\n\n\n\n\n\n\n\n　もし、縦軸の順番を逆にしたい場合は、fct_rev()関数で要素の順番を逆にする。\n\natt_df |>\n  mutate(Regression = fct_inorder(Regression),\n         Method     = fct_inorder(Method),\n         Method     = fct_rev(Method)) |>\n  ggplot() +\n  geom_vline(xintercept = 0) +\n  geom_pointrange(aes(x = estimate, y = Method,\n                      xmin = conf.low, xmax = conf.high,\n                      color = Regression),\n                  position = position_dodge2(1/2)) +\n  labs(x = \"処置群における処置効果（ATT）\", y = \"\", color = \"モデル\",\n       caption = \"注: マッチングの場合、復元マッチングを行った。\") +\n  theme_bw(base_size = 12)"
  },
  {
    "objectID": "slide/matching.html#復元マッチングと非復元マッチング",
    "href": "slide/matching.html#復元マッチングと非復元マッチング",
    "title": "方法論特殊講義III",
    "section": "復元マッチングと非復元マッチング",
    "text": "復元マッチングと非復元マッチング\n\n1:1マッチングの場合に生じる問題: マッチング済みの統制群（処置群）をどう扱うか\n\n他にも近い処置群のケースがあればマッチング \\(\\rightarrow\\) 復元マッチング\n他にも近い処置群のケースがあっても使わない \\(\\rightarrow\\) 非復元マッチング\n\n復元マッチングの場合、統制群の各ケースに重みが付与される。\n\n加重平均 or 重み付け回帰分析が必要\n\n多くのパッケージは非復元がデフォルトとなっているが、推定ごとに結果が変化することも（図BとC）\n\n復元マッチングはバランスが改善されやすいが、サンプルサイズが小さくなる。\n\n正しい方法はなく、分析者の判断が必要。\n\n\n\n\n\n\n\nA) 復元マッチング\n\n\n\n\n\n\n\nB) 非復元マッチング (1)\n\n\n\n\n\n\n\nC) 非復元マッチング (2)"
  }
]